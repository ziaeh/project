{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["Dx8CnTDLt6Cu","X9DK8JJER8Xd","3bRNQKEvSCj9","ZInwWap9SPHg","EcfncD5dTLP_","v40GmaHwueit","YwbebDSmuoDN","vZ2lm_CMYoD6","sWqYk0hpYv2d"],"machine_shape":"hm","authorship_tag":"ABX9TyPe2SemK9sbIHEyBlMc6TAS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"06c77d9c51e84484a3c0622a38b9e3a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_401979aef41c462680332fe50282a014","IPY_MODEL_8ea9e0b0f12046d8a9abe0537a963447","IPY_MODEL_d1b9fa50d63045809077c748ba152336"],"layout":"IPY_MODEL_66cc399b679f47e3b25a65f89589c988"}},"401979aef41c462680332fe50282a014":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_995800c9277247e7b40df1f9a8a350a0","placeholder":"​","style":"IPY_MODEL_6096774c7c2e44f0803d020dc3cd4446","value":"Downloading (…)okenizer_config.json: 100%"}},"8ea9e0b0f12046d8a9abe0537a963447":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dedb0da050b48349934b921fa776cff","max":337,"min":0,"orientation":"horizontal","style":"IPY_MODEL_973c37907fa94a22b98f1e31e3d96b79","value":337}},"d1b9fa50d63045809077c748ba152336":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7412309bae6147d3a644b464274d68a3","placeholder":"​","style":"IPY_MODEL_add09282be354f239de3f2541ee6836f","value":" 337/337 [00:00&lt;00:00, 25.3kB/s]"}},"66cc399b679f47e3b25a65f89589c988":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"995800c9277247e7b40df1f9a8a350a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6096774c7c2e44f0803d020dc3cd4446":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0dedb0da050b48349934b921fa776cff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"973c37907fa94a22b98f1e31e3d96b79":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7412309bae6147d3a644b464274d68a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"add09282be354f239de3f2541ee6836f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4df1ed0020594fef9e9731f9dc261963":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_16b327ee297a4505a89194f90435816e","IPY_MODEL_5dcbb2bfc7014daa9f215b7736173392","IPY_MODEL_36cfa3f4cddb4657bd04bf3745bffe21"],"layout":"IPY_MODEL_9c491fc18aae4b7492ddb46d8a0f784f"}},"16b327ee297a4505a89194f90435816e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31e6d404f7a3435f9ce45a87f14ca8b9","placeholder":"​","style":"IPY_MODEL_31a2b8dca2144b8cb82b491947cf4a02","value":"Downloading (…)/main/tokenizer.json: 100%"}},"5dcbb2bfc7014daa9f215b7736173392":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_16f1abf71fa942b8bf24ba57f251f178","max":1049475,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b8622f2e398243e3a82bd0efd9b5bea5","value":1049475}},"36cfa3f4cddb4657bd04bf3745bffe21":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f30be4d19f9402a83f07f015982fef9","placeholder":"​","style":"IPY_MODEL_a47bcd08fdff45e18e352bea54c07d76","value":" 1.05M/1.05M [00:00&lt;00:00, 1.35MB/s]"}},"9c491fc18aae4b7492ddb46d8a0f784f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31e6d404f7a3435f9ce45a87f14ca8b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31a2b8dca2144b8cb82b491947cf4a02":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16f1abf71fa942b8bf24ba57f251f178":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8622f2e398243e3a82bd0efd9b5bea5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5f30be4d19f9402a83f07f015982fef9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a47bcd08fdff45e18e352bea54c07d76":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77baa506c3954942a4a96288a011b4cd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a57e6d22b61249d38e78792b427e312d","IPY_MODEL_01bdd49fb3ec4b5cb61fe2571758c37b","IPY_MODEL_b96bd1fc4b2e4ab4b279779798737304"],"layout":"IPY_MODEL_40904b0f19c04cc8abdb4b0e494cd374"}},"a57e6d22b61249d38e78792b427e312d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7a0ce907f2c4b32a8f973ee46a4100f","placeholder":"​","style":"IPY_MODEL_06538d8f2d7c47e99c79d6200cf3d125","value":"Downloading (…)cial_tokens_map.json: 100%"}},"01bdd49fb3ec4b5cb61fe2571758c37b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ed887f532f641efb55d38516e8fa76f","max":109,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ba413ca4226241c0bb282e9a07f7388b","value":109}},"b96bd1fc4b2e4ab4b279779798737304":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1563781090244bcda80ff9d68087de08","placeholder":"​","style":"IPY_MODEL_74618eef457f47df9ef285ed69c11b5a","value":" 109/109 [00:00&lt;00:00, 8.19kB/s]"}},"40904b0f19c04cc8abdb4b0e494cd374":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7a0ce907f2c4b32a8f973ee46a4100f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06538d8f2d7c47e99c79d6200cf3d125":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ed887f532f641efb55d38516e8fa76f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba413ca4226241c0bb282e9a07f7388b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1563781090244bcda80ff9d68087de08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74618eef457f47df9ef285ed69c11b5a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e0e779d63064b1b87045cfe3eb3f1f8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cc50364d382945e0a46ea043e8f868df","IPY_MODEL_ee51b708040848ecbf3ef1233c76a822","IPY_MODEL_e1283b67890545a4a8a35491ec8a310d"],"layout":"IPY_MODEL_c77393ffc6ec4c908745769a7f9d6913"}},"cc50364d382945e0a46ea043e8f868df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee6b274d756c48d999b8f9f34c5ff819","placeholder":"​","style":"IPY_MODEL_129edab252ea482ab12bc6db0d7915b7","value":"Downloading (…)lve/main/config.json: 100%"}},"ee51b708040848ecbf3ef1233c76a822":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e3e1640d88f4a2d8e7e3bc7a0948bf6","max":1380,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c205afabcf3e4ced8b3d2384d6e75e94","value":1380}},"e1283b67890545a4a8a35491ec8a310d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c143a85988f4c669c7268d02a1518bb","placeholder":"​","style":"IPY_MODEL_6c7282c6319c418ba045ddae1900dad0","value":" 1.38k/1.38k [00:00&lt;00:00, 119kB/s]"}},"c77393ffc6ec4c908745769a7f9d6913":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee6b274d756c48d999b8f9f34c5ff819":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"129edab252ea482ab12bc6db0d7915b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e3e1640d88f4a2d8e7e3bc7a0948bf6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c205afabcf3e4ced8b3d2384d6e75e94":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c143a85988f4c669c7268d02a1518bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c7282c6319c418ba045ddae1900dad0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6b732af789ce48b48aab6a083bc36a96":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5ddb8320e20345ba8012be9dafabea74","IPY_MODEL_4fcf1b6f59414b1ca8b7cc9255b0200b","IPY_MODEL_cdb67c115ae146d980dad8cad43d0d4e"],"layout":"IPY_MODEL_da854bafea7a4d58bffc356169d1a008"}},"5ddb8320e20345ba8012be9dafabea74":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2edb38d38464962833190529b03be8c","placeholder":"​","style":"IPY_MODEL_32587214916549879fc0c7ac91b1c02c","value":"Downloading pytorch_model.bin: 100%"}},"4fcf1b6f59414b1ca8b7cc9255b0200b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6f53cae71564512b1c405873a9873de","max":495536138,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8a4c2e1f25994a939900e72d96f43bee","value":495536138}},"cdb67c115ae146d980dad8cad43d0d4e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4ccb47e4a41435d829d872eee3b4813","placeholder":"​","style":"IPY_MODEL_de6268b0464240cca875c9fb69b7e5b4","value":" 496M/496M [00:26&lt;00:00, 20.3MB/s]"}},"da854bafea7a4d58bffc356169d1a008":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2edb38d38464962833190529b03be8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32587214916549879fc0c7ac91b1c02c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6f53cae71564512b1c405873a9873de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a4c2e1f25994a939900e72d96f43bee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f4ccb47e4a41435d829d872eee3b4813":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de6268b0464240cca875c9fb69b7e5b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["- 참고 링크:\n","    - https://github.com/huggingface/transformers/blob/v4.33.0/src/transformers/models/bart/modeling_bart.py\n","    - https://huggingface.co/docs/transformers/model_doc/bart\n"],"metadata":{"id":"1v0d-Fgokax-"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ddPe3Bscf_kF","executionInfo":{"status":"ok","timestamp":1694346941715,"user_tz":-540,"elapsed":23936,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"310b6d15-c529-4eb0-f148-fb911ee86bca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install accelerate>=0.20.1\n","!pip install transformers\n","!pip install pytorch-lightning"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cLZmpC94gFJ8","executionInfo":{"status":"ok","timestamp":1694346964664,"user_tz":-540,"elapsed":22957,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"585ca4ac-dcc4-4dc6-a7c3-bcf692996f7e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.1\n","Collecting pytorch-lightning\n","  Downloading pytorch_lightning-2.0.8-py3-none-any.whl (727 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.0/727.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.0.1+cu118)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n","Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n","Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n","  Downloading torchmetrics-1.1.1-py3-none-any.whl (763 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.4/763.4 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n","Collecting lightning-utilities>=0.7.0 (from pytorch-lightning)\n","  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.31.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (16.0.6)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.3.0)\n","Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n","Successfully installed lightning-utilities-0.9.0 pytorch-lightning-2.0.8 torchmetrics-1.1.1\n"]}]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pc8rV7BXG1v7","executionInfo":{"status":"ok","timestamp":1694346964667,"user_tz":-540,"elapsed":37,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"2abfec72-55a7-4417-9ca2-463d3972abc5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Playdata_Python/final_project/transformers-4.33.0/src/transformers/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uLgYSwkAHly2","executionInfo":{"status":"ok","timestamp":1694346965750,"user_tz":-540,"elapsed":1098,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"777a7025-3f93-44c8-95e7-b87cae5c0203"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Playdata_Python/final_project/transformers-4.33.0/src/transformers\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MNQmsNzvICGK","executionInfo":{"status":"ok","timestamp":1694346966287,"user_tz":-540,"elapsed":541,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"3c413cf8-720a-4bc4-d34f-9fce5740dd83"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["'=0.20.1'\n"," activations.py\n"," activations_tf.py\n"," audio_utils.py\n"," benchmark\n"," commands\n"," configuration_utils.py\n"," convert_graph_to_onnx.py\n"," convert_pytorch_checkpoint_to_tf2.py\n"," convert_slow_tokenizer.py\n"," convert_slow_tokenizers_checkpoints_to_fast.py\n"," convert_tf_hub_seq_to_seq_bert_to_pytorch.py\n"," data\n"," debug_utils.py\n"," deepspeed.py\n"," dependency_versions_check.py\n"," dependency_versions_table.py\n"," dynamic_module_utils.py\n"," feature_extraction_sequence_utils.py\n"," feature_extraction_utils.py\n"," file_utils.py\n"," generation_flax_utils.py\n"," generation_tf_utils.py\n"," generation_utils.py\n"," hf_argparser.py\n"," hyperparameter_search.py\n"," image_processing_utils.py\n"," image_transforms.py\n"," image_utils.py\n"," __init__.py\n"," keras_callbacks.py\n"," modelcard.py\n"," modeling_flax_outputs.py\n"," modeling_flax_pytorch_utils.py\n"," modeling_flax_utils.py\n"," modeling_outputs.py\n"," modeling_tf_outputs.py\n"," modeling_tf_pytorch_utils.py\n"," modeling_tf_utils.py\n"," modeling_utils.py\n"," models\n"," onnx\n"," optimization.py\n"," optimization_tf.py\n"," pipelines\n"," processing_utils.py\n"," __pycache__\n"," pytorch_utils.py\n"," sagemaker\n"," testing_utils.py\n"," tf_utils.py\n"," time_series_utils.py\n"," tokenization_utils_base.py\n"," tokenization_utils_fast.py\n"," tokenization_utils.py\n"," tools\n"," trainer_callback.py\n"," trainer_pt_utils.py\n"," trainer.py\n"," trainer_seq2seq.py\n"," trainer_tf.py\n"," trainer_utils.py\n"," training_args.py\n"," training_args_seq2seq.py\n"," training_args_tf.py\n"," utils\n"]}]},{"cell_type":"code","source":["import copy\n","import math\n","import time\n","import warnings\n","from typing import List, Optional, Tuple, Union\n","from tqdm import tqdm\n","\n","import torch\n","import torch.utils.checkpoint\n","from torch import nn\n","from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n","from torch.utils.data import Dataset, DataLoader\n","\n","from transformers.activations import ACT2FN\n","from transformers.modeling_outputs import (\n","    BaseModelOutput,\n","    BaseModelOutputWithPastAndCrossAttentions,\n","    CausalLMOutputWithCrossAttentions,\n","    Seq2SeqLMOutput,\n","    Seq2SeqModelOutput,\n","    Seq2SeqQuestionAnsweringModelOutput,\n","    Seq2SeqSequenceClassifierOutput,\n",")\n","from transformers.modeling_utils import PreTrainedModel\n","from transformers.utils import (\n","    add_code_sample_docstrings,\n","    add_end_docstrings,\n","    add_start_docstrings,\n","    add_start_docstrings_to_model_forward,\n","    logging,\n","    replace_return_docstrings,\n",")\n","from transformers.models.bart.configuration_bart import BartConfig\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BartConfig, BartForConditionalGeneration\n","\n","import nltk\n","from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt"],"metadata":{"id":"JOmiZpuDgl4j","executionInfo":{"status":"ok","timestamp":1694355924884,"user_tz":-540,"elapsed":1652,"user":{"displayName":"David Choi","userId":"14870253108450991755"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["# logger = logging.get_logger(__name__)\n","\n","# _CHECKPOINT_FOR_DOC = \"facebook/bart-base\"\n","# _CONFIG_FOR_DOC = \"BartConfig\"\n","\n","# # Base model docstring\n","# _EXPECTED_OUTPUT_SHAPE = [1, 8, 768]\n","\n","# # SequenceClassification docstring\n","# _CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION = \"valhalla/bart-large-sst2\"\n","# _SEQ_CLASS_EXPECTED_LOSS = 0.0\n","# _SEQ_CLASS_EXPECTED_OUTPUT = \"'POSITIVE'\"\n","\n","# # QuestionAsnwering docstring\n","# _CHECKPOINT_FOR_QA = \"valhalla/bart-large-finetuned-squadv1\"\n","# _QA_EXPECTED_LOSS = 0.59\n","# _QA_EXPECTED_OUTPUT = \"' nice puppet'\"\n","\n","# BART_PRETRAINED_MODEL_ARCHIVE_LIST = [\n","#     \"facebook/bart-large\",\n","#     # see all BART models at https://huggingface.co/models?filter=bart\n","# ]"],"metadata":{"id":"eDi2gJwyTxw_","executionInfo":{"status":"ok","timestamp":1694346974563,"user_tz":-540,"elapsed":15,"user":{"displayName":"David Choi","userId":"14870253108450991755"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# 1. 데이터 불러오기"],"metadata":{"id":"DIKQqMBxfLnp"}},{"cell_type":"code","source":["dataset = pd.read_csv('/content/drive/MyDrive/Playdata_Python/final_project/data/summary_dataset/01.news_r.csv')\n","dataset.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4gk1ihGgFFJ","executionInfo":{"status":"ok","timestamp":1694347011486,"user_tz":-540,"elapsed":1357,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"b6e16975-1610-4006-fa27-55a9c3171dbe"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10800, 2)"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["# 데이터 분할\n","train_df = dataset[:7000]\n","valid_df = dataset[7000:8900]\n","test_df = dataset[8900:]\n","\n","train_df.shape, valid_df.shape, test_df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"627FFdQrhCJf","executionInfo":{"status":"ok","timestamp":1694358411837,"user_tz":-540,"elapsed":9,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"88d2caaf-2358-45b2-ab68-3db22386b700"},"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((7000, 2), (1900, 2), (1900, 2))"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","source":["input_train = train_df['passage']\n","target_train = train_df['summary1']\n","\n","input_valid = valid_df['passage']\n","target_valid = valid_df['summary1']\n","\n","input_test = test_df['passage']\n","target_test = test_df['summary1']\n","\n","input_train.shape, target_train.shape"],"metadata":{"id":"sdXNG_bQcLHJ","executionInfo":{"status":"ok","timestamp":1694358420149,"user_tz":-540,"elapsed":20,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"46fb07a9-1afe-47c2-b2b9-2499821e2176"},"execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((7000,), (7000,))"]},"metadata":{},"execution_count":85}]},{"cell_type":"markdown","source":["# 2. pretrained 모델 구조 파악"],"metadata":{"id":"T8LYrR9KgSMB"}},{"cell_type":"markdown","source":["### 2-1. pretrained 모델 및 토크나이저 불러오기"],"metadata":{"id":"m2dk3Jooaxcy"}},{"cell_type":"code","source":["model_name = 'hyunwoongko/kobart'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":215,"referenced_widgets":["06c77d9c51e84484a3c0622a38b9e3a6","401979aef41c462680332fe50282a014","8ea9e0b0f12046d8a9abe0537a963447","d1b9fa50d63045809077c748ba152336","66cc399b679f47e3b25a65f89589c988","995800c9277247e7b40df1f9a8a350a0","6096774c7c2e44f0803d020dc3cd4446","0dedb0da050b48349934b921fa776cff","973c37907fa94a22b98f1e31e3d96b79","7412309bae6147d3a644b464274d68a3","add09282be354f239de3f2541ee6836f","4df1ed0020594fef9e9731f9dc261963","16b327ee297a4505a89194f90435816e","5dcbb2bfc7014daa9f215b7736173392","36cfa3f4cddb4657bd04bf3745bffe21","9c491fc18aae4b7492ddb46d8a0f784f","31e6d404f7a3435f9ce45a87f14ca8b9","31a2b8dca2144b8cb82b491947cf4a02","16f1abf71fa942b8bf24ba57f251f178","b8622f2e398243e3a82bd0efd9b5bea5","5f30be4d19f9402a83f07f015982fef9","a47bcd08fdff45e18e352bea54c07d76","77baa506c3954942a4a96288a011b4cd","a57e6d22b61249d38e78792b427e312d","01bdd49fb3ec4b5cb61fe2571758c37b","b96bd1fc4b2e4ab4b279779798737304","40904b0f19c04cc8abdb4b0e494cd374","d7a0ce907f2c4b32a8f973ee46a4100f","06538d8f2d7c47e99c79d6200cf3d125","1ed887f532f641efb55d38516e8fa76f","ba413ca4226241c0bb282e9a07f7388b","1563781090244bcda80ff9d68087de08","74618eef457f47df9ef285ed69c11b5a","3e0e779d63064b1b87045cfe3eb3f1f8","cc50364d382945e0a46ea043e8f868df","ee51b708040848ecbf3ef1233c76a822","e1283b67890545a4a8a35491ec8a310d","c77393ffc6ec4c908745769a7f9d6913","ee6b274d756c48d999b8f9f34c5ff819","129edab252ea482ab12bc6db0d7915b7","9e3e1640d88f4a2d8e7e3bc7a0948bf6","c205afabcf3e4ced8b3d2384d6e75e94","6c143a85988f4c669c7268d02a1518bb","6c7282c6319c418ba045ddae1900dad0","6b732af789ce48b48aab6a083bc36a96","5ddb8320e20345ba8012be9dafabea74","4fcf1b6f59414b1ca8b7cc9255b0200b","cdb67c115ae146d980dad8cad43d0d4e","da854bafea7a4d58bffc356169d1a008","e2edb38d38464962833190529b03be8c","32587214916549879fc0c7ac91b1c02c","e6f53cae71564512b1c405873a9873de","8a4c2e1f25994a939900e72d96f43bee","f4ccb47e4a41435d829d872eee3b4813","de6268b0464240cca875c9fb69b7e5b4"]},"id":"ajrcy5HUgFAv","executionInfo":{"status":"ok","timestamp":1694347009698,"user_tz":-540,"elapsed":35148,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"dbf57300-73ef-4986-c7f1-10bd2157b6a5"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/337 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06c77d9c51e84484a3c0622a38b9e3a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4df1ed0020594fef9e9731f9dc261963"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/109 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77baa506c3954942a4a96288a011b4cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e0e779d63064b1b87045cfe3eb3f1f8"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/496M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b732af789ce48b48aab6a083bc36a96"}},"metadata":{}}]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7-ualkegE8I","executionInfo":{"status":"ok","timestamp":1694355028042,"user_tz":-540,"elapsed":454,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"1640922c-e243-4c09-b1be-b081ed3a2b3d"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BartForConditionalGeneration(\n","  (model): BartModel(\n","    (shared): Embedding(30000, 768, padding_idx=3)\n","    (encoder): BartEncoder(\n","      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n","      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n","      (layers): ModuleList(\n","        (0-5): 6 x BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (decoder): BartDecoder(\n","      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n","      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n","      (layers): ModuleList(\n","        (0-5): 6 x BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",")"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["### 2-2. pretrained 모델 성능 테스트"],"metadata":{"id":"c_nR4m42a14c"}},{"cell_type":"code","source":["original_sen_list = input_test.tolist()  # 신문 원본\n","summary_sen_list = target_test.tolist()  # 신문 요약본\n","\n","original_sen_list[:2], summary_sen_list[:2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"56mVc0NTa7hS","executionInfo":{"status":"ok","timestamp":1694355211753,"user_tz":-540,"elapsed":12,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"41dbeb7d-92db-4f7a-ee7c-690797a8b21b"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['정은경 질병관리청장과 봉준호 감독이 미국의 시사주간지 타임(Time)이 선정하는 ‘2020 세계에서 가장 영향력 있는 100인’ 명단에 나란히 이름을 올렸다. 청와대 관계자는 23일 “이번 선정은 K방역이 전 세계가 본받아야 할 모범으로 인정받았다는 점을 확인해주는 데 의미가 있다”며 “방역과 관련해 뛰어난 성과와 업적을 보인다는 점에서 정 청장을 선정한 것”이라고 말했다. 정 청장은 리더스 부분에 등재됐다. 아티스트 부분에 이름을 올린 봉 감독에 대해서도 “매우 기쁜 소식이며 축하의 말씀을 전한다”고 했다. 특히 문재인 대통령은 정 청장에 대한 소개글을 타임에 전달했다. 선정된 100인 소개글 가운데 현직 대통령의 글은 문 대통령이 유일하다.  문 대통령은 소개글에서 “코로나19 팬데믹 상황에서 한국의 방역은 세계의 모범이 됐고, 정 청장은 방역의 최전방에서 국민과 진솔하게 소통해 K방역을 성공으로 이끌었다”고 적었다. 이어 “한국에 첫 확진자가 발생했을 때 정 청장은 정부를 대표해 국민 앞에 섰고 매일 투명하게 상황을 발표했다”며 “질병관리청 최초의 여성 수장으로서 코로나 발생 6개월 전부터 ‘원인불명 집단감염 대응절차’ 매뉴얼을 마련하는 등 질병관리청을 준비된 조직으로 이끌었다”고 평가했다. 문 대통령은 알베르 카뮈의 소설 『페스트』에 등장하는 ‘페스트와 싸우는 유일한 방법은 성실성’이라는 문구를 인용하며 “정 청장의 성실성이야말로 세계 곳곳에서 코로나와 맞서는 수많은 ‘정은경’들에게, 그리고 포스트 코로나 시대를 연 인류 모두에게 영감을 주는 얘기”라고 강조했다. 앞서 문 대통령은 지난 11일 충북 청주에 있는 질병관리본부(질본)를 찾아 정 청장에게 임명장을 수여했다. 정 청장은 차관급이다. 이번 정부 들어 장ㆍ차관급을 통틀어 문 대통령이 현장을 찾아 임명식을 진행한 것은 이때가 처음이다. 이와 관련 청와대 관계자는 “전쟁 중에 야전사령관을 불러 임명장을 주는 것이 아닌 직접 가서 임명장을 드리는 것이기 때문에 초대 청장에 대한 신뢰와 기대 의미를 갖고 있다”고 설명했다. 문 대통령은 지난 2월 20일 영화 ‘기생충’으로 아카데미 작품상 등 4관왕을 차지한 봉준호 감독과 출연진ㆍ제작진을 청와대로 초청해 오찬을 했었다. 당시 오찬 메뉴는 김정숙 여사가 요리한 ‘짜파구리’였다. 봉 감독의 소개글은 영화 ‘설국열차’에 출연했던 배우 틸다 시윈튼이 작성했다.',\n","  '창업가는 기존에 없던 방식으로 고객이 절실하게 필요로 하는 제품을 만들고 개선함으로써 혁신을 만든다. 그리고 그런 제품을 만들기 위해 창업가 스스로가 마치 인간을 탐구하는 학자처럼 직접적인 고객 관찰과 공감 과정을 반드시 거쳐야 한다. 집에서, 학교에서, 직장에서 고객이 어떤 행동을 하고 살고 있으며, 어느 부분에서 짜증과 고통을 느끼는지를 모두 파악하는 것이다. 바로 이 짜증과 고통의 부분을 해결하면서 혁신이 시작된다. 잘못된 문제에 집중하지 말라자기중심적으로 사고하며 고객도 창업가와 같은 심각한 문제를 갖고 해결하고 싶어할 것이라 믿는 바람에 실패하는 경우를 목격한다. 어떤 손실을 감내하더라도 자기만족을 위해 창업하는 것이라면 전혀 문제가 될 것이 없지만, 그렇지 않은 경우 자기중심적 사고는 창업가에게 참으로 위험한 자세라고 보인다. “창업가는 이타적 태도를 갖는 것이 중요하다. 타인인 고객을 이롭게 하겠다는 열정이 있어야 고객의 문제를 해결하는 제품을 만들 수 있기 때문”이라는 프라이머 권도균 대표의 말을 곱씹어 볼 필요가 있다. 고객이 직접 표현하지 못한 문제까지도 파악하기 위해 고객의 실제 행동에 대한 세심한 관찰을 통해 얻은 데이터를 분석해야 한다. 예를 들어 갈비탕에 나오는 갈비가 너무 커 먹기 힘들다고 고객이 반응했다고 하자. 갈비를 작게 잘라서 갈비탕을 내놓는 방법을 선택했으나 고객 반응이 예전 같지 않다면 더 심도 있는 고객관찰을 하고 큰 갈비를 쉽게 집어 잘 뜯어 먹을 수 있는 도구를 만들어 제공해야 한다. 중요한 고객 문제 하나를 선택한 후 이를 해결하기 위한 다양한 아이디어의 분출은 무한한 가능성의 시작이 된다. 문제 발견 후 단 하나의 해결책을 성급하게 도출시키려 하지 말고, 다양한 문제 해결 방법에 대한 아이디어를 쏟아 내도록 노력하자. 최고의 해결책을 만들기 위해 필요하다면 급격히 회사의 체질과 방향을 바꿀 수도 있어야 한다. 고객과 우리가 원하는 것의 교차점 시제품 제작이 용이한 아이디어부터 만들고 이를 고객에게 선보여 그들의 반응을 관찰하면서 더 많은 영감을 얻는 데 집중한다. 시제품에 대한 고객의 솔직한 반응을 얻기 위해서는 고객이 쉽게 인지할 수 있도록 최대한 간단해야 하며, 우리가 시제품을 통해 구현하고자 하는 것이 무엇인지를 솔직하게 전달하는 것이 필요하다. 시제품 자체가 새로운 해결책을 구현한 것이기 때문에 이를 처음 본 고객이 스타트업의 의도와 전혀 다른 방식으로 사용하게 되는 경우를 심심찮게 목격할 가능성도 크며, 이를 통해 더욱 성공적인 제품 개발로 이어질 수도 있다.'],\n"," ['정 청장과 봉 감독이 2020 세계에서 가장 영향력 있는 100인 명단에 오르자 청와대 관계자는 K방역이 모범으로 인정받았음을 확인했다고 말하며 봉 감독에게는 축하를 전했다.',\n","  '인간을 탐구하는 학자와 같이 창업가는 고객이 집 등에서 어떤 행동을 하는지 관찰하고 어떤 부분에서 짜증을 느끼는지 공감하는 과정을 통해 기존에 없던 방식으로 혁신을 만든다.'])"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["# 한글 텍스트를 토큰화하고 요약된 결과를 리스트에 저장하는 테스트\n","summary_sen_by_model_list = []\n","\n","for original_sen in original_sen_list[:20]:\n","    # 문장을 토큰화\n","    input_ids = tokenizer.encode(original_sen, return_tensors=\"pt\")\n","\n","    # 모델을 사용하여 요약 생성\n","    summary_ids = model.generate(input_ids, max_length=150, num_beams=4, length_penalty=2.0, early_stopping=True)\n","\n","    # 요약 결과를 디코딩하여 텍스트로 변환\n","    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","\n","    summary_sen_by_model_list.append(summary_text)\n","\n","summary_sen_by_model_list[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pKJTHxRfa7d6","executionInfo":{"status":"ok","timestamp":1694357100190,"user_tz":-540,"elapsed":81495,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"cbb9b16a-a060-457f-dfee-2bc623141e7c"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['했다. 정은경 질병관리청장과 봉준호 감독이 미국의 시사주간지 타임(Time)이 선정하는 ‘2020 세계에서 가장 영향력 있는 100인’ 명단에 나란히 이름을 올렸다. 청와대 관계자는 23일 “이번 선정은 K방역이 전 세계가 본받아야 할 모범으로 인정받았다는 점을 확인해주는 데 의미가 있다”며 “방역과 관련해 뛰어난 성과와 업적을 보인다는 점에서 정 청장을 선정한 것”이라고 말했다. 정 청장은 리더스 부분에 등재됐다. 아티스트 부분에 이름을 올린 봉 감독에 대해서도 “매우 기쁜 소식이며 축하의 말씀을 전한다”고 했다. 특히 문재인 대통령은 정 청장에 대한 소개글을 타임에 전달했다. 선정된 100인 소개글 가운데 현직 대통령의 글은 문 대통령이 유일하다.  문 대통령은 소개글',\n"," '있다.\\n 창업가는 기존에 없던 방식으로 고객이 절실하게 필요로 하는 제품을 만들고 개선함으로써 혁신을 만든다. 그리고 그런 제품을 만들기 위해 창업가 스스로가 마치 인간을 탐구하는 학자처럼 직접적인 고객 관찰과 공감 과정을 반드시 거쳐야 한다. 집에서, 학교에서, 직장에서 고객이 어떤 행동을 하고 살고 있으며, 어느 부분에서 짜증과 고통을 느끼는지를 모두 파악하는 것이다. 바로 이 짜증과 고통의 부분을 해결하면서 혁신이 시작된다. 잘못된 문제에 집중하지 말라자기중심적으로 사고하며 고객도 창업가와 같은 심각한 문제를 갖고 해결하고 싶어할 것이라 믿는 바람에 실패하는 경우를 목격한다. 어떤 손실을 감내하더라도 자기만족을 위해 창업하는 것이라면 전혀 문제가 될 것이 없지만, 그렇지 않은 경우 자기중심적 사고는 창업가에게 참으로 위험한 자세라고 보인',\n"," '',\n"," '했다. 기대가 너무 큰 탓이었을까. 소문난 잔치엔 먹을 게 없었다. 미국의 전기자동차 기업 테슬라가 22일(현지시간) 개최한 주주총회 겸 ‘배터리 데이’ 얘기다. 테슬라 최고경영자(CEO) 일론 머스크가 지난 4월 “테슬라 역사상 최고로 신나는 날이 될 것”이라고 트윗을 날린 뒤 배터리 데이는 테슬라의 주가 상승을 견인한 호재 중 하나로 꼽혀왔다. 뚜껑을 열어보니 정반대였다.  테슬라 주가는 이날 미국 나스닥(NASDAQ) 증시에서 5. 6% 하락하며 전일 대비 25. 16달러 하락한 424. 23달러로 장을 마감했다. 폐장 후엔 배터리 데이에 대한 실망감이 커지면서',\n"," \"있다.\\n 이달 초 은행권을 긴장하게 했던 '은행원 셀프대출' 사태가 지난해와 2017년에도 각각 적발된 것으로 드러났다. 금융감독원이 부문검사를 통해 이들 사례를 적발해 시정 조치할 것을 지적했음에도, 은행권의 임직원 가족 대출 관련 내부통제 기준엔 여전히 구멍이 적지 않다. '기은 셀프대출' 유사 지적 5년간 2건윤두현 국민의힘 의원실이 23일 금융감독원으로부터 받은 '대출 취급의 적정성 조사' 자료에 따르면 은행원이 은행의 허술한 내부통제 기준을 이용해 자기 가족에게 부당대출을 실행했다가 지적받은 사례가 지난 5년간 2건 더 있었던 것으로 드러났다. 최근 은행권에서 논란이 된 '기업은행 셀프대출' 사태와 비슷한 사례다. 기업은행 셀프대출 사태란 기업은행\"]"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["# BLEU 점수 계산\n","nltk.download('punkt')\n","\n","# 문장을 토큰 리스트로 변환하는 함수\n","def tokenize_sentence(sentence):\n","    return nltk.word_tokenize(sentence)\n","\n","num_sentences = 20  # 20개로 test\n","total_belu_score = []\n","\n","for i in range(num_sentences):\n","\n","    # Reference 문장과 Candidate 문장 토큰화\n","    reference_tokens = tokenize_sentence(summary_sen_list[i])\n","    candidate_tokens = tokenize_sentence(summary_sen_by_model_list[i])\n","\n","    # BLEU Score 계산\n","    bleu_score = sentence_bleu([reference_tokens], candidate_tokens, weights=(1, 0, 0, 0))  # (원본 번역문, 모델 번역문)\n","    total_belu_score.append(bleu_score)\n","\n","    if i < 20:\n","        # BLEU 점수 출력\n","        print(f\"{i}번째 | 원본 번역본: {summary_sen_list[i]}\")\n","        print(f\"{i}번째 | 모델 번역본: {summary_sen_by_model_list[i]}\")\n","        print(f\"{i}번째 | BLEU 점수: {bleu_score}\")\n","        print(\"----------------------------------------------------------------------------------------------------\")\n","    else:\n","        pass\n","\n","# 전체 blue 평균 계산\n","average_bleu_score = sum(total_belu_score) / num_sentences\n","print(f\"100문장에 대한 Belu Score 평균: {average_bleu_score}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dfh1yZgVasJw","executionInfo":{"status":"ok","timestamp":1694357241845,"user_tz":-540,"elapsed":1611,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"1f1448ab-6c15-4afe-b248-37213ea9b2f3"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["0번째 | 원본 번역본: 정 청장과 봉 감독이 2020 세계에서 가장 영향력 있는 100인 명단에 오르자 청와대 관계자는 K방역이 모범으로 인정받았음을 확인했다고 말하며 봉 감독에게는 축하를 전했다.\n","0번째 | 모델 번역본: 했다. 정은경 질병관리청장과 봉준호 감독이 미국의 시사주간지 타임(Time)이 선정하는 ‘2020 세계에서 가장 영향력 있는 100인’ 명단에 나란히 이름을 올렸다. 청와대 관계자는 23일 “이번 선정은 K방역이 전 세계가 본받아야 할 모범으로 인정받았다는 점을 확인해주는 데 의미가 있다”며 “방역과 관련해 뛰어난 성과와 업적을 보인다는 점에서 정 청장을 선정한 것”이라고 말했다. 정 청장은 리더스 부분에 등재됐다. 아티스트 부분에 이름을 올린 봉 감독에 대해서도 “매우 기쁜 소식이며 축하의 말씀을 전한다”고 했다. 특히 문재인 대통령은 정 청장에 대한 소개글을 타임에 전달했다. 선정된 100인 소개글 가운데 현직 대통령의 글은 문 대통령이 유일하다.  문 대통령은 소개글\n","0번째 | BLEU 점수: 0.13513513513513517\n","----------------------------------------------------------------------------------------------------\n","1번째 | 원본 번역본: 인간을 탐구하는 학자와 같이 창업가는 고객이 집 등에서 어떤 행동을 하는지 관찰하고 어떤 부분에서 짜증을 느끼는지 공감하는 과정을 통해 기존에 없던 방식으로 혁신을 만든다.\n","1번째 | 모델 번역본: 있다.\n"," 창업가는 기존에 없던 방식으로 고객이 절실하게 필요로 하는 제품을 만들고 개선함으로써 혁신을 만든다. 그리고 그런 제품을 만들기 위해 창업가 스스로가 마치 인간을 탐구하는 학자처럼 직접적인 고객 관찰과 공감 과정을 반드시 거쳐야 한다. 집에서, 학교에서, 직장에서 고객이 어떤 행동을 하고 살고 있으며, 어느 부분에서 짜증과 고통을 느끼는지를 모두 파악하는 것이다. 바로 이 짜증과 고통의 부분을 해결하면서 혁신이 시작된다. 잘못된 문제에 집중하지 말라자기중심적으로 사고하며 고객도 창업가와 같은 심각한 문제를 갖고 해결하고 싶어할 것이라 믿는 바람에 실패하는 경우를 목격한다. 어떤 손실을 감내하더라도 자기만족을 위해 창업하는 것이라면 전혀 문제가 될 것이 없지만, 그렇지 않은 경우 자기중심적 사고는 창업가에게 참으로 위험한 자세라고 보인\n","1번째 | BLEU 점수: 0.13761467889908258\n","----------------------------------------------------------------------------------------------------\n","2번째 | 원본 번역본: 프랜차이즈 본사가 광고 · 판촉 행사를 하려면 미리 일정 비율 이상의 가맹점주 동의를 받아야 하며 신규 프랜차이즈 본부가 가맹점을 모집하려면 직영점을 1년 이상 운영해야 한다.\n","2번째 | 모델 번역본: \n","2번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","3번째 | 원본 번역본: 테슬라 최고 경영자가 테슬라 역사상 신나는 날이 될 것 이라고 트윗을 날린 뒤 배터리 데이는 테슬라 주가 상승을 견인한 호재 중 하나로 꼽았지만 뚜껑을 열어보니 정반대였다.\n","3번째 | 모델 번역본: 했다. 기대가 너무 큰 탓이었을까. 소문난 잔치엔 먹을 게 없었다. 미국의 전기자동차 기업 테슬라가 22일(현지시간) 개최한 주주총회 겸 ‘배터리 데이’ 얘기다. 테슬라 최고경영자(CEO) 일론 머스크가 지난 4월 “테슬라 역사상 최고로 신나는 날이 될 것”이라고 트윗을 날린 뒤 배터리 데이는 테슬라의 주가 상승을 견인한 호재 중 하나로 꼽혀왔다. 뚜껑을 열어보니 정반대였다.  테슬라 주가는 이날 미국 나스닥(NASDAQ) 증시에서 5. 6% 하락하며 전일 대비 25. 16달러 하락한 424. 23달러로 장을 마감했다. 폐장 후엔 배터리 데이에 대한 실망감이 커지면서\n","3번째 | BLEU 점수: 0.24\n","----------------------------------------------------------------------------------------------------\n","4번째 | 원본 번역본: 기업은행 셀프대출을 통해 기업은행 A차장은 경기도 화성 일대 오피스텔 연립주택을 사들인 뒤 평가차익을 올렸고 이를 적발한 기업은행은 A차장을 면직 처분했다.\n","4번째 | 모델 번역본: 있다.\n"," 이달 초 은행권을 긴장하게 했던 '은행원 셀프대출' 사태가 지난해와 2017년에도 각각 적발된 것으로 드러났다. 금융감독원이 부문검사를 통해 이들 사례를 적발해 시정 조치할 것을 지적했음에도, 은행권의 임직원 가족 대출 관련 내부통제 기준엔 여전히 구멍이 적지 않다. '기은 셀프대출' 유사 지적 5년간 2건윤두현 국민의힘 의원실이 23일 금융감독원으로부터 받은 '대출 취급의 적정성 조사' 자료에 따르면 은행원이 은행의 허술한 내부통제 기준을 이용해 자기 가족에게 부당대출을 실행했다가 지적받은 사례가 지난 5년간 2건 더 있었던 것으로 드러났다. 최근 은행권에서 논란이 된 '기업은행 셀프대출' 사태와 비슷한 사례다. 기업은행 셀프대출 사태란 기업은행\n","4번째 | BLEU 점수: 0.04210526315789473\n","----------------------------------------------------------------------------------------------------\n","5번째 | 원본 번역본: 독감 백신 중단 사태에 백신에 대한 불신이 퍼지며 유통업체 선정 또한 늦어져 정부 책임론이 나오는 등 후폭풍이 거세다.\n","5번째 | 모델 번역본: 있다.\n"," 초유의 독감 백신 중단 사태에 따른 후폭풍이 거세다. 유· 무료를 떠나 독감 백신 전반으로 불신이 퍼지는 분위기다. 백신 조달을 담당한 신성약품이 올해 처음 정부 백신 사업에 참여한 배경을 두고도 각종 의혹이 난무한다. 이런 가운데 접종 시기를 전년보다 한 달가량 앞당겼는데도 공급을 담당할 유통업체 선정은 두 달 가까이 늦어진 점을 두고 정부 책임론도 나온다. 저가 입찰이 영향을 미쳐 업체 선정에 시간이 더 걸렸고 결국 업체의 유통 역량을 제대로 검증하지 못한 채 사업을 진행하다 문제가 생긴 것 아니냐는 얘기다. 23일 조달청 ‘나라장터’(공공기관 물자 조달 시스템)에 따르면 올해 국가예방접종을 위한 인플루엔자 백\n","5번째 | BLEU 점수: 0.09803921568627452\n","----------------------------------------------------------------------------------------------------\n","6번째 | 원본 번역본: 건조일수가 늘어나 산불 위험이 증가했으며 권 박사는 대형 산불의 안전지대는 없다고 경고했다.\n","6번째 | 모델 번역본: 했다. 올해 올해 러시아 시베리아, 미국 캘리포니아, 오스트레일리아 등 세계 곳곳에서 대형 산불이 발생했다. 바싹 말라붙은 산에서 작은 불씨만 남아 있어도 다시 살아나는 '좀비 파이어'가 수개월간 지속됐다. 한국은 대형 산불이나 '좀비 파이어'에서 안전할까. 100ha 이상을 태우고 24시간 이상 지속되는 산불을 ‘대형산불’로 분류하는데, 우리나라도 최근에는 1년에 2~3건씩 발생하고 있다. 국립산림과학원의 권춘근 박사 연구팀은 2018년까지의 자료를 토대로 우리나라의 산불 발생위험을 분석했다. 기온이 상승하고 건조일수가 늘어나 산불 발생위험이 전반적으로 증가했다. 남부지역보다 중부지역의 산불 위험이 더 크게 늘어났다. 권 박사는 “종전과\n","6번째 | BLEU 점수: 0.08510638297872339\n","----------------------------------------------------------------------------------------------------\n","7번째 | 원본 번역본: 고용부는 코로나19로 인해 증가한 무급 휴직자를 위해 고용유지 지원금 지급 조건의 무급 휴직 기간을 30일 이상으로 완화시켰다.\n","7번째 | 모델 번역본: 있다.\n"," 지급 인원(20만 명)이 정해져 있어서다. 1기준 중위소득 60% 이하 저소득층이나 북한 이탈주민과 같은 특정 취약계층 2정부 청년구직지원 프로그램 지난해 참여자 3구직지원 프로그램에 올해 참여 또는 참여 예정자 순으로 대상자가 정해진다. 무급 휴직자, 고용유지지원금 지급 요건 완화 일자리는 지켰지만 월급을 받지 못하는 무급 휴직자도 코로나19로 많이 늘었다. 고용부는 이에 대한 지원 제도를 확대한다. 지난 22일 국무회의에서 의결된 고용보험법 시행령 개정안에 따르면 고용유지지원금 지급 요건이 되는 무급 휴직 기간이 90일 이상에서 30일 이상으로 완화된다. 앞으로는 한 달만 무급 휴직을 해도 정부 지원금이 나간다는 의미\n","7번째 | BLEU 점수: 0.09473684210526317\n","----------------------------------------------------------------------------------------------------\n","8번째 | 원본 번역본: 정부는 전국민 고용보험을 추진하고 있고 예술인과 다양한 형태의 종사자들도 고용보험에 가입할 수 있도록 하는 법 개정안을 제출한 바 있다.\n","8번째 | 모델 번역본: 있다.\n"," 우리는 예상하지 못한 위험에 대비하기 위해 보험이라는 제도를 이용한다. 고용보험은 민간보험과는 성격이 다른 공적 사회보험으로 근로자들이 일하는 동안 노사가 보험료를 분담하여 적립해 두었다가 경제 위기 등으로 일자리 유지가 어려울 때 도움을 받기 위한 제도이다. 우선 현재 일하고 있는 사업장에서 최대한 고용을 유지할 수 있도록 휴업이나 휴직 시에 근로자의 임금 일부를 지원한다. 실제로 이번 코로나19 위기 시에도 6만3000개 사업장에서 1조3407억원의 고용유지지원금을 통해 65만 명이 넘는 일자리를 유지할 수 있었다. 고용 유지를 위해 최대한 노력했음에도 불가피하게 일자리를 잃게 되는 경우에는 실직 기간 동안 생계를 유지할 수 있도록 구직급여를 받으며 다음 일자리를 준비할 수 있는 취업지원서비스\n","8번째 | BLEU 점수: 0.0404040404040404\n","----------------------------------------------------------------------------------------------------\n","9번째 | 원본 번역본: 지상파 예능에서 부동산 전문가와 현장을 답사하며 알짜 부동산 정보를 제공하는 것에 대해 전문가들은 부동산에 대한 대중들의 관심을 방송이 반영한 것으로 본다.\n","9번째 | 모델 번역본: 거예요. 조영구=국제업무지구를 발표했을 때 3억 가던 아파트가 10억까지 갔어요 평당 1억5000, 2억까지 갔어요. (국제업무지구 재추진을) 다시 발표하니까 13억으로 갔어요. 국제업무지구가 들어왔으면 좋겠어요. 권영세 국회의원(서울 용산)=경주마를 수레를 끌게 하면 되겠습니까. 이유리=그래서 언제 개발하는 거에요? 시원하게 말씀해주세요. 권영세=최소 5년 내에는 시작합니다. 화면 상단에는 ‘개발 호재’ ‘재개발 기대로 뭉칫돈이 잠자는 동네’ 등 용산의 부동산 가치를 소개하는 자막들로 채워졌다. 11일 방송된 MBC 파일럿 예능 ‘돈벌래’의 내용 중 일부다. 제목\n","9번째 | BLEU 점수: 0.022471910112359546\n","----------------------------------------------------------------------------------------------------\n","10번째 | 원본 번역본: 서울의 한 중개사는 과거와 달리 부동산에 대한 투자적 관심이 높아졌다고 전한 가운데 부동산 예능이 지상파에서 본격화되는 것에 대한 우려의 시선도 있다.\n","10번째 | 모델 번역본: 있다.\n"," 서울 강남의 한 부동산 중개사는 “ 부동산에 대한 열기가 과거와는 또 다르다. 30대도 그룹을 만들어 지방까지 부동산 임장(현장 답사)을 다니며 꼼꼼하게 따진다. 부동산에 대한 투자적 관심이 한 차원 높아진 분위기”라며 이같은 열기를 전했다. 하지만 지상파에서 부동산 예능을 본격화하는 것에 대한 우려의 시선도 있다. ‘돈벌래’는 방송 직후부터 각종 온라인 커뮤니티에서 도마 위에 올랐다. 서울과 수도권의 급격한 집값 상승이 큰 사회적 문제로 대두된 시기에 지상파에서 부동산에 대한 투자 '팁'을 집중적으로 부각하는 것이 적절하냐는 지적이다. 이에 대해 이선경(32·대학원생)씨는 “노골적으로 특정 지역과 아파트를 거론하면서 어디가 투자하면 좋\n","10번째 | BLEU 점수: 0.12962962962962962\n","----------------------------------------------------------------------------------------------------\n","11번째 | 원본 번역본: A 씨의 자가격리 위반 갈등은 코로나19 확진 판정을 받으면서 불거졌는데 확진 판정을 받은 후 나흘 동안 순천의 장례식에 머문 사실을 순천시가 알게 됐기 때문이다.\n","11번째 | 모델 번역본: 했다. 순천시, 부산시 북구 구상권 청구 왜?자가격리 기간에 전남 순천시를 방문한 부산 383번 신종 코로나바이러스 감염증(코로나19) 확진자의 일탈로 빚어진 지방자치단체 간 갈등이 구상권 청구에 이어 진실공방 양상으로 번지고 있다. 23일 전남 순천시에 따르면 부산 383번 확진자인 60대 남성 A씨가 지난 16일부터 19일까지 순천의 한 장례식장에서 4일간 머물렀다. 부산시 북구보건소는 지난 17일 A씨에게 자가격리 대상자라는 사실을 통보했지만, A씨는 가족의 장례를 다 치른 뒤에야 부산으로 돌아간 것으로 알려졌다. A씨의 자가격리 위반 갈등은 그가 지난 21일 코로나19 확진 판정을 받으면서 불거졌다. 순천\n","11번째 | BLEU 점수: 0.1098901098901099\n","----------------------------------------------------------------------------------------------------\n","12번째 | 원본 번역본: 송도에 입주해 있는 벤쳐기업은 인천경제자유구역청과 르호봇비즈니스인큐베이터의 업무협약 체결 덕분에 르호봇에서 유연하게 연구 및 사무 공간을 확보할 수 있게 되었다.\n","12번째 | 모델 번역본: 있다. 인천 송도가 바이오클러스터 도약을 위한 날개를 달고 K- 바이오를 선도하고 있다. 최근 인천경제자유구역청은 바이오 특화 공유 공유시설과 공유오피스를 제공하는 (주)르호봇비즈니스인큐베이터와 ‘송도바이오클러스터 연구 개발과 산업 생태계 활성화를 위한 업무 협약’을 체결했다. 협약은 바이오 기업들의 유치 활동에 함께 노력한다는 것이 골자다. 송도 IBS 타워 24층에 이달 말 문을 열 예정인 르호봇의 송도 바이오공유센터에는 총 1620m2 규모에 멸균기·무균실험대 등 바이오 연구시설 및 장비를 공유하는 드라이랩 센터가 구축돼 경쟁력 있는 서비스를 제공할 예정이다. 또 회의실 등 다양한 공간도 함께 사용할 수 있는\n","12번째 | BLEU 점수: 0.05555555555555554\n","----------------------------------------------------------------------------------------------------\n","13번째 | 원본 번역본: 코로나19 재확산에 따라 사회적 거리 두기가 강화되자 홍 구청장은 한발 앞선 행정 추진으로 민생 현장을 적극적으로 챙기고 있다.\n","13번째 | 모델 번역본: \n","13번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","14번째 | 원본 번역본: 더불어민주당이 국제 법제사법위 1소위에서 김 의원이 대표발의한 공수처법 개정안을 기습 상정하면서 회의가 파행되었다.\n","14번째 | 모델 번역본: 했다. 더불어민주당이 23일 국회 법제사법위 1소위를 열어 고위공직자범죄수사처(공수처)법 개정안을 기습 상정하면서 회의가 파행되는 소동이 있었다. 이 때문에 뒤이어 예정됐던 법사위 전체회의도 1시간가량 지연됐다. 김용민 민주당 의원이 대표발의한 공수처법 개정안은 공수처장 후보자 추천위원과 관련해 ‘여야 교섭단체 각 2명 선임’이란 조항을 ‘국회 추천 4인’으로 바꾸는 게 골자다. 후보자 추천위원 선정 단계에서 제1야당에 보장했던 처장 후보자 거부권을 무력화하려는 것이다. 민주당은 현행 공수처법상 공수처 출범 시점(7월 15일)이 두 달 넘게 지연되고 있는 점을 법 개정의 필요성으로 짚고\n","14번째 | BLEU 점수: 0.10869565217391305\n","----------------------------------------------------------------------------------------------------\n","15번째 | 원본 번역본: 문 특보는 국내 언론 기고에서 워싱턴의 이분법적 외교정책을 비판하는가 하면 중국 외교는 패도 내지 강건 외교라고 미국과 중국을 동시에 비판했다.\n","15번째 | 모델 번역본: 유엔의 대북 제재를 풀자면 미국과의 협력은 절대적이다. 이는 부정할 수 없는 현실이다. ” 폼페이오 미국 국무장관이 시진핑 중국 주석을 일러 ‘파산한 전체주의 이데올로기의 진짜 신봉자’라고 공격하는 등 미국과 중국이 신냉전으로 돌입하는 모습이다. “미국의 대중국 비판에는 두 가지 해석이 있다. 하나는 11월 대선을 앞둔 한시적·전술적 대응이라는 해석이고, 다른 하나는 초강대국 미국의 예외주의와 우월주의에 기초한 장기적 전략 포석이라는 해석이다. 미국은 공화당·민주당 할 것 없이 중국에 대한 견제심리가 대단하므로 민주당이 집권한다고 해도 신냉전까지는 아니더라도 미·중 관계는 상당히 냉각될 것이다. ” 국내 언론 기고에서 문\n","15번째 | BLEU 점수: 0.07608695652173914\n","----------------------------------------------------------------------------------------------------\n","16번째 | 원본 번역본: 분광요소법이 효율성이 있는지 아니면 쇠퇴하고 있는 방식인지에 대해 각각의 주장이 맞서고 있다.\n","16번째 | 모델 번역본: 있다.\n"," 이름에서 보듯, 컴퓨터가 물리량을 계산하는 기본 단위가 3차원 입체(volume)다. 각 단위들을 종합해 전체 대기 운동을 예측할 때도 오류가 상대적으로 적게 나온다. 그 덕분에 다른 역학코어 방식에 비해 계산속도가 빠르다는 장점이 있다. 기상청은 제한된 시간 내에 예보를 내야 하는 만큼, 계산속도는 어떤 장점보다도 중요한 미덕으로 꼽을 수 있다. 이 교수는 지난해 국감에서 유럽중기예보센터(ECMWF) 발간자료의 한 대목을 인용했다. 다음은 지난해 10월 7일 국정감사 당시 이 교수가 김 청장을 상대로 한 질의 내용이다. 이상돈_ 청장, (엊그제 제 방에 와서 티타임 하면서) “FV는 컴퓨터 용\n","16번째 | BLEU 점수: 0.0196078431372549\n","----------------------------------------------------------------------------------------------------\n","17번째 | 원본 번역본: 슈퍼컴퓨터 도입을 고려하면 분광요소법이 적절하다는 게 기상학계의 견해이며 미국 해양기상청의 보고서에 따르면 분광요소법은 차세대 컴퓨터 기술에 최적이라고 한다.\n","17번째 | 모델 번역본: 있다.\n"," 다만 앞으로의 슈퍼컴퓨터 도입 계획을 고려하면, 분광요소법이 적절하다는 것이 기상학계의 견해다. 한국기상학회가 감사원에 제공한 미국 해양기상청의 보고서에 따르면 “분광요소법은 계산노드(CPU) 수가 10만 개 이상인 차세대 컴퓨터 기술에 최적”이라고 한다. 내년에 도입되는 기상청의 슈퍼컴퓨터 5호기의 노드 개수는 50만 개에 달한다. 홍 단장은 “노드 수 10만 개를 기준으로 할 때 FV 방식의 효율은 분광요소법의 70% 수준으로 떨어진다”며 “슈퍼컴퓨터 5호기에서 KIM은 6km 고해상도를 구현할 수 있지만 FV 방식으론 불가능하다”고 덧붙였다. 독자성 평가, 관 주도 방식은 ‘\n","17번째 | BLEU 점수: 0.1595744680851064\n","----------------------------------------------------------------------------------------------------\n","18번째 | 원본 번역본: 지구 평균기온이 1도 올라감에 따라 변해야 할 날씨는 변하지 않고 변하지 말아야 할 기후는 변하고 있는 상태에서 장마가 아닌 기후위기임을 실감한 건 2020년 여름이다.\n","18번째 | 모델 번역본: 했다. 장마가 아니라 기후위기임을 실감한 2020년 여름이다. 여름은 장마와 폭염 그리고 열대야의 계절이다. 지구 평균기온이 1도 높아진 상태에 따라 변해야 할 것은 변하지 않고 변하지 말아야 할 것은 변하고 있다. 변해야 할 것은 날씨요 변하지 않고 변하지 말아야 할 것은 변하고 있다. 변해야 할 것은 날씨요 변하지 말아야 할 것은 기후다. 하루 하루 날씨는 다양한 변화를 보여줘야 한다. 그런데 무려 56일간의 장마는 변하지 않는 날씨의 위력을 보여주었다. 물 폭탄이 쏟아지고 강산이 할퀴어 졌다. 온대기후대인 한반도가 아열대 기후로 변하고 있다. 변하지 말아야 할 기후가 변함으로써 수목이 달라지고 식생이 달라지고 있다.\n","18번째 | BLEU 점수: 0.19\n","----------------------------------------------------------------------------------------------------\n","19번째 | 원본 번역본: 네이버 뉴스는 직접 고른 언론사·기자 및 AI가 골라주는 개인 맞춤형 뉴스 소비를 위해 편집 방식을 변경했다.\n","19번째 | 모델 번역본: 네이버가 뉴스 편집 방식을 바꾼다. ‘남들이 많이 본 기사’ 대신 ‘내가 선택한 언론사·기자의 기사’를 보여주고, ‘조회 수 많은 기사’보다 ‘인공지능(AI)이 골라준 기사’를 보여주는 방식이다. 네이버에 따르면, 보다 다양하고 개인 맞춤형의 뉴스 소비가 가능해진다. 단, AI의 추천이 강화되면서 투명성 문제를 어떻게 해결할지 주목된다. 23일 네이버는 공식 블로그에 다음 달 중으로 ‘많이 본 뉴스’ 순위를 노출하지 않겠다고 공지했다. 정치ᆞ경제ᆞ사회 분야별로, 전체 언론사의 기사를 ‘조회 수’나 ‘댓글 수’ 많은 순서로 보여주는 서비스를 중단한다는 것. 네이버는 “기사 소비가 다양해지고 구독 언론사·\n","19번째 | BLEU 점수: 0.04545454545454546\n","----------------------------------------------------------------------------------------------------\n","100문장에 대한 Belu Score 평균: 0.08950541144633137\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]}]},{"cell_type":"markdown","source":["# 3. modeling_bart.py 불러오기"],"metadata":{"id":"Dx8CnTDLt6Cu"}},{"cell_type":"markdown","source":["### 3-1. modeling_bart.py 내 함수 불러오기"],"metadata":{"id":"X9DK8JJER8Xd"}},{"cell_type":"code","source":["# def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n","#     \"\"\"\n","#     Shift input ids one token to the right.\n","#     \"\"\"\n","#     shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n","#     shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n","#     shifted_input_ids[:, 0] = decoder_start_token_id\n","\n","#     if pad_token_id is None:\n","#         raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n","#     # replace possible -100 values in labels by `pad_token_id`\n","#     shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n","\n","#     return shifted_input_ids\n","\n","\n","# def _make_causal_mask(\n","#     input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0\n","# ):\n","#     \"\"\"\n","#     Make causal mask used for bi-directional self-attention.\n","#     \"\"\"\n","#     bsz, tgt_len = input_ids_shape\n","#     mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)\n","#     mask_cond = torch.arange(mask.size(-1), device=device)\n","#     mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n","#     mask = mask.to(dtype)\n","\n","#     if past_key_values_length > 0:\n","#         mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n","#     return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n","\n","\n","# def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n","#     \"\"\"\n","#     Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n","#     \"\"\"\n","#     bsz, src_len = mask.size()\n","#     tgt_len = tgt_len if tgt_len is not None else src_len\n","\n","#     expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n","\n","#     inverted_mask = 1.0 - expanded_mask\n","\n","#     return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)"],"metadata":{"id":"sIxVWR8JRvWt","executionInfo":{"status":"ok","timestamp":1694347009704,"user_tz":-540,"elapsed":88,"user":{"displayName":"David Choi","userId":"14870253108450991755"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### 3-2. BartLearnedPositionalEmbedding 불러오기"],"metadata":{"id":"3bRNQKEvSCj9"}},{"cell_type":"code","source":["# class BartLearnedPositionalEmbedding(nn.Embedding):\n","#     \"\"\"\n","#     This module learns positional embeddings up to a fixed maximum size.\n","#     \"\"\"\n","\n","#     def __init__(self, num_embeddings: int, embedding_dim: int):\n","#         # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n","#         # and adjust num_embeddings appropriately. Other models don't have this hack\n","#         self.offset = 2\n","#         super().__init__(num_embeddings + self.offset, embedding_dim)\n","\n","#     def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):\n","#         \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\"\n","\n","#         bsz, seq_len = input_ids.shape[:2]\n","#         positions = torch.arange(\n","#             past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n","#         ).expand(bsz, -1)\n","\n","#         return super().forward(positions + self.offset)"],"metadata":{"id":"Pes76MinRvTu","executionInfo":{"status":"ok","timestamp":1694347009707,"user_tz":-540,"elapsed":89,"user":{"displayName":"David Choi","userId":"14870253108450991755"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### 3-3. BartAttention 불러오기"],"metadata":{"id":"ZInwWap9SPHg"}},{"cell_type":"code","source":["# class BartAttention(nn.Module):\n","#     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n","\n","#     def __init__(\n","#         self,\n","#         embed_dim: int,\n","#         num_heads: int,\n","#         dropout: float = 0.0,\n","#         is_decoder: bool = False,\n","#         bias: bool = True,\n","#     ):\n","#         super().__init__()\n","#         self.embed_dim = embed_dim\n","#         self.num_heads = num_heads\n","#         self.dropout = dropout\n","#         self.head_dim = embed_dim // num_heads\n","\n","#         if (self.head_dim * num_heads) != self.embed_dim:\n","#             raise ValueError(\n","#                 f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n","#                 f\" and `num_heads`: {num_heads}).\"\n","#             )\n","#         self.scaling = self.head_dim**-0.5\n","#         self.is_decoder = is_decoder\n","\n","#         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n","#         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n","#         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n","#         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n","\n","#     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n","#         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n","\n","#     def forward(\n","#         self,\n","#         hidden_states: torch.Tensor,\n","#         key_value_states: Optional[torch.Tensor] = None,\n","#         past_key_value: Optional[Tuple[torch.Tensor]] = None,\n","#         attention_mask: Optional[torch.Tensor] = None,\n","#         layer_head_mask: Optional[torch.Tensor] = None,\n","#         output_attentions: bool = False,\n","#     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n","#         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n","\n","#         # if key_value_states are provided this layer is used as a cross-attention layer\n","#         # for the decoder\n","#         is_cross_attention = key_value_states is not None\n","\n","#         bsz, tgt_len, _ = hidden_states.size()\n","\n","#         # get query proj\n","#         query_states = self.q_proj(hidden_states) * self.scaling\n","#         # get key, value proj\n","#         # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n","#         # is checking that the `sequence_length` of the `past_key_value` is the same as\n","#         # the provided `key_value_states` to support prefix tuning\n","#         if (\n","#             is_cross_attention\n","#             and past_key_value is not None\n","#             and past_key_value[0].shape[2] == key_value_states.shape[1]\n","#         ):\n","#             # reuse k,v, cross_attentions\n","#             key_states = past_key_value[0]\n","#             value_states = past_key_value[1]\n","#         elif is_cross_attention:\n","#             # cross_attentions\n","#             key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n","#             value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n","#         elif past_key_value is not None:\n","#             # reuse k, v, self_attention\n","#             key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n","#             value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n","#             key_states = torch.cat([past_key_value[0], key_states], dim=2)\n","#             value_states = torch.cat([past_key_value[1], value_states], dim=2)\n","#         else:\n","#             # self_attention\n","#             key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n","#             value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n","\n","#         if self.is_decoder:\n","#             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n","#             # Further calls to cross_attention layer can then reuse all cross-attention\n","#             # key/value_states (first \"if\" case)\n","#             # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n","#             # all previous decoder key/value_states. Further calls to uni-directional self-attention\n","#             # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n","#             # if encoder bi-directional self-attention `past_key_value` is always `None`\n","#             past_key_value = (key_states, value_states)\n","\n","#         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n","#         query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n","#         key_states = key_states.reshape(*proj_shape)\n","#         value_states = value_states.reshape(*proj_shape)\n","\n","#         src_len = key_states.size(1)\n","#         attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n","\n","#         if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n","#             raise ValueError(\n","#                 f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n","#                 f\" {attn_weights.size()}\"\n","#             )\n","\n","#         if attention_mask is not None:\n","#             if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n","#                 raise ValueError(\n","#                     f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n","#                 )\n","#             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n","#             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n","\n","#         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n","\n","#         if layer_head_mask is not None:\n","#             if layer_head_mask.size() != (self.num_heads,):\n","#                 raise ValueError(\n","#                     f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n","#                     f\" {layer_head_mask.size()}\"\n","#                 )\n","#             attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n","#             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n","\n","#         if output_attentions:\n","#             # this operation is a bit awkward, but it's required to\n","#             # make sure that attn_weights keeps its gradient.\n","#             # In order to do so, attn_weights have to be reshaped\n","#             # twice and have to be reused in the following\n","#             attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n","#             attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n","#         else:\n","#             attn_weights_reshaped = None\n","\n","#         attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n","\n","#         attn_output = torch.bmm(attn_probs, value_states)\n","\n","#         if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n","#             raise ValueError(\n","#                 f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n","#                 f\" {attn_output.size()}\"\n","#             )\n","\n","#         attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n","#         attn_output = attn_output.transpose(1, 2)\n","\n","#         # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n","#         # partitioned across GPUs when using tensor-parallelism.\n","#         attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n","\n","#         attn_output = self.out_proj(attn_output)\n","\n","#         return attn_output, attn_weights_reshaped, past_key_value"],"metadata":{"id":"qCozTUBERvQa","executionInfo":{"status":"ok","timestamp":1694347009709,"user_tz":-540,"elapsed":90,"user":{"displayName":"David Choi","userId":"14870253108450991755"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["### 3-4. BartPreTrainedModel 불러오기"],"metadata":{"id":"EcfncD5dTLP_"}},{"cell_type":"code","source":["# class BartPreTrainedModel(PreTrainedModel):\n","#     config_class = BartConfig\n","#     base_model_prefix = \"model\"\n","#     supports_gradient_checkpointing = True\n","#     _keys_to_ignore_on_load_unexpected = [\"encoder.version\", \"decoder.version\"]\n","#     _no_split_modules = [r\"BartEncoderLayer\", r\"BartDecoderLayer\"]\n","#     _skip_keys_device_placement = \"past_key_values\"\n","\n","#     def _init_weights(self, module):\n","#         std = self.config.init_std\n","#         if isinstance(module, nn.Linear):\n","#             module.weight.data.normal_(mean=0.0, std=std)\n","#             if module.bias is not None:\n","#                 module.bias.data.zero_()\n","#         elif isinstance(module, nn.Embedding):\n","#             module.weight.data.normal_(mean=0.0, std=std)\n","#             if module.padding_idx is not None:\n","#                 module.weight.data[module.padding_idx].zero_()\n","\n","#     def _set_gradient_checkpointing(self, module, value=False):\n","#         if isinstance(module, (BartDecoder, model.get_encoder())):  # BartEncoder -> model.get_encoder()\n","#             module.gradient_checkpointing = value\n","\n","#     @property\n","#     def dummy_inputs(self):\n","#         pad_token = self.config.pad_token_id\n","#         input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n","#         dummy_inputs = {\n","#             \"attention_mask\": input_ids.ne(pad_token),\n","#             \"input_ids\": input_ids,\n","#         }\n","#         return dummy_inputs"],"metadata":{"id":"dX8vGAwcTJ5W","executionInfo":{"status":"ok","timestamp":1694347009710,"user_tz":-540,"elapsed":89,"user":{"displayName":"David Choi","userId":"14870253108450991755"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["### 3-5. BartDecoderLayer 불러오기"],"metadata":{"id":"v40GmaHwueit"}},{"cell_type":"code","source":["# class BartDecoderLayer(nn.Module):\n","#     def __init__(self, config: BartConfig):\n","#         super().__init__()\n","#         self.embed_dim = config.d_model\n","\n","#         self.self_attn = BartAttention(\n","#             embed_dim=self.embed_dim,\n","#             num_heads=config.decoder_attention_heads,\n","#             dropout=config.attention_dropout,\n","#             is_decoder=True,\n","#         )\n","#         self.dropout = config.dropout\n","#         self.activation_fn = ACT2FN[config.activation_function]\n","#         self.activation_dropout = config.activation_dropout\n","\n","#         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n","#         self.encoder_attn = BartAttention(\n","#             self.embed_dim,\n","#             config.decoder_attention_heads,\n","#             dropout=config.attention_dropout,\n","#             is_decoder=True,\n","#         )\n","#         self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n","#         self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n","#         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n","#         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n","\n","#     def forward(\n","#         self,\n","#         hidden_states: torch.Tensor,\n","#         attention_mask: Optional[torch.Tensor] = None,\n","#         encoder_hidden_states: Optional[torch.Tensor] = None,\n","#         encoder_attention_mask: Optional[torch.Tensor] = None,\n","#         layer_head_mask: Optional[torch.Tensor] = None,\n","#         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n","#         past_key_value: Optional[Tuple[torch.Tensor]] = None,\n","#         output_attentions: Optional[bool] = False,\n","#         use_cache: Optional[bool] = True,\n","#     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n","#         \"\"\"\n","#         Args:\n","#             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n","#             attention_mask (`torch.FloatTensor`): attention mask of size\n","#                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n","#             encoder_hidden_states (`torch.FloatTensor`):\n","#                 cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n","#             encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n","#                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n","#             layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n","#                 `(encoder_attention_heads,)`.\n","#             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n","#                 size `(decoder_attention_heads,)`.\n","#             past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n","#             output_attentions (`bool`, *optional*):\n","#                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n","#                 returned tensors for more detail.\n","#         \"\"\"\n","#         residual = hidden_states\n","\n","#         # Self Attention\n","#         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n","#         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n","#         # add present self-attn cache to positions 1,2 of present_key_value tuple\n","#         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n","#             hidden_states=hidden_states,\n","#             past_key_value=self_attn_past_key_value,\n","#             attention_mask=attention_mask,\n","#             layer_head_mask=layer_head_mask,\n","#             output_attentions=output_attentions,\n","#         )\n","#         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n","#         hidden_states = residual + hidden_states\n","#         hidden_states = self.self_attn_layer_norm(hidden_states)\n","\n","#         # Cross-Attention Block\n","#         cross_attn_present_key_value = None\n","#         cross_attn_weights = None\n","#         if encoder_hidden_states is not None:\n","#             residual = hidden_states\n","\n","#             # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n","#             cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n","#             hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n","#                 hidden_states=hidden_states,\n","#                 key_value_states=encoder_hidden_states,\n","#                 attention_mask=encoder_attention_mask,\n","#                 layer_head_mask=cross_attn_layer_head_mask,\n","#                 past_key_value=cross_attn_past_key_value,\n","#                 output_attentions=output_attentions,\n","#             )\n","#             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n","#             hidden_states = residual + hidden_states\n","#             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n","\n","#             # add cross-attn to positions 3,4 of present_key_value tuple\n","#             present_key_value = present_key_value + cross_attn_present_key_value\n","\n","#         # Fully Connected\n","#         residual = hidden_states\n","#         hidden_states = self.activation_fn(self.fc1(hidden_states))\n","#         hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n","#         hidden_states = self.fc2(hidden_states)\n","#         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n","#         hidden_states = residual + hidden_states\n","#         hidden_states = self.final_layer_norm(hidden_states)\n","\n","#         outputs = (hidden_states,)\n","\n","#         if output_attentions:\n","#             outputs += (self_attn_weights, cross_attn_weights)\n","\n","#         if use_cache:\n","#             outputs += (present_key_value,)\n","\n","#         return outputs"],"metadata":{"id":"rSiETDz7gEP9","executionInfo":{"status":"ok","timestamp":1694347009712,"user_tz":-540,"elapsed":90,"user":{"displayName":"David Choi","userId":"14870253108450991755"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["### 3-5. BartDecoder 재정의"],"metadata":{"id":"YwbebDSmuoDN"}},{"cell_type":"code","source":["# class BartDecoder(BartPreTrainedModel):\n","#     \"\"\"\n","#     Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`BartDecoderLayer`]\n","\n","#     Args:\n","#         config: BartConfig\n","#         embed_tokens (nn.Embedding): output embedding\n","#     \"\"\"\n","\n","#     def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n","#         super().__init__(config)\n","#         self.dropout = config.dropout\n","#         self.layerdrop = config.decoder_layerdrop\n","#         self.padding_idx = config.pad_token_id\n","#         self.max_target_positions = config.max_position_embeddings\n","#         self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n","\n","#         self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)\n","\n","#         if embed_tokens is not None:\n","#             self.embed_tokens.weight = embed_tokens.weight\n","\n","#         self.embed_positions = BartLearnedPositionalEmbedding(\n","#             config.max_position_embeddings,\n","#             config.d_model,\n","#         )\n","#         self.layers = nn.ModuleList([BartDecoderLayer(config) for _ in range(config.decoder_layers)])\n","#         self.layernorm_embedding = nn.LayerNorm(config.d_model)\n","\n","#         self.gradient_checkpointing = False\n","#         # Initialize weights and apply final processing\n","#         self.post_init()\n","\n","#     def get_input_embeddings(self):\n","#         return self.embed_tokens\n","\n","#     def set_input_embeddings(self, value):\n","#         self.embed_tokens = value\n","\n","#     def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n","#         # create causal mask\n","#         # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n","#         combined_attention_mask = None\n","#         if input_shape[-1] > 1:\n","#             combined_attention_mask = _make_causal_mask(\n","#                 input_shape,\n","#                 inputs_embeds.dtype,\n","#                 device=inputs_embeds.device,\n","#                 past_key_values_length=past_key_values_length,\n","#             )\n","\n","#         if attention_mask is not None:\n","#             # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n","#             expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n","#                 inputs_embeds.device\n","#             )\n","#             combined_attention_mask = (\n","#                 expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n","#             )\n","\n","#         return combined_attention_mask\n","\n","#     def forward(\n","#         self,\n","#         input_ids: torch.LongTensor = None,\n","#         attention_mask: Optional[torch.Tensor] = None,\n","#         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n","#         encoder_attention_mask: Optional[torch.LongTensor] = None,\n","#         head_mask: Optional[torch.Tensor] = None,\n","#         cross_attn_head_mask: Optional[torch.Tensor] = None,\n","#         past_key_values: Optional[List[torch.FloatTensor]] = None,\n","#         inputs_embeds: Optional[torch.FloatTensor] = None,\n","#         use_cache: Optional[bool] = None,\n","#         output_attentions: Optional[bool] = None,\n","#         output_hidden_states: Optional[bool] = None,\n","#         return_dict: Optional[bool] = None,\n","#     ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n","#         r\"\"\"\n","#         Args:\n","#             input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n","#                 Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n","#                 provide it.\n","\n","#                 Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n","#                 [`PreTrainedTokenizer.__call__`] for details.\n","\n","#                 [What are input IDs?](../glossary#input-ids)\n","#             attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n","#                 Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n","\n","#                 - 1 for tokens that are **not masked**,\n","#                 - 0 for tokens that are **masked**.\n","\n","#                 [What are attention masks?](../glossary#attention-mask)\n","#             encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n","#                 Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n","#                 of the decoder.\n","#             encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):\n","#                 Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values\n","#                 selected in `[0, 1]`:\n","\n","#                 - 1 for tokens that are **not masked**,\n","#                 - 0 for tokens that are **masked**.\n","\n","#                 [What are attention masks?](../glossary#attention-mask)\n","#             head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n","#                 Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n","\n","#                 - 1 indicates the head is **not masked**,\n","#                 - 0 indicates the head is **masked**.\n","\n","#             cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n","#                 Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\n","#                 cross-attention on hidden heads. Mask values selected in `[0, 1]`:\n","\n","#                 - 1 indicates the head is **not masked**,\n","#                 - 0 indicates the head is **masked**.\n","\n","#             past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n","#                 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n","#                 shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n","#                 shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n","\n","#                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n","#                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n","\n","#                 If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n","#                 that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n","#                 all `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of\n","#                 shape `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing\n","#                 `input_ids` you can choose to directly pass an embedded representation. This is useful if you want more\n","#                 control over how to convert `input_ids` indices into associated vectors than the model's internal\n","#                 embedding lookup matrix.\n","#             output_attentions (`bool`, *optional*):\n","#                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n","#                 returned tensors for more detail.\n","#             output_hidden_states (`bool`, *optional*):\n","#                 Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n","#                 for more detail.\n","#             return_dict (`bool`, *optional*):\n","#                 Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n","#         \"\"\"\n","#         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","#         output_hidden_states = (\n","#             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","#         )\n","#         use_cache = use_cache if use_cache is not None else self.config.use_cache\n","#         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","#         # retrieve input_ids and inputs_embeds\n","#         if input_ids is not None and inputs_embeds is not None:\n","#             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n","#         elif input_ids is not None:\n","#             input = input_ids\n","#             input_shape = input.shape\n","#             input_ids = input_ids.view(-1, input_shape[-1])\n","#         elif inputs_embeds is not None:\n","#             input_shape = inputs_embeds.size()[:-1]\n","#             input = inputs_embeds[:, :, -1]\n","#         else:\n","#             raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n","\n","#         # past_key_values_length\n","#         past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n","\n","#         if inputs_embeds is None:\n","#             inputs_embeds = self.embed_tokens(input) * self.embed_scale\n","\n","#         attention_mask = self._prepare_decoder_attention_mask(\n","#             attention_mask, input_shape, inputs_embeds, past_key_values_length\n","#         )\n","\n","#         # expand encoder attention mask\n","#         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n","#             # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n","#             encoder_attention_mask = _expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n","\n","#         # embed positions\n","#         positions = self.embed_positions(input, past_key_values_length)\n","#         positions = positions.to(inputs_embeds.device)\n","\n","#         hidden_states = inputs_embeds + positions\n","#         hidden_states = self.layernorm_embedding(hidden_states)\n","\n","#         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n","\n","#         if self.gradient_checkpointing and self.training:\n","#             if use_cache:\n","#                 logger.warning_once(\n","#                     \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n","#                 )\n","#                 use_cache = False\n","\n","#         # decoder layers\n","#         all_hidden_states = () if output_hidden_states else None\n","#         all_self_attns = () if output_attentions else None\n","#         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n","#         next_decoder_cache = () if use_cache else None\n","\n","#         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n","#         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n","#             if attn_mask is not None:\n","#                 if attn_mask.size()[0] != (len(self.layers)):\n","#                     raise ValueError(\n","#                         f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n","#                         f\" {head_mask.size()[0]}.\"\n","#                     )\n","\n","#         for idx, decoder_layer in enumerate(self.layers):\n","#             # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n","#             if output_hidden_states:\n","#                 all_hidden_states += (hidden_states,)\n","#             if self.training:\n","#                 dropout_probability = torch.rand([])\n","#                 if dropout_probability < self.layerdrop:\n","#                     continue\n","\n","#             past_key_value = past_key_values[idx] if past_key_values is not None else None\n","\n","#             if self.gradient_checkpointing and self.training:\n","\n","#                 def create_custom_forward(module):\n","#                     def custom_forward(*inputs):\n","#                         # None for past_key_value\n","#                         return module(*inputs, output_attentions, use_cache)\n","\n","#                     return custom_forward\n","\n","#                 layer_outputs = torch.utils.checkpoint.checkpoint(\n","#                     create_custom_forward(decoder_layer),\n","#                     hidden_states,\n","#                     attention_mask,\n","#                     encoder_hidden_states,\n","#                     encoder_attention_mask,\n","#                     head_mask[idx] if head_mask is not None else None,\n","#                     cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n","#                     None,\n","#                 )\n","#             else:\n","#                 layer_outputs = decoder_layer(\n","#                     hidden_states,\n","#                     attention_mask=attention_mask,\n","#                     encoder_hidden_states=encoder_hidden_states,\n","#                     encoder_attention_mask=encoder_attention_mask,\n","#                     layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n","#                     cross_attn_layer_head_mask=(\n","#                         cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n","#                     ),\n","#                     past_key_value=past_key_value,\n","#                     output_attentions=output_attentions,\n","#                     use_cache=use_cache,\n","#                 )\n","#             hidden_states = layer_outputs[0]\n","\n","#             if use_cache:\n","#                 next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n","\n","#             if output_attentions:\n","#                 all_self_attns += (layer_outputs[1],)\n","\n","#                 if encoder_hidden_states is not None:\n","#                     all_cross_attentions += (layer_outputs[2],)\n","\n","#         # add hidden states from the last decoder layer\n","#         if output_hidden_states:\n","#             all_hidden_states += (hidden_states,)\n","\n","#         next_cache = next_decoder_cache if use_cache else None\n","#         if not return_dict:\n","#             return tuple(\n","#                 v\n","#                 for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n","#                 if v is not None\n","#             )\n","#         return BaseModelOutputWithPastAndCrossAttentions(\n","#             last_hidden_state=hidden_states,\n","#             past_key_values=next_cache,\n","#             hidden_states=all_hidden_states,\n","#             attentions=all_self_attns,\n","#             cross_attentions=all_cross_attentions,\n","#         )"],"metadata":{"id":"J9MCH5CMgEM5","executionInfo":{"status":"ok","timestamp":1694347009716,"user_tz":-540,"elapsed":93,"user":{"displayName":"David Choi","userId":"14870253108450991755"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["### 3-6. BartModel 불러오기"],"metadata":{"id":"vZ2lm_CMYoD6"}},{"cell_type":"code","source":["# # @add_start_docstrings(\n","# #     \"The bare BART Model outputting raw hidden-states without any specific head on top.\",\n","# #     BART_START_DOCSTRING,\n","# # )\n","# class BartModel(BartPreTrainedModel):\n","#     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n","\n","#     def __init__(self, config: BartConfig):\n","#         super().__init__(config)\n","\n","#         padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n","#         self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n","\n","#         self.encoder = model.get_encoder()\n","#         self.decoder = BartDecoder(config, self.shared)\n","\n","#         # Initialize weights and apply final processing\n","#         self.post_init()\n","\n","#     def get_input_embeddings(self):\n","#         return self.shared\n","\n","#     def set_input_embeddings(self, value):\n","#         self.shared = value\n","#         self.encoder.embed_tokens = self.shared\n","#         self.decoder.embed_tokens = self.shared\n","\n","#     def get_encoder(self):\n","#         return self.encoder\n","\n","#     def get_decoder(self):\n","#         return self.decoder\n","\n","#     # @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n","#     @add_code_sample_docstrings(\n","#         checkpoint=_CHECKPOINT_FOR_DOC,\n","#         output_type=Seq2SeqModelOutput,\n","#         config_class=_CONFIG_FOR_DOC,\n","#         expected_output=_EXPECTED_OUTPUT_SHAPE,\n","#     )\n","#     def forward(\n","#         self,\n","#         input_ids: torch.LongTensor = None,\n","#         attention_mask: Optional[torch.Tensor] = None,\n","#         decoder_input_ids: Optional[torch.LongTensor] = None,\n","#         decoder_attention_mask: Optional[torch.LongTensor] = None,\n","#         head_mask: Optional[torch.Tensor] = None,\n","#         decoder_head_mask: Optional[torch.Tensor] = None,\n","#         cross_attn_head_mask: Optional[torch.Tensor] = None,\n","#         encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n","#         past_key_values: Optional[List[torch.FloatTensor]] = None,\n","#         inputs_embeds: Optional[torch.FloatTensor] = None,\n","#         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n","#         use_cache: Optional[bool] = None,\n","#         output_attentions: Optional[bool] = None,\n","#         output_hidden_states: Optional[bool] = None,\n","#         return_dict: Optional[bool] = None,\n","#     ) -> Union[Tuple, Seq2SeqModelOutput]:\n","#         # different to other models, Bart automatically creates decoder_input_ids from\n","#         # input_ids if no decoder_input_ids are provided\n","#         if decoder_input_ids is None and decoder_inputs_embeds is None:\n","#             if input_ids is None:\n","#                 raise ValueError(\n","#                     \"If no `decoder_input_ids` or `decoder_inputs_embeds` are \"\n","#                     \"passed, `input_ids` cannot be `None`. Please pass either \"\n","#                     \"`input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`.\"\n","#                 )\n","\n","#             decoder_input_ids = shift_tokens_right(\n","#                 input_ids, self.config.pad_token_id, self.config.decoder_start_token_id\n","#             )\n","\n","#         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","#         output_hidden_states = (\n","#             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","#         )\n","#         use_cache = use_cache if use_cache is not None else self.config.use_cache\n","#         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","#         if encoder_outputs is None:\n","#             encoder_outputs = self.encoder(\n","#                 input_ids=input_ids,\n","#                 attention_mask=attention_mask,\n","#                 head_mask=head_mask,\n","#                 inputs_embeds=inputs_embeds,\n","#                 output_attentions=output_attentions,\n","#                 output_hidden_states=output_hidden_states,\n","#                 return_dict=return_dict,\n","#             )\n","#         # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n","#         elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n","#             encoder_outputs = BaseModelOutput(\n","#                 last_hidden_state=encoder_outputs[0],\n","#                 hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n","#                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n","#             )\n","\n","#         # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n","#         decoder_outputs = self.decoder(\n","#             input_ids=decoder_input_ids,\n","#             attention_mask=decoder_attention_mask,\n","#             encoder_hidden_states=encoder_outputs[0],\n","#             encoder_attention_mask=attention_mask,\n","#             head_mask=decoder_head_mask,\n","#             cross_attn_head_mask=cross_attn_head_mask,\n","#             past_key_values=past_key_values,\n","#             inputs_embeds=decoder_inputs_embeds,\n","#             use_cache=use_cache,\n","#             output_attentions=output_attentions,\n","#             output_hidden_states=output_hidden_states,\n","#             return_dict=return_dict,\n","#         )\n","\n","#         if not return_dict:\n","#             return decoder_outputs + encoder_outputs\n","\n","#         return Seq2SeqModelOutput(\n","#             last_hidden_state=decoder_outputs.last_hidden_state,\n","#             past_key_values=decoder_outputs.past_key_values,\n","#             decoder_hidden_states=decoder_outputs.hidden_states,\n","#             decoder_attentions=decoder_outputs.attentions,\n","#             cross_attentions=decoder_outputs.cross_attentions,\n","#             encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n","#             encoder_hidden_states=encoder_outputs.hidden_states,\n","#             encoder_attentions=encoder_outputs.attentions,\n","#         )"],"metadata":{"id":"Sfqv5fybgEKS","executionInfo":{"status":"ok","timestamp":1694347009717,"user_tz":-540,"elapsed":92,"user":{"displayName":"David Choi","userId":"14870253108450991755"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["### 3-7. BartForConditionalGeneration 불러오기"],"metadata":{"id":"sWqYk0hpYv2d"}},{"cell_type":"code","source":["# # @add_start_docstrings(\n","# #     \"The BART Model with a language modeling head. Can be used for summarization.\", BART_START_DOCSTRING\n","# # )\n","# class BartForConditionalGeneration(BartPreTrainedModel):\n","#     base_model_prefix = \"model\"\n","#     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n","#     _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n","\n","#     def __init__(self, config: BartConfig):\n","#         super().__init__(config)\n","#         self.model = BartModel(config)\n","#         self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n","#         self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n","\n","#         # Initialize weights and apply final processing\n","#         self.post_init()\n","\n","#     def get_encoder(self):\n","#         return self.model.get_encoder()\n","\n","#     def get_decoder(self):\n","#         return self.model.get_decoder()\n","\n","#     def resize_token_embeddings(self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None) -> nn.Embedding:\n","#         new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n","#         self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n","#         return new_embeddings\n","\n","#     def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n","#         old_num_tokens = self.final_logits_bias.shape[-1]\n","#         if new_num_tokens <= old_num_tokens:\n","#             new_bias = self.final_logits_bias[:, :new_num_tokens]\n","#         else:\n","#             extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n","#             new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n","#         self.register_buffer(\"final_logits_bias\", new_bias)\n","\n","#     def get_output_embeddings(self):\n","#         return self.lm_head\n","\n","#     def set_output_embeddings(self, new_embeddings):\n","#         self.lm_head = new_embeddings\n","\n","#     # @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n","#     # @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n","#     # @add_end_docstrings(BART_GENERATION_EXAMPLE)\n","#     def forward(\n","#         self,\n","#         input_ids: torch.LongTensor = None,\n","#         attention_mask: Optional[torch.Tensor] = None,\n","#         decoder_input_ids: Optional[torch.LongTensor] = None,\n","#         decoder_attention_mask: Optional[torch.LongTensor] = None,\n","#         head_mask: Optional[torch.Tensor] = None,\n","#         decoder_head_mask: Optional[torch.Tensor] = None,\n","#         cross_attn_head_mask: Optional[torch.Tensor] = None,\n","#         encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n","#         past_key_values: Optional[List[torch.FloatTensor]] = None,\n","#         inputs_embeds: Optional[torch.FloatTensor] = None,\n","#         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n","#         labels: Optional[torch.LongTensor] = None,\n","#         use_cache: Optional[bool] = None,\n","#         output_attentions: Optional[bool] = None,\n","#         output_hidden_states: Optional[bool] = None,\n","#         return_dict: Optional[bool] = None,\n","#     ) -> Union[Tuple, Seq2SeqLMOutput]:\n","#         r\"\"\"\n","#         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n","#             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n","#             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n","#             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n","\n","#         Returns:\n","#         \"\"\"\n","#         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","#         if labels is not None:\n","#             if use_cache:\n","#                 logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n","#             use_cache = False\n","#             if decoder_input_ids is None and decoder_inputs_embeds is None:\n","#                 decoder_input_ids = shift_tokens_right(\n","#                     labels, self.config.pad_token_id, self.config.decoder_start_token_id\n","#                 )\n","\n","#         outputs = self.model(\n","#             input_ids,\n","#             attention_mask=attention_mask,\n","#             decoder_input_ids=decoder_input_ids,\n","#             encoder_outputs=encoder_outputs,\n","#             decoder_attention_mask=decoder_attention_mask,\n","#             head_mask=head_mask,\n","#             decoder_head_mask=decoder_head_mask,\n","#             cross_attn_head_mask=cross_attn_head_mask,\n","#             past_key_values=past_key_values,\n","#             inputs_embeds=inputs_embeds,\n","#             decoder_inputs_embeds=decoder_inputs_embeds,\n","#             use_cache=use_cache,\n","#             output_attentions=output_attentions,\n","#             output_hidden_states=output_hidden_states,\n","#             return_dict=return_dict,\n","#         )\n","\n","#         lm_logits = self.lm_head(outputs[0])\n","#         lm_logits = lm_logits + self.final_logits_bias.to(lm_logits.device)\n","\n","#         masked_lm_loss = None\n","#         if labels is not None:\n","#             labels = labels.to(lm_logits.device)\n","#             loss_fct = CrossEntropyLoss()\n","#             masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n","\n","#         if not return_dict:\n","#             output = (lm_logits,) + outputs[1:]\n","#             return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n","\n","#         return Seq2SeqLMOutput(\n","#             loss=masked_lm_loss,\n","#             logits=lm_logits,\n","#             past_key_values=outputs.past_key_values,\n","#             decoder_hidden_states=outputs.decoder_hidden_states,\n","#             decoder_attentions=outputs.decoder_attentions,\n","#             cross_attentions=outputs.cross_attentions,\n","#             encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n","#             encoder_hidden_states=outputs.encoder_hidden_states,\n","#             encoder_attentions=outputs.encoder_attentions,\n","#         )\n","\n","#     def prepare_inputs_for_generation(\n","#         self,\n","#         decoder_input_ids,\n","#         past_key_values=None,\n","#         attention_mask=None,\n","#         decoder_attention_mask=None,\n","#         head_mask=None,\n","#         decoder_head_mask=None,\n","#         cross_attn_head_mask=None,\n","#         use_cache=None,\n","#         encoder_outputs=None,\n","#         **kwargs,\n","#     ):\n","#         # cut decoder_input_ids if past_key_values is used\n","#         if past_key_values is not None:\n","#             decoder_input_ids = decoder_input_ids[:, -1:]\n","\n","#         return {\n","#             \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n","#             \"encoder_outputs\": encoder_outputs,\n","#             \"past_key_values\": past_key_values,\n","#             \"decoder_input_ids\": decoder_input_ids,\n","#             \"attention_mask\": attention_mask,\n","#             \"decoder_attention_mask\": decoder_attention_mask,\n","#             \"head_mask\": head_mask,\n","#             \"decoder_head_mask\": decoder_head_mask,\n","#             \"cross_attn_head_mask\": cross_attn_head_mask,\n","#             \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n","#         }\n","\n","#     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n","#         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n","\n","#     @staticmethod\n","#     def _reorder_cache(past_key_values, beam_idx):\n","#         reordered_past = ()\n","#         for layer_past in past_key_values:\n","#             # cached cross_attention states don't have to be reordered -> they are always the same\n","#             reordered_past += (\n","#                 tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n","#             )\n","#         return reordered_past"],"metadata":{"id":"s9CavUZlgEHb","executionInfo":{"status":"ok","timestamp":1694347009718,"user_tz":-540,"elapsed":91,"user":{"displayName":"David Choi","userId":"14870253108450991755"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# 4. Fine-Tuning\n","- d_model: 768 -> 256\n","- encoder & decoder layers: 6 -> 3\n","- encoder & decoder ffn_dim: 3072 -> 1024"],"metadata":{"id":"lXtfKwzAZJOX"}},{"cell_type":"code","source":["# 기존 config\n","config = BartConfig.from_pretrained(\"hyunwoongko/kobart\")\n","config"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jHQN99zQV4bG","executionInfo":{"status":"ok","timestamp":1694358921697,"user_tz":-540,"elapsed":24,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"5cd8aa00-02d4-457e-93fd-cbcb9574fecd"},"execution_count":105,"outputs":[{"output_type":"stream","name":"stderr","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]},{"output_type":"execute_result","data":{"text/plain":["BartConfig {\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 1,\n","  \"classif_dropout\": 0.1,\n","  \"classifier_dropout\": 0.1,\n","  \"d_model\": 768,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 3072,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 6,\n","  \"decoder_start_token_id\": 1,\n","  \"do_blenderbot_90_layernorm\": false,\n","  \"dropout\": 0.1,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 3072,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 6,\n","  \"eos_token_id\": 1,\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": false,\n","  \"forced_eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"NEGATIVE\",\n","    \"1\": \"POSITIVE\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"kobart_version\": 2.0,\n","  \"label2id\": {\n","    \"NEGATIVE\": 0,\n","    \"POSITIVE\": 1\n","  },\n","  \"max_position_embeddings\": 1026,\n","  \"model_type\": \"bart\",\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 3,\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n","  \"transformers_version\": \"4.33.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 30000\n","}"]},"metadata":{},"execution_count":105}]},{"cell_type":"code","source":["# 수정된 config\n","config.d_model = 256            # 768 -> 256\n","config.decoder_layers = 3       # 6 -> 3\n","config.decoder_ffn_dim = 1024   # 3072 -> 1024\n","config"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kqiormhcWPNP","executionInfo":{"status":"ok","timestamp":1694358921698,"user_tz":-540,"elapsed":17,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"6c10075f-ed33-4487-b11e-823a778d9064"},"execution_count":106,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BartConfig {\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 1,\n","  \"classif_dropout\": 0.1,\n","  \"classifier_dropout\": 0.1,\n","  \"d_model\": 256,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 1024,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 3,\n","  \"decoder_start_token_id\": 1,\n","  \"do_blenderbot_90_layernorm\": false,\n","  \"dropout\": 0.1,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 3072,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 6,\n","  \"eos_token_id\": 1,\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": false,\n","  \"forced_eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"NEGATIVE\",\n","    \"1\": \"POSITIVE\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"kobart_version\": 2.0,\n","  \"label2id\": {\n","    \"NEGATIVE\": 0,\n","    \"POSITIVE\": 1\n","  },\n","  \"max_position_embeddings\": 1026,\n","  \"model_type\": \"bart\",\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 3,\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n","  \"transformers_version\": \"4.33.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 30000\n","}"]},"metadata":{},"execution_count":106}]},{"cell_type":"code","source":["new_model = BartForConditionalGeneration.from_pretrained(\"hyunwoongko/kobart\", config=config, ignore_mismatched_sizes=True)\n","new_model"],"metadata":{"id":"riesFAo8gEDH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694358922147,"user_tz":-540,"elapsed":456,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"b05dd147-f91e-4576-90c0-30d7c8f4f13f"},"execution_count":107,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at hyunwoongko/kobart were not used when initializing BartForConditionalGeneration: ['decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.bias', 'decoder.layers.5.fc2.bias', 'decoder.layers.3.fc2.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.4.fc1.weight', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.4.fc2.weight', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.3.fc2.weight', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc2.bias', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.5.self_attn_layer_norm.bias']\n","- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at hyunwoongko/kobart and are newly initialized because the shapes did not match:\n","- shared.weight: found shape torch.Size([30000, 768]) in the checkpoint and torch.Size([30000, 256]) in the model instantiated\n","- encoder.embed_tokens.weight: found shape torch.Size([30000, 768]) in the checkpoint and torch.Size([30000, 256]) in the model instantiated\n","- encoder.embed_positions.weight: found shape torch.Size([1028, 768]) in the checkpoint and torch.Size([1028, 256]) in the model instantiated\n","- encoder.layers.0.self_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.0.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.0.self_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.0.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.0.self_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.0.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.0.self_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.0.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.0.self_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.0.self_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.0.fc1.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 256]) in the model instantiated\n","- encoder.layers.0.fc2.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([256, 3072]) in the model instantiated\n","- encoder.layers.0.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.0.final_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.0.final_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.1.self_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.1.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.1.self_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.1.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.1.self_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.1.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.1.self_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.1.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.1.self_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.1.self_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.1.fc1.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 256]) in the model instantiated\n","- encoder.layers.1.fc2.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([256, 3072]) in the model instantiated\n","- encoder.layers.1.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.1.final_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.1.final_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.2.self_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.2.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.2.self_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.2.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.2.self_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.2.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.2.self_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.2.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.2.self_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.2.self_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.2.fc1.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 256]) in the model instantiated\n","- encoder.layers.2.fc2.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([256, 3072]) in the model instantiated\n","- encoder.layers.2.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.2.final_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.2.final_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.3.self_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.3.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.3.self_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.3.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.3.self_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.3.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.3.self_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.3.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.3.self_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.3.self_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.3.fc1.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 256]) in the model instantiated\n","- encoder.layers.3.fc2.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([256, 3072]) in the model instantiated\n","- encoder.layers.3.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.3.final_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.3.final_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.4.self_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.4.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.4.self_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.4.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.4.self_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.4.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.4.self_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.4.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.4.self_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.4.self_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.4.fc1.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 256]) in the model instantiated\n","- encoder.layers.4.fc2.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([256, 3072]) in the model instantiated\n","- encoder.layers.4.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.4.final_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.4.final_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.5.self_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.5.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.5.self_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.5.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.5.self_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.5.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.5.self_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- encoder.layers.5.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.5.self_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.5.self_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.5.fc1.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 256]) in the model instantiated\n","- encoder.layers.5.fc2.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([256, 3072]) in the model instantiated\n","- encoder.layers.5.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.5.final_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layers.5.final_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layernorm_embedding.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- encoder.layernorm_embedding.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.embed_tokens.weight: found shape torch.Size([30000, 768]) in the checkpoint and torch.Size([30000, 256]) in the model instantiated\n","- decoder.embed_positions.weight: found shape torch.Size([1028, 768]) in the checkpoint and torch.Size([1028, 256]) in the model instantiated\n","- decoder.layers.0.self_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.0.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.0.self_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.0.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.0.self_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.0.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.0.self_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.0.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.0.self_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.0.self_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.0.encoder_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.0.encoder_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.0.encoder_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.0.encoder_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.0.encoder_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.0.encoder_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.0.encoder_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.0.encoder_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.0.encoder_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.0.encoder_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.0.fc1.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n","- decoder.layers.0.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1024]) in the model instantiated\n","- decoder.layers.0.fc2.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n","- decoder.layers.0.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.0.final_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.0.final_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.1.self_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.1.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.1.self_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.1.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.1.self_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.1.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.1.self_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.1.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.1.self_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.1.self_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.1.encoder_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.1.encoder_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.1.encoder_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.1.encoder_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.1.encoder_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.1.encoder_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.1.encoder_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.1.encoder_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.1.encoder_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.1.encoder_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.1.fc1.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n","- decoder.layers.1.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1024]) in the model instantiated\n","- decoder.layers.1.fc2.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n","- decoder.layers.1.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.1.final_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.1.final_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.2.self_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.2.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.2.self_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.2.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.2.self_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.2.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.2.self_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.2.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.2.self_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.2.self_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.2.encoder_attn.k_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.2.encoder_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.2.encoder_attn.v_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.2.encoder_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.2.encoder_attn.q_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.2.encoder_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.2.encoder_attn.out_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n","- decoder.layers.2.encoder_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.2.encoder_attn_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.2.encoder_attn_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.2.fc1.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n","- decoder.layers.2.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1024]) in the model instantiated\n","- decoder.layers.2.fc2.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n","- decoder.layers.2.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.2.final_layer_norm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layers.2.final_layer_norm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layernorm_embedding.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","- decoder.layernorm_embedding.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BartForConditionalGeneration(\n","  (model): BartModel(\n","    (shared): Embedding(30000, 256, padding_idx=3)\n","    (encoder): BartEncoder(\n","      (embed_tokens): Embedding(30000, 256, padding_idx=3)\n","      (embed_positions): BartLearnedPositionalEmbedding(1028, 256)\n","      (layers): ModuleList(\n","        (0-5): 6 x BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=256, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=256, bias=True)\n","          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (decoder): BartDecoder(\n","      (embed_tokens): Embedding(30000, 256, padding_idx=3)\n","      (embed_positions): BartLearnedPositionalEmbedding(1028, 256)\n","      (layers): ModuleList(\n","        (0-2): 3 x BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (lm_head): Linear(in_features=256, out_features=30000, bias=False)\n",")"]},"metadata":{},"execution_count":107}]},{"cell_type":"markdown","source":["# 5. Dataset 및 DataLodaer 정의"],"metadata":{"id":"Gyefd3KPbX_b"}},{"cell_type":"code","source":["# 데이터 로딩 및 전처리를 위한 클래스 정의\n","class CustomDataset(Dataset):\n","    def __init__(self, source_df, target_df, tokenizer, max_length=512):\n","        self.source_sentences = source_df.reset_index().drop('index', axis=1).values.tolist()\n","        self.target_sentences = target_df.reset_index().drop('index', axis=1).values.tolist()\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.source_sentences)\n","\n","    def __getitem__(self, idx):\n","        source_text = self.source_sentences[idx][0]\n","        target_text = self.target_sentences[idx][0]\n","\n","        # 토큰화 및 패딩\n","        source_tokens = self.tokenizer.encode(source_text, add_special_tokens=True, max_length=self.max_length, padding=\"max_length\", truncation=True)\n","        target_tokens = self.tokenizer.encode(target_text, add_special_tokens=True, max_length=self.max_length, padding=\"max_length\", truncation=True)\n","\n","        # 아래 부분을 수정하여 input_ids를 반환하도록 재정의\n","        return {'input_ids': torch.tensor(source_tokens, dtype=torch.long),\n","                'decoder_input_ids': torch.tensor(target_tokens, dtype=torch.long)}"],"metadata":{"id":"wTw0-l_Dd0YK","executionInfo":{"status":"ok","timestamp":1694358768990,"user_tz":-540,"elapsed":972,"user":{"displayName":"David Choi","userId":"14870253108450991755"}}},"execution_count":100,"outputs":[]},{"cell_type":"code","source":["######################################## [Test] ########################################"],"metadata":{"id":"lHmo_ZwGqRC9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# source_sentences = input_train.reset_index().drop('index', axis=1).values.tolist()\n","# print(source_sentences[0][0])\n","\n","# source_text = source_sentences[0][0]\n","# print(source_text)\n","\n","# source_tokens = tokenizer.encode(source_text, add_special_tokens=True, max_length=512, padding='max_length', truncation=True)\n","# print(source_tokens)"],"metadata":{"id":"4aXPtrqmrWF_","executionInfo":{"status":"ok","timestamp":1694347011489,"user_tz":-540,"elapsed":12,"user":{"displayName":"David Choi","userId":"14870253108450991755"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["######################################## [Test] ########################################"],"metadata":{"id":"sCRRVE5NqSgq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = 'hyunwoongko/kobart'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# 데이터셋 생성\n","train_dataset = CustomDataset(input_train, target_train, tokenizer)\n","valid_dataset = CustomDataset(input_valid, target_valid, tokenizer)\n","test_dataset = CustomDataset(input_test, target_test, tokenizer)\n","\n","train_dataset, valid_dataset, test_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZYV-ZOJPiLvW","executionInfo":{"status":"ok","timestamp":1694358771495,"user_tz":-540,"elapsed":33,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"56709185-3638-4c36-c578-4dd0e36424bd"},"execution_count":101,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<__main__.CustomDataset at 0x7d2e8c1b7a90>,\n"," <__main__.CustomDataset at 0x7d2edbd18dc0>,\n"," <__main__.CustomDataset at 0x7d2edc1e34c0>)"]},"metadata":{},"execution_count":101}]},{"cell_type":"code","source":["# train_dataset[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TCqmCqcMu20M","executionInfo":{"status":"ok","timestamp":1694358771498,"user_tz":-540,"elapsed":23,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"1fe879b7-9c66-4d88-d7f8-611210427a01"},"execution_count":102,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([    0, 16735, 12335, 11821, 14031, 10952, 11841, 11810, 14299, 14143,\n","         16121, 15991, 19858,  1543, 18044, 11914, 14085, 10770,  9092, 18025,\n","         10496, 15116, 19211, 14141, 16749, 14862, 14245, 14243, 16344, 14671,\n","         14725, 14483, 11471,   245, 16735, 10952, 11810, 17301, 12007, 14067,\n","          9034, 14143, 16121, 15991, 19858, 18463, 22075, 23658, 14189, 16367,\n","         19775, 11214, 14807, 15425, 25437, 12005, 21308, 11786, 14114, 12332,\n","         21245, 14174, 11264, 11950, 14101, 15460, 12074, 14105, 12005, 18817,\n","         11846, 10226, 14130, 16414, 28403, 19790, 17454, 13469, 17242, 14311,\n","         23449, 25891,  9714,   243, 14040, 16267, 14075, 10500, 10788, 12060,\n","         13590, 16338, 11786, 15859, 12007, 16904, 16728, 16261, 22075, 19727,\n","         16922, 17125, 14130, 26294, 14038, 27368, 14328, 14048, 14038, 13672,\n","         12333, 14410, 20122, 14736, 18154, 14144, 16626, 14737, 16527, 15735,\n","          9103, 14159, 14532, 11950, 14130, 14281, 16781, 15962, 15262,  9760,\n","         14160, 12346, 16247, 20388, 15196, 14108, 28403, 14196,  9993,  9066,\n","         14673,  9698, 19454, 22887, 15029, 27375, 14346, 20602, 14583, 21368,\n","         12005, 23962, 14959, 15135, 14473, 12034,   373, 11028, 10770,  9092,\n","           239, 12865, 12034, 16589,   373, 11028, 19132,   240, 15196, 14108,\n","         10215, 14244, 16247, 14169,  9123, 11908, 14038, 16277, 15063, 14025,\n","         10671, 11810, 16863, 11863, 14270, 16735, 12471, 14248, 10512,  8981,\n","         14083, 10293, 12034, 16634, 24294, 29815, 15033, 15615, 14038,  9133,\n","         13758, 14288, 14527, 14596, 14318, 19754, 17942, 16954, 16338, 11264,\n","         14222, 18433, 14863, 16636, 15442, 27134, 16247, 17507, 11841, 14957,\n","         23412, 14325, 15302, 26294, 14038, 18173, 22809, 14561, 15964, 14196,\n","         15521, 14143, 12865, 10770, 16058, 17045, 21766, 12037, 14775,  9160,\n","         11699, 23373, 14188, 19290, 25686, 14040, 19887, 28816, 14449, 14059,\n","         16247, 14554, 14038, 15699, 14025, 13331,  9501, 14128, 24433, 10314,\n","         13590, 18870, 10500, 20688, 14146, 21320, 11788, 14877, 22534, 14058,\n","         14221,  9501, 12034, 17727, 15296, 22075, 13590, 14262, 14447, 15991,\n","         19858,  1547, 10215, 17094, 14171, 21006, 17499, 11788, 27483,  9770,\n","         15457, 14697, 14281, 11207, 13258, 14058, 25204, 14703, 15353, 19475,\n","         18260, 14775, 14328, 17798, 10443, 14730, 15615, 17507, 11841, 18400,\n","         14141, 15682, 29543, 23191, 18752, 29641, 27080, 12344, 13737, 14872,\n","         14128, 18908, 15274, 16533, 14230, 16301, 14621, 12332, 10213, 19162,\n","         14226, 12710, 14130, 14039, 10177, 14303,  1700, 11160, 11160, 12034,\n","         23629, 15712, 14319, 18617, 20466, 16735, 11285, 14790, 14130, 14188,\n","         21049, 14199, 21786, 19290,  9754, 14700, 16735, 21990, 14025, 11767,\n","         12258, 14094, 21990, 17700, 13672, 14094, 16346, 17809, 14274, 16247,\n","         18648, 15930, 15801, 20401, 14450, 11806,  9879, 13679, 24112, 16245,\n","         14459, 11319, 12141, 19290, 23102, 22666, 11900, 14635, 14130, 14957,\n","         11268,  9123, 15426, 16270,  9908, 16060, 12034, 18747,  9006, 15094,\n","         14411,  9770, 19653, 20172, 27463, 16817, 22417, 10500, 14787, 27782,\n","         14210, 21158, 20388, 17552, 13195, 15347, 14226, 12869, 17544, 16888,\n","          9267, 14042, 14130, 15037, 15475, 12007, 14067,  9034, 17507, 11841,\n","         18400, 25036,  9694, 14036, 16060, 14303, 16028, 14059, 14452, 14082,\n","         16060,   243, 16028, 10443, 15705, 20231, 14042, 14259, 25011, 15475,\n","         12005, 15423, 14406, 14709, 15615, 14775,  9160, 11699, 23373, 14188,\n","          9770, 19698, 16735, 15475, 16298, 28061,  9102, 16297, 16156, 16247,\n","         20410, 14143, 13848, 14082,  1543, 14188, 16346, 14143, 13848, 14426,\n","          1543, 14447, 15991, 11806, 15071, 17737, 11786, 14146, 10443,  1700,\n","         14298, 21088, 14995, 14473, 12034, 17035, 18650, 20753, 14143, 24172,\n","         14136, 10952, 11810,  1543, 15548, 16220, 12034, 17125, 14130, 15416,\n","         17990,     1]),\n"," 'decoder_input_ids': tensor([    0, 14447, 15991, 11806, 15396, 14091, 22075, 15392, 11786, 27368,\n","         14554, 14038, 14328, 14048, 14410, 20122, 21354, 17400, 14144, 16626,\n","         17483, 16527, 15735,  9103, 14159, 14174, 11950, 14297, 14596, 14527,\n","         14038, 14288, 14517, 19754,     1,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3])}"]},"metadata":{},"execution_count":102}]},{"cell_type":"code","source":["# len(train_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r1k5siQ-tp9B","executionInfo":{"status":"ok","timestamp":1694347012726,"user_tz":-540,"elapsed":28,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"28996803-1758-44a2-c85c-384f62c09e43"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7000"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["batch_size = 16\n","\n","train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n","valid_dl = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","\n","train_dl, valid_dl"],"metadata":{"id":"ZPljFGXXc1oI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694358938971,"user_tz":-540,"elapsed":10,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"92fef47e-33bc-4b0f-8db4-eedf04ad118d"},"execution_count":108,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<torch.utils.data.dataloader.DataLoader at 0x7d2eb83c2ad0>,\n"," <torch.utils.data.dataloader.DataLoader at 0x7d2eb83c34f0>)"]},"metadata":{},"execution_count":108}]},{"cell_type":"code","source":["######################################## [Test] ########################################"],"metadata":{"id":"QblpaNeTqDAQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# batch = next(iter(train_dl))\n","# batch"],"metadata":{"id":"aTbEwTtvhfzl","executionInfo":{"status":"ok","timestamp":1694347012727,"user_tz":-540,"elapsed":15,"user":{"displayName":"David Choi","userId":"14870253108450991755"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# new_model(input_ids=batch['input_ids'], decoder_input_ids=batch['decoder_input_ids'])"],"metadata":{"id":"jCf_wDdlhft3","executionInfo":{"status":"ok","timestamp":1694347012728,"user_tz":-540,"elapsed":15,"user":{"displayName":"David Choi","userId":"14870253108450991755"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["######################################## [Test] ########################################"],"metadata":{"id":"thbk0KgaqCZR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6. 모델 학습"],"metadata":{"id":"at4FmRpuycYX"}},{"cell_type":"code","source":["######################################## [Test] ########################################"],"metadata":{"id":"zHivm3bNmtZg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 2\n","\n","ex_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n","batch = next(iter(ex_dl))\n","batch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xfhc2YbFmtWF","executionInfo":{"status":"ok","timestamp":1694358470704,"user_tz":-540,"elapsed":815,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"f0263570-d094-4ad7-94f7-62fbceefa357"},"execution_count":89,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[    0, 16735, 12335,  ..., 15416, 17990,     1],\n","         [    0, 19601, 15791,  ...,     3,     3,     3]]),\n"," 'decoder_input_ids': tensor([[    0, 14447, 15991,  ...,     3,     3,     3],\n","         [    0, 14360, 11790,  ...,     3,     3,     3]])}"]},"metadata":{},"execution_count":89}]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","criterion = nn.CrossEntropyLoss()\n","new_model = new_model.to(device)\n","optimizer = torch.optim.Adam(params=new_model.parameters(), lr=0.001)\n","\n","input_ids = batch['input_ids'].to(device)\n","decoder_input_ids = batch['decoder_input_ids'].to(device)\n","\n","optimizer.zero_grad()\n","\n","logits = new_model(input_ids, decoder_input_ids).logits\n","print(logits)\n","print('--------------------------------------------------------------------------------')\n","\n","labels = decoder_input_ids[:, 1:].contiguous()  # 시작 토큰을 제외하고 실제 레이블을 가져옴\n","print(labels)\n","print('--------------------------------------------------------------------------------')\n","\n","# 손실 계산\n","logits_flat = logits.view(-1, logits.shape[-1])  # 로짓 텐서를 얻고나서 view 메서드 적용\n","print(logits_flat)\n","print('--------------------------------------------------------------------------------')\n","\n","labels_flat = labels.view(-1)\n","print(labels_flat)\n","print('--------------------------------------------------------------------------------')\n","\n","loss = criterion(logits_flat[:labels_flat.shape[0]], labels_flat)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_uK-lQDmtSi","executionInfo":{"status":"ok","timestamp":1694358658797,"user_tz":-540,"elapsed":8,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"fdb41264-2082-4e72-ae2d-fcee5655adaa"},"execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ -5.8352,   1.9084, -10.5518,  ...,  -0.3057,  -7.3703,   0.1606],\n","         [ -6.8168,   1.2824,  -9.3200,  ...,   0.3930,  -7.2935,   0.2376],\n","         [ -7.2227,   2.5095, -11.5742,  ...,  -0.4693,  -5.9092,  -0.0818],\n","         ...,\n","         [ -7.7116,  -3.2558, -11.9401,  ...,  -3.2661, -12.2101,  -5.9589],\n","         [ -7.9871,  -0.3418, -13.3756,  ...,  -3.4589, -10.6925,  -6.1063],\n","         [ -8.2278,  -3.3080, -11.1270,  ...,  -3.4042, -12.3817,  -6.1029]],\n","\n","        [[ -7.5942,   1.9930, -11.9048,  ...,  -0.8138,  -7.4947,  -0.4168],\n","         [ -7.2117,   2.1575, -11.2467,  ...,  -0.4999,  -6.9740,  -0.1622],\n","         [ -7.1256,   2.4535, -11.7506,  ...,  -0.2463,  -6.5110,  -0.5759],\n","         ...,\n","         [ -8.2570,  -0.7979, -12.1158,  ...,  -3.3909, -10.9554,  -6.6212],\n","         [ -7.8320,  -2.4193, -13.1435,  ...,  -4.0297, -12.8379,  -6.8377],\n","         [ -8.5323,  -2.0027, -14.0358,  ...,  -4.0323, -11.8916,  -6.1416]]],\n","       device='cuda:0', grad_fn=<AddBackward0>)\n","--------------------------------------------------------------------------------\n","tensor([[14447, 15991, 11806,  ...,     3,     3,     3],\n","        [14360, 11790, 19958,  ...,     3,     3,     3]], device='cuda:0')\n","--------------------------------------------------------------------------------\n","tensor([[ -5.8352,   1.9084, -10.5518,  ...,  -0.3057,  -7.3703,   0.1606],\n","        [ -6.8168,   1.2824,  -9.3200,  ...,   0.3930,  -7.2935,   0.2376],\n","        [ -7.2227,   2.5095, -11.5742,  ...,  -0.4693,  -5.9092,  -0.0818],\n","        ...,\n","        [ -8.2570,  -0.7979, -12.1158,  ...,  -3.3909, -10.9554,  -6.6212],\n","        [ -7.8320,  -2.4193, -13.1435,  ...,  -4.0297, -12.8379,  -6.8377],\n","        [ -8.5323,  -2.0027, -14.0358,  ...,  -4.0323, -11.8916,  -6.1416]],\n","       device='cuda:0', grad_fn=<ViewBackward0>)\n","--------------------------------------------------------------------------------\n","tensor([14447, 15991, 11806,  ...,     3,     3,     3], device='cuda:0')\n","--------------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["######################################## [Test] ########################################"],"metadata":{"id":"f2khS_0_mtK-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 디바이스, 손실 함수, 모델, 옵티마이저 정의\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","criterion = nn.CrossEntropyLoss()\n","new_model = new_model.to(device)\n","optimizer = torch.optim.Adam(params=new_model.parameters(), lr=0.001)\n","\n","# encoder 파라미터 동결시키기\n","for param in new_model.model.encoder.parameters():\n","    param.requires_grad = False\n","\n","# loss 저장 리스트\n","save_loss_list = []\n","\n","num_epochs = 20\n","for epoch in range(num_epochs):\n","    new_model.train()\n","    total_loss = 0.0\n","    for batch in train_dl:\n","        try:\n","            input_ids = batch['input_ids'].to(device)\n","            decoder_input_ids = batch['decoder_input_ids'].to(device)\n","\n","            optimizer.zero_grad()\n","\n","            logits = new_model(input_ids, decoder_input_ids).logits\n","\n","            # 패딩을 제거하고 실제 레이블을 얻기 위해 decoder_input_ids에서 패딩 토큰을 제거\n","            labels = decoder_input_ids[:, 1:].contiguous()  # 시작 토큰을 제외하고 실제 레이블을 가져옴\n","\n","             # 손실 계산\n","            logits_flat = logits.view(-1, logits.shape[-1])  # 로짓 텐서를 얻고나서 view 메서드 적용\n","            labels_flat = labels.view(-1)\n","            loss = criterion(logits_flat[:labels_flat.shape[0]], labels_flat)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","            one_epoch_loss = total_loss / len(train_dl)\n","\n","        except KeyboardInterrupt:\n","            # KeyboardInterrupt가 발생하면 학습을 중단하고 모델을 저장한 후 나가기\n","            print(\"KeyboardInterrupt: Saving model and exiting...\")\n","            torch.save(new_model, '/content/drive/MyDrive/Playdata_Python/final_project/models/summary_dataset/summary_model_v1_1.pth')\n","            exit()\n","\n","    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {one_epoch_loss}')\n","    save_loss_list.append(one_epoch_loss)\n","\n","# 학습이 완료되면 모델을 저장\n","torch.save(new_model, '/content/drive/MyDrive/Playdata_Python/final_project/models/summary_dataset/summary_model_v1_.pth')\n",""],"metadata":{"id":"EV_9LC5Sc1jT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694362944153,"user_tz":-540,"elapsed":3860295,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"2ca709e8-292a-450f-baad-e8d25cac734d"},"execution_count":109,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Loss: 3.9867693686594157\n","Epoch 2/20, Loss: 1.5009451769802669\n","Epoch 3/20, Loss: 0.8997986041791907\n","Epoch 4/20, Loss: 0.7816888092587527\n","Epoch 5/20, Loss: 0.7420801067188995\n","Epoch 6/20, Loss: 0.7232020852500445\n","Epoch 7/20, Loss: 0.7124655898575369\n","Epoch 8/20, Loss: 0.8528921585104782\n","Epoch 9/20, Loss: 0.8257707870442029\n","Epoch 10/20, Loss: 0.7946464288724612\n","Epoch 11/20, Loss: 0.78233391713334\n","Epoch 12/20, Loss: 0.7771789389386025\n","Epoch 13/20, Loss: 0.7724168530610054\n","Epoch 14/20, Loss: 0.7694898943106333\n","Epoch 15/20, Loss: 0.7668941269998681\n","Epoch 16/20, Loss: 0.7632033038357077\n","Epoch 17/20, Loss: 0.7297640803469915\n","Epoch 18/20, Loss: 0.7080696619808946\n","Epoch 19/20, Loss: 0.704470173817247\n","Epoch 20/20, Loss: 0.7039912164755607\n"]}]},{"cell_type":"code","source":["plt.plot(save_loss_list, label='Train Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"id":"pqVkxGL57tYs","executionInfo":{"status":"ok","timestamp":1694362960191,"user_tz":-540,"elapsed":873,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"38b7c45b-ffad-4fd5-a4e3-7bd1cf507beb"},"execution_count":110,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["## 7. 모델 평가"],"metadata":{"id":"FsHAcmq95ddi"}},{"cell_type":"code","source":["new_model = torch.load('/content/drive/MyDrive/Playdata_Python/final_project/models/summary_dataset/summary_model_v1_1.pth')"],"metadata":{"id":"eBOAxwdLlUM_","executionInfo":{"status":"ok","timestamp":1694362990758,"user_tz":-540,"elapsed":433,"user":{"displayName":"David Choi","userId":"14870253108450991755"}}},"execution_count":111,"outputs":[]},{"cell_type":"code","source":["new_model.eval()  # 모델을 평가 모드로 설정\n","\n","epoch_loss = 0.0\n","\n","with torch.no_grad():  # 그라디언트 업데이트 비활성화\n","\n","    for batch in valid_dl:\n","        try:\n","            input_ids = batch['input_ids'].to(device)\n","            decoder_input_ids = batch['decoder_input_ids'].to(device)\n","\n","            logits = new_model(input_ids, decoder_input_ids).logits\n","\n","            # 패딩을 제거하고 실제 레이블을 얻기 위해 decoder_input_ids에서 패딩 토큰을 제거\n","            labels = decoder_input_ids[:, 1:].contiguous()  # 시작 토큰을 제외하고 실제 레이블을 가져옴\n","\n","            # 손실 계산\n","            logits_flat = logits.view(-1, logits.shape[-1])  # 로짓 텐서를 얻고나서 view 메서드 적용\n","            labels_flat = labels.view(-1)\n","            loss = criterion(logits_flat[:labels_flat.shape[0]], labels_flat)\n","\n","            # 전체 손실 값 계산\n","            epoch_loss += loss.item()\n","\n","        except KeyboardInterrupt:\n","            print(\"KeyboardInterrupt: Exiting validation...\")\n","            break\n","\n","print(f'Validation Loss: {epoch_loss/len(valid_dl)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sdDR_5LX3w4m","executionInfo":{"status":"ok","timestamp":1694363042956,"user_tz":-540,"elapsed":23028,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"54fbb581-fe2b-443d-c8be-a3323cb51b14"},"execution_count":114,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss: 6.2557912674270755\n"]}]},{"cell_type":"code","source":["original_sen_list[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"73hcSHzqmiDm","executionInfo":{"status":"ok","timestamp":1694363000605,"user_tz":-540,"elapsed":886,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"a02f2708-0b15-42a8-8d1a-fb338cf61787"},"execution_count":112,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'정은경 질병관리청장과 봉준호 감독이 미국의 시사주간지 타임(Time)이 선정하는 ‘2020 세계에서 가장 영향력 있는 100인’ 명단에 나란히 이름을 올렸다. 청와대 관계자는 23일 “이번 선정은 K방역이 전 세계가 본받아야 할 모범으로 인정받았다는 점을 확인해주는 데 의미가 있다”며 “방역과 관련해 뛰어난 성과와 업적을 보인다는 점에서 정 청장을 선정한 것”이라고 말했다. 정 청장은 리더스 부분에 등재됐다. 아티스트 부분에 이름을 올린 봉 감독에 대해서도 “매우 기쁜 소식이며 축하의 말씀을 전한다”고 했다. 특히 문재인 대통령은 정 청장에 대한 소개글을 타임에 전달했다. 선정된 100인 소개글 가운데 현직 대통령의 글은 문 대통령이 유일하다.  문 대통령은 소개글에서 “코로나19 팬데믹 상황에서 한국의 방역은 세계의 모범이 됐고, 정 청장은 방역의 최전방에서 국민과 진솔하게 소통해 K방역을 성공으로 이끌었다”고 적었다. 이어 “한국에 첫 확진자가 발생했을 때 정 청장은 정부를 대표해 국민 앞에 섰고 매일 투명하게 상황을 발표했다”며 “질병관리청 최초의 여성 수장으로서 코로나 발생 6개월 전부터 ‘원인불명 집단감염 대응절차’ 매뉴얼을 마련하는 등 질병관리청을 준비된 조직으로 이끌었다”고 평가했다. 문 대통령은 알베르 카뮈의 소설 『페스트』에 등장하는 ‘페스트와 싸우는 유일한 방법은 성실성’이라는 문구를 인용하며 “정 청장의 성실성이야말로 세계 곳곳에서 코로나와 맞서는 수많은 ‘정은경’들에게, 그리고 포스트 코로나 시대를 연 인류 모두에게 영감을 주는 얘기”라고 강조했다. 앞서 문 대통령은 지난 11일 충북 청주에 있는 질병관리본부(질본)를 찾아 정 청장에게 임명장을 수여했다. 정 청장은 차관급이다. 이번 정부 들어 장ㆍ차관급을 통틀어 문 대통령이 현장을 찾아 임명식을 진행한 것은 이때가 처음이다. 이와 관련 청와대 관계자는 “전쟁 중에 야전사령관을 불러 임명장을 주는 것이 아닌 직접 가서 임명장을 드리는 것이기 때문에 초대 청장에 대한 신뢰와 기대 의미를 갖고 있다”고 설명했다. 문 대통령은 지난 2월 20일 영화 ‘기생충’으로 아카데미 작품상 등 4관왕을 차지한 봉준호 감독과 출연진ㆍ제작진을 청와대로 초청해 오찬을 했었다. 당시 오찬 메뉴는 김정숙 여사가 요리한 ‘짜파구리’였다. 봉 감독의 소개글은 영화 ‘설국열차’에 출연했던 배우 틸다 시윈튼이 작성했다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":112}]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# 한글 텍스트를 토큰화하고 요약된 결과를 리스트에 저장\n","new_summary_sen_by_model_list = []\n","\n","for original_sen in original_sen_list[:20]:\n","    # 문장을 토큰화\n","    input_ids = tokenizer.encode(original_sen, return_tensors=\"pt\")\n","    input_ids = input_ids.to(device)\n","\n","    # 모델을 사용하여 요약 생성\n","    summary_ids = new_model.generate(input_ids, max_length=150, num_beams=4, length_penalty=2.0, early_stopping=True).to(device)\n","    summary_ids = summary_ids.to(device)\n","\n","    # 요약 결과를 디코딩하여 텍스트로 변환\n","    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","\n","    new_summary_sen_by_model_list.append(summary_text)\n","\n","new_summary_sen_by_model_list[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZEwsXLtbem7w","executionInfo":{"status":"ok","timestamp":1694363083222,"user_tz":-540,"elapsed":13832,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"fa9ada3d-0ecb-4a31-8118-9609388c69d3"},"execution_count":115,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로',\n"," '로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로',\n"," '로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로',\n"," '로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로',\n"," '로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로로']"]},"metadata":{},"execution_count":115}]},{"cell_type":"code","source":["# BLEU 점수 계산\n","nltk.download('punkt')\n","\n","# 문장을 토큰 리스트로 변환하는 함수\n","def tokenize_sentence(sentence):\n","    return nltk.word_tokenize(sentence)\n","\n","num_sentences = 20  # 20개로 test\n","total_belu_score = []\n","\n","for i in range(num_sentences):\n","\n","    # Reference 문장과 Candidate 문장 토큰화\n","    reference_tokens = tokenize_sentence(summary_sen_list[i])\n","    candidate_tokens = tokenize_sentence(new_summary_sen_by_model_list[i])\n","\n","    # BLEU Score 계산\n","    bleu_score = sentence_bleu([reference_tokens], candidate_tokens, weights=(1, 0, 0, 0))  # (원본 번역문, 모델 번역문)\n","    total_belu_score.append(bleu_score)\n","\n","    if i < 20:\n","        # BLEU 점수 출력\n","        print(f\"{i}번째 | 원본 번역본: {summary_sen_list[i]}\")\n","        print(f\"{i}번째 | 모델 번역본: {new_summary_sen_by_model_list[i]}\")\n","        print(f\"{i}번째 | BLEU 점수: {bleu_score}\")\n","        print(\"----------------------------------------------------------------------------------------------------\")\n","    else:\n","        pass\n","\n","# 전체 blue 평균 계산\n","average_bleu_score = sum(total_belu_score) / num_sentences\n","print(f\"100문장에 대한 Belu Score 평균: {average_bleu_score}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xe2Jcwovem7z","executionInfo":{"status":"ok","timestamp":1694357749928,"user_tz":-540,"elapsed":678,"user":{"displayName":"David Choi","userId":"14870253108450991755"}},"outputId":"b67b8a55-df25-4ab9-c112-8ae99544ba3b"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["0번째 | 원본 번역본: 정 청장과 봉 감독이 2020 세계에서 가장 영향력 있는 100인 명단에 오르자 청와대 관계자는 K방역이 모범으로 인정받았음을 확인했다고 말하며 봉 감독에게는 축하를 전했다.\n","0번째 | 모델 번역본: 로로이이\n","0번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","1번째 | 원본 번역본: 인간을 탐구하는 학자와 같이 창업가는 고객이 집 등에서 어떤 행동을 하는지 관찰하고 어떤 부분에서 짜증을 느끼는지 공감하는 과정을 통해 기존에 없던 방식으로 혁신을 만든다.\n","1번째 | 모델 번역본: 로로\n","1번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","2번째 | 원본 번역본: 프랜차이즈 본사가 광고 · 판촉 행사를 하려면 미리 일정 비율 이상의 가맹점주 동의를 받아야 하며 신규 프랜차이즈 본부가 가맹점을 모집하려면 직영점을 1년 이상 운영해야 한다.\n","2번째 | 모델 번역본: 로로로\n","2번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","3번째 | 원본 번역본: 테슬라 최고 경영자가 테슬라 역사상 신나는 날이 될 것 이라고 트윗을 날린 뒤 배터리 데이는 테슬라 주가 상승을 견인한 호재 중 하나로 꼽았지만 뚜껑을 열어보니 정반대였다.\n","3번째 | 모델 번역본: 에로로에\n","3번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","4번째 | 원본 번역본: 기업은행 셀프대출을 통해 기업은행 A차장은 경기도 화성 일대 오피스텔 연립주택을 사들인 뒤 평가차익을 올렸고 이를 적발한 기업은행은 A차장을 면직 처분했다.\n","4번째 | 모델 번역본: 로로에\n","4번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","5번째 | 원본 번역본: 독감 백신 중단 사태에 백신에 대한 불신이 퍼지며 유통업체 선정 또한 늦어져 정부 책임론이 나오는 등 후폭풍이 거세다.\n","5번째 | 모델 번역본: 에에로이\n","5번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","6번째 | 원본 번역본: 건조일수가 늘어나 산불 위험이 증가했으며 권 박사는 대형 산불의 안전지대는 없다고 경고했다.\n","6번째 | 모델 번역본: 에에에\n","6번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","7번째 | 원본 번역본: 고용부는 코로나19로 인해 증가한 무급 휴직자를 위해 고용유지 지원금 지급 조건의 무급 휴직 기간을 30일 이상으로 완화시켰다.\n","7번째 | 모델 번역본: 로에로로로에에\n","7번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","8번째 | 원본 번역본: 정부는 전국민 고용보험을 추진하고 있고 예술인과 다양한 형태의 종사자들도 고용보험에 가입할 수 있도록 하는 법 개정안을 제출한 바 있다.\n","8번째 | 모델 번역본: 에로\n","8번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","9번째 | 원본 번역본: 지상파 예능에서 부동산 전문가와 현장을 답사하며 알짜 부동산 정보를 제공하는 것에 대해 전문가들은 부동산에 대한 대중들의 관심을 방송이 반영한 것으로 본다.\n","9번째 | 모델 번역본: 로에이\n","9번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","10번째 | 원본 번역본: 서울의 한 중개사는 과거와 달리 부동산에 대한 투자적 관심이 높아졌다고 전한 가운데 부동산 예능이 지상파에서 본격화되는 것에 대한 우려의 시선도 있다.\n","10번째 | 모델 번역본: 로에에\n","10번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","11번째 | 원본 번역본: A 씨의 자가격리 위반 갈등은 코로나19 확진 판정을 받으면서 불거졌는데 확진 판정을 받은 후 나흘 동안 순천의 장례식에 머문 사실을 순천시가 알게 됐기 때문이다.\n","11번째 | 모델 번역본: 에에이\n","11번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","12번째 | 원본 번역본: 송도에 입주해 있는 벤쳐기업은 인천경제자유구역청과 르호봇비즈니스인큐베이터의 업무협약 체결 덕분에 르호봇에서 유연하게 연구 및 사무 공간을 확보할 수 있게 되었다.\n","12번째 | 모델 번역본: 로로로에에\n","12번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","13번째 | 원본 번역본: 코로나19 재확산에 따라 사회적 거리 두기가 강화되자 홍 구청장은 한발 앞선 행정 추진으로 민생 현장을 적극적으로 챙기고 있다.\n","13번째 | 모델 번역본: 로로로로\n","13번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","14번째 | 원본 번역본: 더불어민주당이 국제 법제사법위 1소위에서 김 의원이 대표발의한 공수처법 개정안을 기습 상정하면서 회의가 파행되었다.\n","14번째 | 모델 번역본: 로19에\n","14번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","15번째 | 원본 번역본: 문 특보는 국내 언론 기고에서 워싱턴의 이분법적 외교정책을 비판하는가 하면 중국 외교는 패도 내지 강건 외교라고 미국과 중국을 동시에 비판했다.\n","15번째 | 모델 번역본: 로로로에에\n","15번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","16번째 | 원본 번역본: 분광요소법이 효율성이 있는지 아니면 쇠퇴하고 있는 방식인지에 대해 각각의 주장이 맞서고 있다.\n","16번째 | 모델 번역본: 로로이\n","16번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","17번째 | 원본 번역본: 슈퍼컴퓨터 도입을 고려하면 분광요소법이 적절하다는 게 기상학계의 견해이며 미국 해양기상청의 보고서에 따르면 분광요소법은 차세대 컴퓨터 기술에 최적이라고 한다.\n","17번째 | 모델 번역본: 이\n","17번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","18번째 | 원본 번역본: 지구 평균기온이 1도 올라감에 따라 변해야 할 날씨는 변하지 않고 변하지 말아야 할 기후는 변하고 있는 상태에서 장마가 아닌 기후위기임을 실감한 건 2020년 여름이다.\n","18번째 | 모델 번역본: 과에\n","18번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","19번째 | 원본 번역본: 네이버 뉴스는 직접 고른 언론사·기자 및 AI가 골라주는 개인 맞춤형 뉴스 소비를 위해 편집 방식을 변경했다.\n","19번째 | 모델 번역본: 에로\n","19번째 | BLEU 점수: 0\n","----------------------------------------------------------------------------------------------------\n","100문장에 대한 Belu Score 평균: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"bVKbbmHEl3oB"},"execution_count":null,"outputs":[]}]}