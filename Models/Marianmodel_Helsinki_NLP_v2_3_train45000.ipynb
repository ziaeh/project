{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC0SlFtr2dDs"
      },
      "source": [
        "- https://huggingface.co/Helsinki-NLP/opus-mt-ko-en\n",
        "- https://huggingface.co/docs/transformers/model_doc/marian\n",
        "- https://github.com/huggingface/transformers/blob/v4.33.0/src/transformers/models/marian/modeling_marian.py#L1106"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHX_1i3e8uku",
        "outputId": "bbf0190a-175e-4acb-bc44-292c239e5e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o_ltyOg42XZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4833131c-0227-4234-b75c-df27eebafb94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.0.53)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install --upgrade transformers\n",
        "!pip install accelerate -U\n",
        "!pip install sentencepiece\n",
        "!pip install sacremoses\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qBBWwi0h2kKV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import MarianTokenizer, MarianMTModel, MarianConfig, MarianModel\n",
        "from transformers import AdamW, get_scheduler\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ5vAyT5_hxi"
      },
      "source": [
        "# 1.데이터셋 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "bQvYzxYa8VBt",
        "outputId": "dcb1587a-2841-41b3-d33c-13895ee08a4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  원문  \\\n",
              "0                                    그들 체면이 말이 아닙니다.   \n",
              "1           한화의 공격력이 나아지지 않는다면 지난해와 같은 성적은 기대하기 어렵다.   \n",
              "2  오프라인 프로그램에서는 창의적 사고기법과 문제해결방법, 의사소통 능력과 협업능력 등...   \n",
              "3  행사에는 유주현 대한건설협회 회장을 비롯해 최영묵 건설공제조합 이사장, 임영헌 대한...   \n",
              "4                       이들은 데이터 무제한 요금제가 있다는 것에 놀랐다.   \n",
              "\n",
              "                                                 번역문  \n",
              "0      They ought to be ashamed of their reputation.  \n",
              "1  If Hanwha's offense doesn't improve, it's hard...  \n",
              "2  In the offline program, creativity and charact...  \n",
              "3  More than 100 people attended the event, inclu...  \n",
              "4  They were surprised that there was an unlimite...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bf049e9f-f702-4710-bdc9-3e27dbd60bff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>원문</th>\n",
              "      <th>번역문</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>그들 체면이 말이 아닙니다.</td>\n",
              "      <td>They ought to be ashamed of their reputation.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>한화의 공격력이 나아지지 않는다면 지난해와 같은 성적은 기대하기 어렵다.</td>\n",
              "      <td>If Hanwha's offense doesn't improve, it's hard...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>오프라인 프로그램에서는 창의적 사고기법과 문제해결방법, 의사소통 능력과 협업능력 등...</td>\n",
              "      <td>In the offline program, creativity and charact...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>행사에는 유주현 대한건설협회 회장을 비롯해 최영묵 건설공제조합 이사장, 임영헌 대한...</td>\n",
              "      <td>More than 100 people attended the event, inclu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>이들은 데이터 무제한 요금제가 있다는 것에 놀랐다.</td>\n",
              "      <td>They were surprised that there was an unlimite...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf049e9f-f702-4710-bdc9-3e27dbd60bff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bf049e9f-f702-4710-bdc9-3e27dbd60bff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bf049e9f-f702-4710-bdc9-3e27dbd60bff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c6a005f3-e04d-4f22-8112-dd2eb837f89d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c6a005f3-e04d-4f22-8112-dd2eb837f89d')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c6a005f3-e04d-4f22-8112-dd2eb837f89d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# 데이터 경로\n",
        "path = \"/content/drive/MyDrive/data/shuffled_kor_eng_concat_dataset.csv\"\n",
        "\n",
        "# 데이터 불러오기\n",
        "df = pd.read_csv(path, encoding='utf-8')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kus4JCp_hKV",
        "outputId": "7a242725-7d25-4235-fb4b-0bce480c8b76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((45000, 2), (4000, 2), (1000, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# 데이터 분할\n",
        "train_df = df[:45000]   #5만개\n",
        "valid_df = df[45000:49000]\n",
        "test_df = df[49000:50000]\n",
        "\n",
        "train_df.shape, valid_df.shape, test_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_train = train_df['원문']\n",
        "target_train = train_df['번역문']\n",
        "\n",
        "input_valid = valid_df['원문']\n",
        "target_valid = valid_df['번역문']\n",
        "\n",
        "input_test = test_df['원문']\n",
        "target_test = test_df['번역문']\n",
        "\n",
        "input_train.shape, target_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqogkLbzaS_a",
        "outputId": "eb64b373-1cc4-4994-b479-7c9fe2ef1e5e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((45000,), (45000,))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGUpZtSIAquN"
      },
      "source": [
        "# 2.기존 모델 및 토크나이저 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rT2_3VLp7fww"
      },
      "outputs": [],
      "source": [
        "# 모델 정의\n",
        "model_name = \"Helsinki-NLP/opus-mt-ko-en\"\n",
        "\n",
        "# 토크나이저 불러오기\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 모델 불러오기\n",
        "model = MarianMTModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXQ-dzgR6dQo"
      },
      "outputs": [],
      "source": [
        "# 번역할 텍스트를 정의\n",
        "input_text = \"안녕하세요, 과연 영어로 번역이 가능한가요?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ8DTOmv3e3x",
        "outputId": "10d1b2e2-37de-46b6-c7d5-c1512c64c0fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 4192,     3,  8906, 40529, 12496,    48,  5403,  7083,     7,     0]])\n",
            "tensor([[65000,  3306,     2,   622,    18, 17452,   177,  2411,     7,     0]])\n",
            "번역 결과: Hello. Can you translate into English?\n"
          ]
        }
      ],
      "source": [
        "# 한글 텍스트를 토큰화하고 번역 수행\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "print(input_ids)\n",
        "\n",
        "translated_ids = model.generate(input_ids, num_beams=4, max_length=150, early_stopping=True)\n",
        "print(translated_ids)\n",
        "\n",
        "translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# 번역 결과 출력\n",
        "print(\"번역 결과:\", translated_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translated = model.generate(**tokenizer(input_text, return_tensors=\"pt\", padding=True))\n",
        "print(translated)\n",
        "\n",
        "tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
        "tgt_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PagyroQDGC84",
        "outputId": "dbea5017-0531-4604-a488-d8b5951632b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[65000,  3306,     2,   622,    18, 17452,   177,  2411,     7,     0]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello. Can you translate into English?']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ6YIV6xAvTK"
      },
      "source": [
        "# 3.기존 모델 BeLU 측정 - 10개"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_test = test_df['원문']\n",
        "target_test = test_df['번역문']"
      ],
      "metadata": {
        "id": "mPApfL-0aYo4"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cc--Gh0BDo5",
        "outputId": "a9419f17-7b45-4405-9dde-96f160448bec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['대학 1학년에서는 기본적으로 여러 과목들을 배웁니다.',\n",
              "  '경찰과 소방당국은 김 씨가 암벽에서 발을 헛디뎌 사고가 난 것으로 추정하고 있으며 정확한 사고 경위를 조사 중이다.',\n",
              "  '나는 기술팀에 있는 챈이라고 합니다.',\n",
              "  '편지 속의 그 문구는 자연스럽지 않습니다.',\n",
              "  '인터넷을 통해 마음이 맞는 사람들끼리 그룹을 만들어서 한국어를 배우고, 모임을 갖고 만나면서 한국어로 대화를 나누는 사람들도 증가하고 있다.'],\n",
              " ['When you are a freshman, you basically learn various subjects.',\n",
              "  'Police and fire authorities suspect that Kim lost his footing on the rock wall and caused the accident, and are investigating the exact circumstances.',\n",
              "  \"I'm Chan from the technical department.\",\n",
              "  'That words in the letter are not natural.',\n",
              "  'There are also increasing numbers of people who learn Korean by forming a group of people that they like on the internet, and have a meeting and talk in Korean.'])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# 원본과 번역본\n",
        "original_text_list = input_test[:10].tolist()      # 한글\n",
        "translated_text_list = target_test[:10].tolist()   # 영어\n",
        "\n",
        "original_text_list[:5], translated_text_list[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2xIvPORBWgE",
        "outputId": "06fab9f5-560f-4612-c08c-57acee77aaac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In my freshman year in college, I basically learn a lot of subjects.',\n",
              " \"Police and fire departments estimate Mr. Kim's foot has been knocked off the rock wall, and they are investigating exactly how the accident took place.\",\n",
              " \"I'm Chen on the tech team.\",\n",
              " 'That phrase in the letter is not natural.',\n",
              " 'More and more people are using the Internet to create groups of right-hearted people who are learning Korean, who are meeting with the group, and talking to them in Korean.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# 한글 텍스트를 토큰화하고 번역된 결과를 리스트에 저장하는 테스트\n",
        "modeling_text_list = []\n",
        "\n",
        "for korean in original_text_list:\n",
        "    input_ids = tokenizer.encode(korean, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    translated_ids = model.generate(input_ids, num_beams=4, max_length=150, early_stopping=True)\n",
        "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
        "    modeling_text_list.append(translated_text)\n",
        "\n",
        "modeling_text_list[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5NNkcSwAu2u",
        "outputId": "046f53e9-9edc-4307-887f-22991d3d4609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0번째 | 원본 번역본: When you are a freshman, you basically learn various subjects.\n",
            "0번째 | 모델 번역본: In my freshman year in college, I basically learn a lot of subjects.\n",
            "0번째 | 원본 번역본 토큰: ['▁W', 'h', 'en', '▁you', '▁are', '▁a', '▁f', 're', 'sh', 'man', ',', '▁you', '▁b', 'as', 'ic', 'ally', '▁l', 'ear', 'n', '▁', 'va', 'ri', 'ous', '▁s', 'ub', 'ject', 's', '.']\n",
            "0번째 | 모델 번역본 토큰: ['▁In', '▁my', '▁f', 're', 'sh', 'man', '▁y', 'ear', '▁in', '▁c', 'ol', 'le', 'ge', ',', '▁I', '▁b', 'as', 'ic', 'ally', '▁l', 'ear', 'n', '▁a', '▁l', 'ot', '▁of', '▁s', 'ub', 'ject', 's', '.']\n",
            "0번째 | BLEU 점수: 0.5806451612903226\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1번째 | 원본 번역본: Police and fire authorities suspect that Kim lost his footing on the rock wall and caused the accident, and are investigating the exact circumstances.\n",
            "1번째 | 모델 번역본: Police and fire departments estimate Mr. Kim's foot has been knocked off the rock wall, and they are investigating exactly how the accident took place.\n",
            "1번째 | 원본 번역본 토큰: ['▁P', 'ol', 'ice', '▁and', '▁f', 'ire', '▁a', 'ut', 'ho', 'rit', 'ies', '▁s', 'us', 'pe', 'c', 't', '▁that', '▁K', 'im', '▁l', 'ost', '▁his', '▁f', 'oo', 'ting', '▁on', '▁the', '▁r', 'ock', '▁w', 'all', '▁and', '▁', 'ca', 'us', 'ed', '▁the', '▁a', 'cc', 'id', 'ent', ',', '▁and', '▁are', '▁in', 've', 'st', 'ig', 'at', 'ing', '▁the', '▁', 'ex', 'act', '▁c', 'ir', 'c', 'um', 'st', 'ance', 's', '.']\n",
            "1번째 | 모델 번역본 토큰: ['▁P', 'ol', 'ice', '▁and', '▁f', 'ire', '▁de', 'p', 'art', 'ment', 's', '▁', 'est', 'im', 'ate', '▁Mr', '.', '▁K', 'im', \"'\", 's', '▁f', 'o', 'ot', '▁h', 'as', '▁be', 'en', '▁k', 'n', 'ock', 'ed', '▁', 'off', '▁the', '▁r', 'ock', '▁w', 'all', ',', '▁and', '▁they', '▁are', '▁in', 've', 'st', 'ig', 'at', 'ing', '▁', 'ex', 'act', 'ly', '▁h', 'ow', '▁the', '▁a', 'cc', 'id', 'ent', '▁t', 'ook', '▁', 'pl', 'ace', '.']\n",
            "1번째 | BLEU 점수: 0.5303030303030303\n",
            "----------------------------------------------------------------------------------------------------\n",
            "2번째 | 원본 번역본: I'm Chan from the technical department.\n",
            "2번째 | 모델 번역본: I'm Chen on the tech team.\n",
            "2번째 | 원본 번역본 토큰: ['▁I', \"'\", 'm', '▁C', 'han', '▁from', '▁the', '▁', 'te', 'ch', 'n', 'ical', '▁de', 'p', 'art', 'ment', '.']\n",
            "2번째 | 모델 번역본 토큰: ['▁I', \"'\", 'm', '▁Ch', 'en', '▁on', '▁the', '▁', 'te', 'ch', '▁', 'te', 'am', '.']\n",
            "2번째 | BLEU 점수: 0.4612101411459368\n",
            "----------------------------------------------------------------------------------------------------\n",
            "3번째 | 원본 번역본: That words in the letter are not natural.\n",
            "3번째 | 모델 번역본: That phrase in the letter is not natural.\n",
            "3번째 | 원본 번역본 토큰: ['▁That', '▁w', 'ord', 's', '▁in', '▁the', '▁l', 'et', 'ter', '▁are', '▁not', '▁', 'na', 'tu', 'ral', '.']\n",
            "3번째 | 모델 번역본 토큰: ['▁That', '▁p', 'hr', 'ase', '▁in', '▁the', '▁l', 'et', 'ter', '▁is', '▁not', '▁', 'na', 'tu', 'ral', '.']\n",
            "3번째 | BLEU 점수: 0.75\n",
            "----------------------------------------------------------------------------------------------------\n",
            "4번째 | 원본 번역본: There are also increasing numbers of people who learn Korean by forming a group of people that they like on the internet, and have a meeting and talk in Korean.\n",
            "4번째 | 모델 번역본: More and more people are using the Internet to create groups of right-hearted people who are learning Korean, who are meeting with the group, and talking to them in Korean.\n",
            "4번째 | 원본 번역본 토큰: ['▁The', 're', '▁are', '▁', 'al', 's', 'o', '▁in', 'c', 're', 'as', 'ing', '▁n', 'um', 'ber', 's', '▁of', '▁', 'pe', 'op', 'le', '▁who', '▁l', 'ear', 'n', '▁K', 'ore', 'an', '▁by', '▁for', 'm', 'ing', '▁a', '▁g', 'ro', 'up', '▁of', '▁', 'pe', 'op', 'le', '▁that', '▁they', '▁like', '▁on', '▁the', '▁in', 'ter', 'net', ',', '▁and', '▁have', '▁a', '▁me', 'et', 'ing', '▁and', '▁t', 'al', 'k', '▁in', '▁K', 'ore', 'an', '.']\n",
            "4번째 | 모델 번역본 토큰: ['▁M', 'ore', '▁and', '▁m', 'ore', '▁', 'pe', 'op', 'le', '▁are', '▁', 'us', 'ing', '▁the', '▁In', 'ter', 'net', '▁to', '▁c', 're', 'ate', '▁', 'gr', 'ou', 'ps', '▁of', '▁right', '-', 'he', 'art', 'ed', '▁', 'pe', 'op', 'le', '▁who', '▁are', '▁l', 'ear', 'n', 'ing', '▁K', 'ore', 'an', ',', '▁who', '▁are', '▁me', 'et', 'ing', '▁with', '▁the', '▁g', 'ro', 'up', ',', '▁and', '▁t', 'al', 'king', '▁to', '▁them', '▁in', '▁K', 'ore', 'an', '.']\n",
            "4번째 | BLEU 점수: 0.5970149253731343\n",
            "----------------------------------------------------------------------------------------------------\n",
            "5번째 | 원본 번역본: The health authority requested to take caution to prevent the breakout of heatstrokes, as severe heat is expected for this week as well.\n",
            "5번째 | 모델 번역본: Health authorities are calling on us to be careful this week to avoid the expected extreme heat wave.\n",
            "5번째 | 원본 번역본 토큰: ['▁The', '▁he', 'al', 'th', '▁', 'au', 'th', 'or', 'ity', '▁re', 'que', 's', 'ted', '▁to', '▁t', 'ake', '▁c', 'au', 'tion', '▁to', '▁p', 're', 'v', 'ent', '▁the', '▁b', 're', 'ak', 'out', '▁of', '▁he', 'at', 'st', 'ro', 'k', 'es', ',', '▁as', '▁s', 'ever', 'e', '▁he', 'at', '▁is', '▁', 'ex', 'pe', 'c', 'ted', '▁for', '▁this', '▁w', 'eek', '▁as', '▁w', 'ell', '.']\n",
            "5번째 | 모델 번역본 토큰: ['▁He', 'al', 'th', '▁a', 'ut', 'ho', 'rit', 'ies', '▁are', '▁', 'cal', 'ling', '▁on', '▁', 'us', '▁to', '▁be', '▁c', 'ar', 'ef', 'ul', '▁this', '▁w', 'eek', '▁to', '▁a', 'vo', 'id', '▁the', '▁', 'ex', 'pe', 'c', 'ted', '▁', 'ext', 're', 'me', '▁he', 'at', '▁w', 'ave', '.']\n",
            "5번째 | BLEU 점수: 0.33586390776285685\n",
            "----------------------------------------------------------------------------------------------------\n",
            "6번째 | 원본 번역본: I guess that's fair.\n",
            "6번째 | 모델 번역본: Maybe that's fair to everyone.\n",
            "6번째 | 원본 번역본 토큰: ['▁I', '▁g', 'u', 'ess', '▁that', \"'\", 's', '▁f', 'a', 'ir', '.']\n",
            "6번째 | 모델 번역본 토큰: ['▁M', 'ay', 'be', '▁that', \"'\", 's', '▁f', 'a', 'ir', '▁to', '▁e', 'very', 'one', '.']\n",
            "6번째 | BLEU 점수: 0.5\n",
            "----------------------------------------------------------------------------------------------------\n",
            "7번째 | 원본 번역본: The Korean wave, such as Korean K-POP, Korean dramas, and Korean movies, has led to an interest in Korea, and more and more tourists and international students are visiting Korea every year.\n",
            "7번째 | 모델 번역본: South Korea's KPOP and Korean dramas, Korean films, and so on, are a growing trend among tourists and students who visit South Korea each year.\n",
            "7번째 | 원본 번역본 토큰: ['▁The', '▁K', 'ore', 'an', '▁w', 'ave', ',', '▁s', 'uch', '▁as', '▁K', 'ore', 'an', '▁K', '-', 'P', 'OP', ',', '▁K', 'ore', 'an', '▁dr', 'am', 'as', ',', '▁and', '▁K', 'ore', 'an', '▁m', 'ov', 'ies', ',', '▁h', 'as', '▁l', 'ed', '▁to', '▁', 'an', '▁in', 'ter', 'est', '▁in', '▁K', 'ore', 'a', ',', '▁and', '▁m', 'ore', '▁and', '▁m', 'ore', '▁to', 'ur', 'ist', 's', '▁and', '▁', 'int', 'ern', 'ation', 'al', '▁st', 'ud', 'ent', 's', '▁are', '▁', 'vi', 'si', 'ting', '▁K', 'ore', 'a', '▁e', 'very', '▁y', 'ear', '.']\n",
            "7번째 | 모델 번역본 토큰: ['▁S', 'out', 'h', '▁K', 'ore', 'a', \"'\", 's', '▁K', 'P', 'OP', '▁and', '▁K', 'ore', 'an', '▁dr', 'am', 'as', ',', '▁K', 'ore', 'an', '▁f', 'il', 'm', 's', ',', '▁and', '▁so', '▁on', ',', '▁are', '▁a', '▁g', 'row', 'ing', '▁t', 'r', 'end', '▁a', 'mon', 'g', '▁to', 'ur', 'ist', 's', '▁and', '▁st', 'ud', 'ent', 's', '▁who', '▁v', 'is', 'it', '▁S', 'out', 'h', '▁K', 'ore', 'a', '▁e', 'ach', '▁y', 'ear', '.']\n",
            "7번째 | BLEU 점수: 0.4466367937281526\n",
            "----------------------------------------------------------------------------------------------------\n",
            "8번째 | 원본 번역본: The uniform fits me well so that I am happy about it.\n",
            "8번째 | 모델 번역본: He's got a happy heart. He's got a good school uniform.\n",
            "8번째 | 원본 번역본 토큰: ['▁The', '▁', 'un', 'if', 'or', 'm', '▁f', 'it', 's', '▁me', '▁w', 'ell', '▁so', '▁that', '▁I', '▁', 'am', '▁h', 'app', 'y', '▁about', '▁it', '.']\n",
            "8번째 | 모델 번역본 토큰: ['▁He', \"'\", 's', '▁got', '▁a', '▁h', 'app', 'y', '▁he', 'art', '.', '▁He', \"'\", 's', '▁got', '▁a', '▁good', '▁s', 'ch', 'ool', '▁', 'un', 'if', 'or', 'm', '.']\n",
            "8번째 | BLEU 점수: 0.38461538461538464\n",
            "----------------------------------------------------------------------------------------------------\n",
            "9번째 | 원본 번역본: Kim went ahead and explained the problems of the educational content.\n",
            "9번째 | 모델 번역본: The problem with the education was that Mr. Kim came up and explained:\n",
            "9번째 | 원본 번역본 토큰: ['▁K', 'im', '▁w', 'ent', '▁a', 'he', 'ad', '▁and', '▁', 'ex', 'pl', 'ain', 'ed', '▁the', '▁pro', 'bl', 'em', 's', '▁of', '▁the', '▁e', 'du', 'c', 'ation', 'al', '▁', 'con', 't', 'ent', '.']\n",
            "9번째 | 모델 번역본 토큰: ['▁The', '▁pro', 'bl', 'em', '▁with', '▁the', '▁e', 'du', 'c', 'ation', '▁was', '▁that', '▁Mr', '.', '▁K', 'im', '▁c', 'ame', '▁up', '▁and', '▁', 'ex', 'pl', 'ain', 'ed', ':']\n",
            "9번째 | BLEU 점수: 0.5606102548356732\n",
            "----------------------------------------------------------------------------------------------------\n",
            "100문장에 대한 Belu Score 평균: 0.5146899599054491\n"
          ]
        }
      ],
      "source": [
        "# 문장을 토큰 리스트로 변환하는 함수\n",
        "def num_to_char(input_ids):\n",
        "    text = []\n",
        "    vocab = tokenizer.get_vocab()\n",
        "    for number in input_ids:\n",
        "        if number in [0, 65000]:\n",
        "            continue  # number가 0, 65000이면 다음 반복으로 넘어감\n",
        "        char = next(key for key, value in vocab.items() if value == number)\n",
        "        text.append(char)\n",
        "    return text\n",
        "\n",
        "\n",
        "# 모델 정의\n",
        "model_name = \"Helsinki-NLP/opus-mt-ko-en\"\n",
        "# 토크나이저 불러오기\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "num_sentences = 10  # 10개로 test함으로 범위를 10으로 지정\n",
        "total_belu_score = []\n",
        "\n",
        "for i in range(num_sentences):\n",
        "\n",
        "    # Reference 문장과 Candidate 문장 토큰화\n",
        "    reference_tokens = num_to_char(tokenizer(translated_text_list[i])['input_ids'])\n",
        "    candidate_tokens = num_to_char(tokenizer(modeling_text_list[i])['input_ids'])\n",
        "\n",
        "    # BLEU Score 계산\n",
        "    bleu_score = sentence_bleu([reference_tokens], candidate_tokens, weights=(1, 0, 0, 0))  # (원본 번역문, 모델 번역문)\n",
        "    total_belu_score.append(bleu_score)\n",
        "\n",
        "    if i < 20:\n",
        "        # BLEU 점수 출력\n",
        "        print(f\"{i}번째 | 원본 번역본: {translated_text_list[i]}\")\n",
        "        print(f\"{i}번째 | 모델 번역본: {modeling_text_list[i]}\")\n",
        "        print(f\"{i}번째 | 원본 번역본 토큰: {reference_tokens}\")\n",
        "        print(f\"{i}번째 | 모델 번역본 토큰: {candidate_tokens}\")\n",
        "        print(f\"{i}번째 | BLEU 점수: {bleu_score}\")\n",
        "        print(\"----------------------------------------------------------------------------------------------------\")\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "# 전체 blue 평균 계산\n",
        "average_bleu_score = sum(total_belu_score) / num_sentences\n",
        "print(f\"100문장에 대한 Belu Score 평균: {average_bleu_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KraJzXS9HZSa"
      },
      "source": [
        "# 4.Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1bzgiIBU4mr"
      },
      "source": [
        "### 4-1. Dataset 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gMrM1qy7JYvO"
      },
      "outputs": [],
      "source": [
        "# 데이터 로딩 및 전처리를 위한 클래스 정의\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, source_df, target_df, tokenizer, max_length=128):\n",
        "        self.source_sentences = source_df.reset_index().drop('index', axis=1).values.tolist()\n",
        "        self.target_sentences = target_df.reset_index().drop('index', axis=1).values.tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source_text = self.source_sentences[idx][0]\n",
        "        target_text = self.target_sentences[idx][0]\n",
        "\n",
        "        # 토큰화 및 패딩\n",
        "        source_tokens = self.tokenizer(source_text, add_special_tokens=True, max_length=self.max_length, padding=\"max_length\", truncation=True)\n",
        "        target_tokens = self.tokenizer(target_text, add_special_tokens=True, max_length=self.max_length, padding=\"max_length\", truncation=True)\n",
        "\n",
        "        # 아래 부분을 수정하여 input_ids를 반환하도록 재정의\n",
        "        return {'input_ids': torch.tensor(source_tokens['input_ids'], dtype=torch.long),\n",
        "                'attention_mask': torch.tensor(source_tokens['attention_mask'], dtype=torch.long),\n",
        "                'decoder_input_ids': torch.tensor(target_tokens['input_ids'], dtype=torch.long)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R_mkbMZzSKAN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "e5b8bbac-90fb-4022-ef71-c1ccb617bbf6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  원문  \\\n",
              "0                                    그들 체면이 말이 아닙니다.   \n",
              "1           한화의 공격력이 나아지지 않는다면 지난해와 같은 성적은 기대하기 어렵다.   \n",
              "2  오프라인 프로그램에서는 창의적 사고기법과 문제해결방법, 의사소통 능력과 협업능력 등...   \n",
              "3  행사에는 유주현 대한건설협회 회장을 비롯해 최영묵 건설공제조합 이사장, 임영헌 대한...   \n",
              "4                       이들은 데이터 무제한 요금제가 있다는 것에 놀랐다.   \n",
              "\n",
              "                                                 번역문  \n",
              "0      They ought to be ashamed of their reputation.  \n",
              "1  If Hanwha's offense doesn't improve, it's hard...  \n",
              "2  In the offline program, creativity and charact...  \n",
              "3  More than 100 people attended the event, inclu...  \n",
              "4  They were surprised that there was an unlimite...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a5c7121-89bd-4b4a-8a15-ec632d077be7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>원문</th>\n",
              "      <th>번역문</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>그들 체면이 말이 아닙니다.</td>\n",
              "      <td>They ought to be ashamed of their reputation.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>한화의 공격력이 나아지지 않는다면 지난해와 같은 성적은 기대하기 어렵다.</td>\n",
              "      <td>If Hanwha's offense doesn't improve, it's hard...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>오프라인 프로그램에서는 창의적 사고기법과 문제해결방법, 의사소통 능력과 협업능력 등...</td>\n",
              "      <td>In the offline program, creativity and charact...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>행사에는 유주현 대한건설협회 회장을 비롯해 최영묵 건설공제조합 이사장, 임영헌 대한...</td>\n",
              "      <td>More than 100 people attended the event, inclu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>이들은 데이터 무제한 요금제가 있다는 것에 놀랐다.</td>\n",
              "      <td>They were surprised that there was an unlimite...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a5c7121-89bd-4b4a-8a15-ec632d077be7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2a5c7121-89bd-4b4a-8a15-ec632d077be7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2a5c7121-89bd-4b4a-8a15-ec632d077be7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a2f6fc5d-5681-4a40-a7df-c828ed22a492\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a2f6fc5d-5681-4a40-a7df-c828ed22a492')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a2f6fc5d-5681-4a40-a7df-c828ed22a492 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "EQCBS1m6Luk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04fbe71b-c5b6-440e-cab0-cef132d1d42c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [54, 633, 53, 4519, 16000, 9661, 5386, 11788, 33163, 195, 532, 12832, 92, 5321, 820, 9, 23211, 2, 0, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ],
      "source": [
        "# source_sentences = train_df['원문'].reset_index().drop('index', axis = 1).values.tolist()\n",
        "# print(source_sentences)\n",
        "\n",
        "# source_text = source_sentences[1][0]\n",
        "# print(source_text)\n",
        "\n",
        "source_tokens = tokenizer(source_text, add_special_tokens=True, max_length=128, padding=\"max_length\", truncation=True)\n",
        "print(source_tokens) # {\"input_ids\" : [...], \"attention_mask\" : [...]}\n",
        "\n",
        "# print(torch.tensor([source_tokens['input_ids'], source_tokens['attention_mask']], dtype=torch.long))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# target_sentences = train_df['번역문'].reset_index().drop('index', axis = 1).values.tolist()\n",
        "# print(target_sentences)\n",
        "\n",
        "# target_text = target_sentences[0][0]\n",
        "# print(target_text)\n",
        "\n",
        "target_tokens = tokenizer(target_text, add_special_tokens=True, max_length=128, padding=\"max_length\", truncation=True)\n",
        "print(target_tokens)  # {\"input_ids\" : [...], \"attention_mask\" : [...]}\n",
        "\n",
        "# print(torch.tensor([target_tokens['input_ids'], target_tokens['attention_mask']], dtype=torch.long))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE65wT_5Jnpf",
        "outputId": "d733ecb9-db28-46db-ee8a-58b9cae519b5"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [180, 9, 20747, 49, 5, 31, 9, 19231, 2135, 194, 6, 4, 2959, 1640, 1438, 4885, 1664, 2, 0, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psU_tXaxJYrC",
        "outputId": "98b76aed-8e6d-45b1-94e1-34300632f6bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<__main__.CustomDataset at 0x79600fb62950>,\n",
              " <__main__.CustomDataset at 0x79603d37aa10>,\n",
              " <__main__.CustomDataset at 0x79603b280370>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# 토크나이저 및 vocab 로드\n",
        "model_name = \"Helsinki-NLP/opus-mt-ko-en\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 데이터셋 생성\n",
        "train_dataset = CustomDataset(input_train, target_train, tokenizer)\n",
        "valid_dataset = CustomDataset(input_valid, target_valid, tokenizer)\n",
        "test_dataset = CustomDataset(input_test, target_test, tokenizer)\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens = tokenizer.special_tokens_map\n",
        "print(special_tokens)"
      ],
      "metadata": {
        "id": "tkRsn0YNZioC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc0e339-f488-48dc-fa42-017e0dac5bb4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = tokenizer.get_vocab()\n",
        "vocab"
      ],
      "metadata": {
        "id": "9MD8hD7EZmki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b05973c-172e-4ec3-e6d6-2cccc4f2ac9a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'</s>': 0,\n",
              " '<unk>': 1,\n",
              " '.': 2,\n",
              " ',': 3,\n",
              " '▁the': 4,\n",
              " '▁to': 5,\n",
              " '▁of': 6,\n",
              " '?': 7,\n",
              " '▁and': 8,\n",
              " '▁': 9,\n",
              " 's': 10,\n",
              " '▁a': 11,\n",
              " \"'\": 12,\n",
              " '▁in': 13,\n",
              " '▁들': 14,\n",
              " '▁that': 15,\n",
              " '▁I': 16,\n",
              " '▁”': 17,\n",
              " '▁you': 18,\n",
              " '▁“': 19,\n",
              " '▁is': 20,\n",
              " '▁이': 21,\n",
              " '▁-': 22,\n",
              " '▁을': 23,\n",
              " '▁it': 24,\n",
              " '▁의': 25,\n",
              " '▁for': 26,\n",
              " '▁은': 27,\n",
              " '!': 28,\n",
              " '▁에': 29,\n",
              " '▁’': 30,\n",
              " '▁be': 31,\n",
              " '▁‘': 32,\n",
              " '▁with': 33,\n",
              " '▁not': 34,\n",
              " '▁was': 35,\n",
              " '▁have': 36,\n",
              " '▁we': 37,\n",
              " '▁are': 38,\n",
              " '▁on': 39,\n",
              " '▁가': 40,\n",
              " '▁는': 41,\n",
              " '▁this': 42,\n",
              " '▁를': 43,\n",
              " '▁그': 44,\n",
              " '▁—': 45,\n",
              " ':': 46,\n",
              " '’': 47,\n",
              " '이': 48,\n",
              " 't': 49,\n",
              " '▁as': 50,\n",
              " '을': 51,\n",
              " ')': 52,\n",
              " '의': 53,\n",
              " '▁한': 54,\n",
              " '는': 55,\n",
              " '▁they': 56,\n",
              " '▁The': 57,\n",
              " '▁(': 58,\n",
              " '▁he': 59,\n",
              " '▁his': 60,\n",
              " '▁will': 61,\n",
              " '가': 62,\n",
              " '▁God': 63,\n",
              " '에': 64,\n",
              " '▁from': 65,\n",
              " '...': 66,\n",
              " '▁can': 67,\n",
              " '▁by': 68,\n",
              " '▁your': 69,\n",
              " '▁And': 70,\n",
              " '▁what': 71,\n",
              " '▁do': 72,\n",
              " '▁all': 73,\n",
              " '▁에서': 74,\n",
              " '▁me': 75,\n",
              " '▁으로': 76,\n",
              " '▁their': 77,\n",
              " '▁or': 78,\n",
              " '를': 79,\n",
              " '▁one': 80,\n",
              " '▁who': 81,\n",
              " '▁them': 82,\n",
              " '▁at': 83,\n",
              " '▁my': 84,\n",
              " '고': 85,\n",
              " '▁[': 86,\n",
              " '▁You': 87,\n",
              " '▁과': 88,\n",
              " '▁로': 89,\n",
              " '▁Jehovah': 90,\n",
              " '▁It': 91,\n",
              " '은': 92,\n",
              " '▁about': 93,\n",
              " '▁an': 94,\n",
              " '▁were': 95,\n",
              " '도': 96,\n",
              " '지': 97,\n",
              " '▁입니다': 98,\n",
              " '▁but': 99,\n",
              " '▁so': 100,\n",
              " 're': 101,\n",
              " '▁What': 102,\n",
              " '▁수': 103,\n",
              " '▁had': 104,\n",
              " '▁him': 105,\n",
              " '▁has': 106,\n",
              " 'm': 107,\n",
              " ';': 108,\n",
              " '▁there': 109,\n",
              " '▁out': 110,\n",
              " '▁our': 111,\n",
              " '▁인': 112,\n",
              " '▁We': 113,\n",
              " '▁에게': 114,\n",
              " '▁people': 115,\n",
              " '▁\"': 116,\n",
              " '▁So': 117,\n",
              " '▁when': 118,\n",
              " '▁up': 119,\n",
              " '▁would': 120,\n",
              " '▁know': 121,\n",
              " '▁우리': 122,\n",
              " '-': 123,\n",
              " '▁이다': 124,\n",
              " '▁He': 125,\n",
              " '▁us': 126,\n",
              " '▁But': 127,\n",
              " '▁just': 128,\n",
              " '▁if': 129,\n",
              " 'ing': 130,\n",
              " '로': 131,\n",
              " '▁1': 132,\n",
              " '▁와': 133,\n",
              " '▁time': 134,\n",
              " '▁like': 135,\n",
              " '▁no': 136,\n",
              " '▁more': 137,\n",
              " '▁In': 138,\n",
              " '한': 139,\n",
              " '▁나': 140,\n",
              " '▁도': 141,\n",
              " '▁하는': 142,\n",
              " '▁her': 143,\n",
              " ']': 144,\n",
              " '▁been': 145,\n",
              " '그': 146,\n",
              " '▁그리고': 147,\n",
              " '▁Jesus': 148,\n",
              " '▁Bible': 149,\n",
              " '▁some': 150,\n",
              " '자': 151,\n",
              " '요': 152,\n",
              " '▁which': 153,\n",
              " '▁those': 154,\n",
              " '▁did': 155,\n",
              " '▁said': 156,\n",
              " '나': 157,\n",
              " '▁A': 158,\n",
              " '▁하고': 159,\n",
              " '▁·': 160,\n",
              " '다': 161,\n",
              " '과': 162,\n",
              " '기': 163,\n",
              " '▁히': 164,\n",
              " '▁This': 165,\n",
              " '▁these': 166,\n",
              " '▁get': 167,\n",
              " '에서': 168,\n",
              " '▁here': 169,\n",
              " '▁right': 170,\n",
              " '▁합니다': 171,\n",
              " '▁may': 172,\n",
              " 'd': 173,\n",
              " '▁내': 174,\n",
              " '▁life': 175,\n",
              " '▁년': 176,\n",
              " '▁into': 177,\n",
              " '▁how': 178,\n",
              " '▁then': 179,\n",
              " '▁They': 180,\n",
              " '▁way': 181,\n",
              " '▁2': 182,\n",
              " '▁could': 183,\n",
              " '▁see': 184,\n",
              " '▁other': 185,\n",
              " '▁good': 186,\n",
              " '▁over': 187,\n",
              " '▁than': 188,\n",
              " '▁now': 189,\n",
              " '▁don': 190,\n",
              " '▁go': 191,\n",
              " '라': 192,\n",
              " '▁also': 193,\n",
              " 'ed': 194,\n",
              " '와': 195,\n",
              " '▁she': 196,\n",
              " '▁How': 197,\n",
              " '일': 198,\n",
              " '▁하지만': 199,\n",
              " '▁man': 200,\n",
              " '▁That': 201,\n",
              " '▁many': 202,\n",
              " '▁things': 203,\n",
              " '사람': 204,\n",
              " '▁에는': 205,\n",
              " '으로': 206,\n",
              " '▁world': 207,\n",
              " '▁even': 208,\n",
              " '게': 209,\n",
              " '▁make': 210,\n",
              " '▁because': 211,\n",
              " '서': 212,\n",
              " '▁내가': 213,\n",
              " '▁only': 214,\n",
              " '▁하였다': 215,\n",
              " '▁want': 216,\n",
              " '▁있는': 217,\n",
              " '▁should': 218,\n",
              " '어': 219,\n",
              " '▁going': 220,\n",
              " '▁say': 221,\n",
              " '▁된': 222,\n",
              " '▁such': 223,\n",
              " '▁think': 224,\n",
              " '면': 225,\n",
              " 'll': 226,\n",
              " '▁years': 227,\n",
              " '▁day': 228,\n",
              " '하고': 229,\n",
              " '▁3': 230,\n",
              " '▁If': 231,\n",
              " '▁한다': 232,\n",
              " '▁여호와': 233,\n",
              " '만': 234,\n",
              " '▁need': 235,\n",
              " '▁come': 236,\n",
              " '▁first': 237,\n",
              " '▁two': 238,\n",
              " '▁할': 239,\n",
              " '▁work': 240,\n",
              " '야': 241,\n",
              " '▁하여': 242,\n",
              " '▁very': 243,\n",
              " '▁하게': 244,\n",
              " '▁안': 245,\n",
              " '▁take': 246,\n",
              " '▁그러나': 247,\n",
              " \"▁'\": 248,\n",
              " '인': 249,\n",
              " 've': 250,\n",
              " '할': 251,\n",
              " '▁해': 252,\n",
              " '해': 253,\n",
              " '▁No': 254,\n",
              " '하는': 255,\n",
              " '▁더': 256,\n",
              " '▁back': 257,\n",
              " '▁made': 258,\n",
              " '는것': 259,\n",
              " '것': 260,\n",
              " '▁its': 261,\n",
              " '▁모든': 262,\n",
              " '▁does': 263,\n",
              " '▁있습니다': 264,\n",
              " '입니다': 265,\n",
              " '▁down': 266,\n",
              " '▁any': 267,\n",
              " '▁before': 268,\n",
              " '우리': 269,\n",
              " '수': 270,\n",
              " '▁being': 271,\n",
              " '▁earth': 272,\n",
              " '▁help': 273,\n",
              " '▁하는것': 274,\n",
              " '▁네': 275,\n",
              " '“': 276,\n",
              " '▁much': 277,\n",
              " '▁let': 278,\n",
              " '▁For': 279,\n",
              " '적': 280,\n",
              " '▁where': 281,\n",
              " '▁love': 282,\n",
              " '▁When': 283,\n",
              " '」': 284,\n",
              " '▁난': 285,\n",
              " '▁really': 286,\n",
              " '▁Why': 287,\n",
              " '▁got': 288,\n",
              " '아': 289,\n",
              " '▁well': 290,\n",
              " '▁다른': 291,\n",
              " '▁께서': 292,\n",
              " 'a': 293,\n",
              " '.\"': 294,\n",
              " 'ly': 295,\n",
              " '▁이나': 296,\n",
              " '▁after': 297,\n",
              " '▁다': 298,\n",
              " '▁당신': 299,\n",
              " '시': 300,\n",
              " '▁children': 301,\n",
              " '▁There': 302,\n",
              " '▁제': 303,\n",
              " '여호와': 304,\n",
              " '▁4': 305,\n",
              " '▁times': 306,\n",
              " '▁shall': 307,\n",
              " '▁10': 308,\n",
              " '▁5': 309,\n",
              " '\"': 310,\n",
              " '▁께서는': 311,\n",
              " '▁new': 312,\n",
              " '▁일': 313,\n",
              " '▁something': 314,\n",
              " '▁때': 315,\n",
              " '▁must': 316,\n",
              " 'S': 317,\n",
              " '▁--': 318,\n",
              " '▁우리가': 319,\n",
              " '▁most': 320,\n",
              " '▁through': 321,\n",
              " 'I': 322,\n",
              " '▁little': 323,\n",
              " '▁own': 324,\n",
              " '▁give': 325,\n",
              " '▁하였습니다': 326,\n",
              " '▁As': 327,\n",
              " '▁이제': 328,\n",
              " '▁그래서': 329,\n",
              " '▁것을': 330,\n",
              " '▁Christ': 331,\n",
              " '▁too': 332,\n",
              " '▁어떻게': 333,\n",
              " '▁says': 334,\n",
              " '네': 335,\n",
              " '▁same': 336,\n",
              " '▁우리는': 337,\n",
              " '▁제가': 338,\n",
              " '▁another': 339,\n",
              " '사': 340,\n",
              " '▁men': 341,\n",
              " '▁하나님': 342,\n",
              " '(': 343,\n",
              " '▁예수': 344,\n",
              " '▁She': 345,\n",
              " '▁것': 346,\n",
              " '▁Well': 347,\n",
              " '스': 348,\n",
              " '주': 349,\n",
              " 'y': 350,\n",
              " '내': 351,\n",
              " '▁thing': 352,\n",
              " '▁Witnesses': 353,\n",
              " '▁never': 354,\n",
              " '는데': 355,\n",
              " '▁Allah': 356,\n",
              " '▁find': 357,\n",
              " '▁Oh': 358,\n",
              " '▁입니까': 359,\n",
              " 'er': 360,\n",
              " '▁family': 361,\n",
              " '▁전': 362,\n",
              " '▁page': 363,\n",
              " '▁/': 364,\n",
              " '▁에대한': 365,\n",
              " '▁함': 366,\n",
              " '습니다': 367,\n",
              " '▁Do': 368,\n",
              " '▁look': 369,\n",
              " '▁great': 370,\n",
              " '▁Now': 371,\n",
              " '▁Yeah': 372,\n",
              " '▁name': 373,\n",
              " '▁Christian': 374,\n",
              " '중': 375,\n",
              " '▁한것': 376,\n",
              " '리': 377,\n",
              " '▁아': 378,\n",
              " '▁others': 379,\n",
              " '말': 380,\n",
              " '▁why': 381,\n",
              " '죠': 382,\n",
              " '니': 383,\n",
              " '\\u2060': 384,\n",
              " '▁두': 385,\n",
              " '▁어떤': 386,\n",
              " '▁put': 387,\n",
              " '▁왜': 388,\n",
              " '▁year': 389,\n",
              " '▁every': 390,\n",
              " '▁away': 391,\n",
              " '하나님': 392,\n",
              " '▁might': 393,\n",
              " '▁6': 394,\n",
              " '들이': 395,\n",
              " '▁자': 396,\n",
              " '▁에서는': 397,\n",
              " '▁am': 398,\n",
              " '▁All': 399,\n",
              " '▁use': 400,\n",
              " '▁좀': 401,\n",
              " '▁doing': 402,\n",
              " '▁off': 403,\n",
              " '▁그것': 404,\n",
              " '▁Paul': 405,\n",
              " '▁의증인': 406,\n",
              " '▁still': 407,\n",
              " '▁came': 408,\n",
              " 'the': 409,\n",
              " '▁여기': 410,\n",
              " '▁8': 411,\n",
              " '▁long': 412,\n",
              " '▁place': 413,\n",
              " '▁잘': 414,\n",
              " '▁저는': 415,\n",
              " '▁그들은': 416,\n",
              " '▁15': 417,\n",
              " '▁아니': 418,\n",
              " '▁home': 419,\n",
              " '▁하': 420,\n",
              " '▁명': 421,\n",
              " '▁today': 422,\n",
              " '▁사람': 423,\n",
              " '들': 424,\n",
              " '▁에대해': 425,\n",
              " '▁found': 426,\n",
              " '▁against': 427,\n",
              " '▁gonna': 428,\n",
              " '성': 429,\n",
              " '▁C': 430,\n",
              " '▁Lord': 431,\n",
              " '▁거야': 432,\n",
              " '▁called': 433,\n",
              " '▁Then': 434,\n",
              " '▁저': 435,\n",
              " '▁three': 436,\n",
              " '▁Let': 437,\n",
              " '합니다': 438,\n",
              " '▁each': 439,\n",
              " '▁part': 440,\n",
              " '▁become': 441,\n",
              " '▁또한': 442,\n",
              " '▁인가': 443,\n",
              " '▁x': 444,\n",
              " '▁Yes': 445,\n",
              " '신': 446,\n",
              " '▁true': 447,\n",
              " '▁person': 448,\n",
              " '--': 449,\n",
              " '대': 450,\n",
              " '▁One': 451,\n",
              " '▁used': 452,\n",
              " '▁까지': 453,\n",
              " '▁upon': 454,\n",
              " '▁example': 455,\n",
              " '「': 456,\n",
              " 'e': 457,\n",
              " '하여': 458,\n",
              " '▁말': 459,\n",
              " '▁고': 460,\n",
              " '▁My': 461,\n",
              " '▁Is': 462,\n",
              " '▁라고': 463,\n",
              " '▁그분': 464,\n",
              " '▁death': 465,\n",
              " '▁20': 466,\n",
              " '▁heart': 467,\n",
              " '하게': 468,\n",
              " '▁tell': 469,\n",
              " '▁7': 470,\n",
              " '▁주': 471,\n",
              " '▁again': 472,\n",
              " '하지': 473,\n",
              " '▁To': 474,\n",
              " '▁에있는': 475,\n",
              " '▁keep': 476,\n",
              " '하': 477,\n",
              " '며': 478,\n",
              " '▁게': 479,\n",
              " '▁있어': 480,\n",
              " '▁지금': 481,\n",
              " '▁그들이': 482,\n",
              " '▁이런': 483,\n",
              " '▁은그': 484,\n",
              " '▁course': 485,\n",
              " '▁mean': 486,\n",
              " '비': 487,\n",
              " '▁human': 488,\n",
              " '▁것입니다': 489,\n",
              " '▁하기': 490,\n",
              " '▁feel': 491,\n",
              " 'E': 492,\n",
              " '소': 493,\n",
              " 'in': 494,\n",
              " '▁많은': 495,\n",
              " '치': 496,\n",
              " '▁faith': 497,\n",
              " '물': 498,\n",
              " '에게': 499,\n",
              " '부': 500,\n",
              " '하느님': 501,\n",
              " '▁end': 502,\n",
              " '전': 503,\n",
              " '마': 504,\n",
              " '▁B': 505,\n",
              " '드': 506,\n",
              " '▁성서': 507,\n",
              " '▁월': 508,\n",
              " '▁중': 509,\n",
              " '▁Christians': 510,\n",
              " '▁words': 511,\n",
              " '▁old': 512,\n",
              " '오': 513,\n",
              " '때': 514,\n",
              " '▁오': 515,\n",
              " '▁정말': 516,\n",
              " '▁around': 517,\n",
              " '안': 518,\n",
              " '▁만': 519,\n",
              " '라고': 520,\n",
              " '▁believe': 521,\n",
              " '▁거': 522,\n",
              " '▁under': 523,\n",
              " '▁그래': 524,\n",
              " '▁means': 525,\n",
              " '▁among': 526,\n",
              " '▁시': 527,\n",
              " '▁또': 528,\n",
              " '▁그러므로': 529,\n",
              " '▁spirit': 530,\n",
              " '▁「': 531,\n",
              " '▁같은': 532,\n",
              " '▁12': 533,\n",
              " '▁didn': 534,\n",
              " '장': 535,\n",
              " '▁number': 536,\n",
              " '▁다시': 537,\n",
              " '▁house': 538,\n",
              " '▁예': 539,\n",
              " '▁Kingdom': 540,\n",
              " '▁서': 541,\n",
              " '▁actually': 542,\n",
              " '▁water': 543,\n",
              " '거나': 544,\n",
              " '▁told': 545,\n",
              " '들을': 546,\n",
              " '진': 547,\n",
              " '▁parents': 548,\n",
              " '▁했습니다': 549,\n",
              " '트': 550,\n",
              " '▁better': 551,\n",
              " '▁kind': 552,\n",
              " '▁처럼': 553,\n",
              " '▁Okay': 554,\n",
              " '▁할것': 555,\n",
              " '▁last': 556,\n",
              " '도움': 557,\n",
              " '▁both': 558,\n",
              " '선': 559,\n",
              " '바': 560,\n",
              " '▁done': 561,\n",
              " '▁ones': 562,\n",
              " '▁without': 563,\n",
              " '▁days': 564,\n",
              " '▁between': 565,\n",
              " '▁truth': 566,\n",
              " '▁하느님': 567,\n",
              " '▁left': 568,\n",
              " '▁show': 569,\n",
              " '자신': 570,\n",
              " '▁날': 571,\n",
              " '▁9': 572,\n",
              " '길': 573,\n",
              " '제': 574,\n",
              " '▁b': 575,\n",
              " '▁while': 576,\n",
              " '▁His': 577,\n",
              " '▁것이': 578,\n",
              " '▁young': 579,\n",
              " '여': 580,\n",
              " '도록': 581,\n",
              " '▁thought': 582,\n",
              " '거': 583,\n",
              " '▁그런': 584,\n",
              " '▁못': 585,\n",
              " '▁이라고': 586,\n",
              " '▁always': 587,\n",
              " '▁무슨': 588,\n",
              " '▁ever': 589,\n",
              " '▁세': 590,\n",
              " '기때문': 591,\n",
              " '▁3:': 592,\n",
              " '생각': 593,\n",
              " '▁hand': 594,\n",
              " 'th': 595,\n",
              " '▁fact': 596,\n",
              " '위': 597,\n",
              " '문제': 598,\n",
              " '▁different': 599,\n",
              " '▁live': 600,\n",
              " '▁S': 601,\n",
              " '▁power': 602,\n",
              " '구': 603,\n",
              " '▁임': 604,\n",
              " '▁것은': 605,\n",
              " '▁himself': 606,\n",
              " '▁book': 607,\n",
              " '▁went': 608,\n",
              " '▁set': 609,\n",
              " '▁하나님은': 610,\n",
              " '▁together': 611,\n",
              " '▁2:': 612,\n",
              " '▁New': 613,\n",
              " '성서': 614,\n",
              " 'x': 615,\n",
              " '▁given': 616,\n",
              " '▁그냥': 617,\n",
              " '후': 618,\n",
              " '▁이며': 619,\n",
              " '그분': 620,\n",
              " '▁이러': 621,\n",
              " '▁Can': 622,\n",
              " '▁건': 623,\n",
              " '▁있어요': 624,\n",
              " '세': 625,\n",
              " '▁talk': 626,\n",
              " '분': 627,\n",
              " '▁18': 628,\n",
              " '▁son': 629,\n",
              " '▁되는': 630,\n",
              " '▁themselves': 631,\n",
              " '▁able': 632,\n",
              " '화': 633,\n",
              " '▁의말씀': 634,\n",
              " '던': 635,\n",
              " '▁너희': 636,\n",
              " '▁걸': 637,\n",
              " '▁call': 638,\n",
              " 'al': 639,\n",
              " '▁위해': 640,\n",
              " '▁John': 641,\n",
              " '▁바로': 642,\n",
              " '▁한사람': 643,\n",
              " '▁그럼': 644,\n",
              " '미': 645,\n",
              " '▁그는': 646,\n",
              " '▁word': 647,\n",
              " '▁이것': 648,\n",
              " 'n': 649,\n",
              " '▁Israel': 650,\n",
              " '▁father': 651,\n",
              " '▁그렇게': 652,\n",
              " '▁though': 653,\n",
              " '▁될': 654,\n",
              " '▁few': 655,\n",
              " '▁saying': 656,\n",
              " '▁At': 657,\n",
              " '▁often': 658,\n",
              " '두': 659,\n",
              " '된': 660,\n",
              " '▁congregation': 661,\n",
              " '들은': 662,\n",
              " '▁는무엇': 663,\n",
              " '▁사실': 664,\n",
              " '▁took': 665,\n",
              " '실': 666,\n",
              " '▁mind': 667,\n",
              " '▁Some': 668,\n",
              " '시간': 669,\n",
              " '▁가장': 670,\n",
              " '▁에도': 671,\n",
              " '▁30': 672,\n",
              " '▁Hey': 673,\n",
              " '▁됩니다': 674,\n",
              " '상': 675,\n",
              " '▁system': 676,\n",
              " '▁study': 677,\n",
              " '지만': 678,\n",
              " '▁Who': 679,\n",
              " 'C': 680,\n",
              " '▁important': 681,\n",
              " '▁11': 682,\n",
              " '▁spiritual': 683,\n",
              " '▁하는데': 684,\n",
              " '설명': 685,\n",
              " '예수': 686,\n",
              " '▁하다': 687,\n",
              " '▁wife': 688,\n",
              " '▁면삽': 689,\n",
              " '▁blood': 690,\n",
              " '▁이렇게': 691,\n",
              " '▁는그': 692,\n",
              " '▁read': 693,\n",
              " '▁께': 694,\n",
              " 'The': 695,\n",
              " '▁O': 696,\n",
              " '▁bring': 697,\n",
              " '▁having': 698,\n",
              " '▁whom': 699,\n",
              " 'r': 700,\n",
              " '히': 701,\n",
              " '▁living': 702,\n",
              " '▁함께': 703,\n",
              " '▁point': 704,\n",
              " '▁했다': 705,\n",
              " '▁지': 706,\n",
              " '▁4:': 707,\n",
              " '▁대한': 708,\n",
              " '▁night': 709,\n",
              " '▁However': 710,\n",
              " '▁Your': 711,\n",
              " '▁sure': 712,\n",
              " '▁mother': 713,\n",
              " '▁Because': 714,\n",
              " '▁Not': 715,\n",
              " '▁1:': 716,\n",
              " '속': 717,\n",
              " '▁Are': 718,\n",
              " '▁money': 719,\n",
              " '▁실로': 720,\n",
              " '▁anything': 721,\n",
              " '▁같이': 722,\n",
              " '▁hope': 723,\n",
              " '▁그러한': 724,\n",
              " '보': 725,\n",
              " '▁city': 726,\n",
              " '더': 727,\n",
              " '▁These': 728,\n",
              " '▁land': 729,\n",
              " '▁whole': 730,\n",
              " '▁좋은': 731,\n",
              " '▁next': 732,\n",
              " '▁later': 733,\n",
              " '년': 734,\n",
              " '▁gave': 735,\n",
              " '▁lot': 736,\n",
              " '▁best': 737,\n",
              " '▁body': 738,\n",
              " '▁하는사람': 739,\n",
              " '기도': 740,\n",
              " '산': 741,\n",
              " '질': 742,\n",
              " '타': 743,\n",
              " '▁Picture': 744,\n",
              " '▁high': 745,\n",
              " '▁나는': 746,\n",
              " '▁until': 747,\n",
              " '▁nothing': 748,\n",
              " '▁네가': 749,\n",
              " '▁마': 750,\n",
              " '▁making': 751,\n",
              " '저': 752,\n",
              " '▁child': 753,\n",
              " '▁그가': 754,\n",
              " '▁women': 755,\n",
              " '당신': 756,\n",
              " '▁food': 757,\n",
              " '▁누가': 758,\n",
              " '우': 759,\n",
              " '▁during': 760,\n",
              " '▁since': 761,\n",
              " '▁On': 762,\n",
              " '▁5:': 763,\n",
              " '▁order': 764,\n",
              " '때문': 765,\n",
              " '▁뭐': 766,\n",
              " '▁하며': 767,\n",
              " '▁개': 768,\n",
              " '▁그의': 769,\n",
              " '▁change': 770,\n",
              " '▁became': 771,\n",
              " '▁너': 772,\n",
              " '자기': 773,\n",
              " '▁equal': 774,\n",
              " '▁wrong': 775,\n",
              " '▁woman': 776,\n",
              " '▁당신이': 777,\n",
              " '▁대해': 778,\n",
              " '▁yet': 779,\n",
              " '▁바울': 780,\n",
              " '▁Come': 781,\n",
              " '▁Our': 782,\n",
              " '▁problem': 783,\n",
              " '▁ask': 784,\n",
              " '▁dead': 785,\n",
              " '군': 786,\n",
              " '▁far': 787,\n",
              " '▁있을': 788,\n",
              " '▁그들의': 789,\n",
              " '▁이것은': 790,\n",
              " '▁알고': 791,\n",
              " '원': 792,\n",
              " '▁Just': 793,\n",
              " 'A': 794,\n",
              " '으며': 795,\n",
              " '데': 796,\n",
              " '▁news': 797,\n",
              " '▁real': 798,\n",
              " '▁By': 799,\n",
              " '▁어': 800,\n",
              " '▁이건': 801,\n",
              " '▁알': 802,\n",
              " '▁좋아': 803,\n",
              " '및': 804,\n",
              " '음': 805,\n",
              " '집': 806,\n",
              " '▁four': 807,\n",
              " '▁reason': 808,\n",
              " '▁하면': 809,\n",
              " '▁saw': 810,\n",
              " '▁care': 811,\n",
              " '▁understand': 812,\n",
              " '차': 813,\n",
              " '▁대': 814,\n",
              " '▁하나': 815,\n",
              " '▁someone': 816,\n",
              " '▁이었다': 817,\n",
              " '▁되': 818,\n",
              " '▁너희가': 819,\n",
              " '하기': 820,\n",
              " '등': 821,\n",
              " '▁bad': 822,\n",
              " '▁하는일': 823,\n",
              " '려': 824,\n",
              " '▁After': 825,\n",
              " '▁되었다': 826,\n",
              " '▁로부터': 827,\n",
              " '▁matter': 828,\n",
              " '▁6:': 829,\n",
              " '▁unto': 830,\n",
              " '▁보다': 831,\n",
              " '▁learn': 832,\n",
              " 'es': 833,\n",
              " '▁이스라엘': 834,\n",
              " '▁wanted': 835,\n",
              " '피': 836,\n",
              " '▁모두': 837,\n",
              " '▁할때': 838,\n",
              " '▁coming': 839,\n",
              " '그것': 840,\n",
              " '▁seen': 841,\n",
              " '했습니다': 842,\n",
              " '▁1,': 843,\n",
              " '이야': 844,\n",
              " '▁case': 845,\n",
              " '▁everything': 846,\n",
              " '▁♪': 847,\n",
              " '무엇': 848,\n",
              " '▁16': 849,\n",
              " '▁그러면': 850,\n",
              " '▁fine': 851,\n",
              " '▁그게': 852,\n",
              " 'i': 853,\n",
              " '▁없습니다': 854,\n",
              " '단': 855,\n",
              " '▁한일': 856,\n",
              " '▁hard': 857,\n",
              " '면서': 858,\n",
              " '▁아주': 859,\n",
              " '▁Word': 860,\n",
              " '▁view': 861,\n",
              " '는일': 862,\n",
              " '파': 863,\n",
              " '▁full': 864,\n",
              " '심': 865,\n",
              " '▁apostle': 866,\n",
              " '▁try': 867,\n",
              " '▁peace': 868,\n",
              " '▁turn': 869,\n",
              " '살': 870,\n",
              " '▁되어': 871,\n",
              " '▁school': 872,\n",
              " '개': 873,\n",
              " '약': 874,\n",
              " '었다': 875,\n",
              " '▁side': 876,\n",
              " '▁sent': 877,\n",
              " '▁큰': 878,\n",
              " '▁asked': 879,\n",
              " '▁nor': 880,\n",
              " '▁David': 881,\n",
              " '▁따라서': 882,\n",
              " '습니까': 883,\n",
              " '디': 884,\n",
              " '▁already': 885,\n",
              " '▁Don': 886,\n",
              " '까지': 887,\n",
              " '▁began': 888,\n",
              " '▁brought': 889,\n",
              " '가운데': 890,\n",
              " '▁face': 891,\n",
              " '▁percent': 892,\n",
              " '▁해서': 893,\n",
              " '▁겁니다': 894,\n",
              " '▁knowledge': 895,\n",
              " '▁사람이': 896,\n",
              " 'c': 897,\n",
              " '▁United': 898,\n",
              " '▁With': 899,\n",
              " '▁cannot': 900,\n",
              " '▁번': 901,\n",
              " '▁*': 902,\n",
              " '▁간': 903,\n",
              " '▁물론': 904,\n",
              " '보다': 905,\n",
              " '▁후': 906,\n",
              " '었습니다': 907,\n",
              " '▁okay': 908,\n",
              " '▁사': 909,\n",
              " '모': 910,\n",
              " '많은': 911,\n",
              " '▁에게는': 912,\n",
              " '▁worship': 913,\n",
              " '▁problems': 914,\n",
              " '병': 915,\n",
              " '▁□': 916,\n",
              " '어떤': 917,\n",
              " '▁25': 918,\n",
              " '▁wrote': 919,\n",
              " '▁war': 920,\n",
              " 'T': 921,\n",
              " '▁fear': 922,\n",
              " '▁second': 923,\n",
              " '▁사람들이': 924,\n",
              " '▁close': 925,\n",
              " '▁enough': 926,\n",
              " '▁king': 927,\n",
              " '다른': 928,\n",
              " '이러': 929,\n",
              " '▁없어': 930,\n",
              " '자녀': 931,\n",
              " '▁당신은': 932,\n",
              " '▁century': 933,\n",
              " '줄': 934,\n",
              " '?\"': 935,\n",
              " '▁일이': 936,\n",
              " '▁13': 937,\n",
              " '▁small': 938,\n",
              " '▁yourself': 939,\n",
              " '▁five': 940,\n",
              " '▁eyes': 941,\n",
              " '▁question': 942,\n",
              " '▁만약': 943,\n",
              " '말씀': 944,\n",
              " '▁hear': 945,\n",
              " '▁leave': 946,\n",
              " '건': 947,\n",
              " '▁너무': 948,\n",
              " '▁start': 949,\n",
              " '▁light': 950,\n",
              " '사랑': 951,\n",
              " '▁곧': 952,\n",
              " '점': 953,\n",
              " '프': 954,\n",
              " '려고': 955,\n",
              " '▁없는': 956,\n",
              " '날': 957,\n",
              " '러': 958,\n",
              " '▁줄': 959,\n",
              " '▁넌': 960,\n",
              " '▁자기': 961,\n",
              " '▁17': 962,\n",
              " '▁하면서': 963,\n",
              " '▁오늘날': 964,\n",
              " 'o': 965,\n",
              " '▁라': 966,\n",
              " '▁면': 967,\n",
              " '▁그들': 968,\n",
              " '해서': 969,\n",
              " '▁open': 970,\n",
              " '▁heard': 971,\n",
              " '▁World': 972,\n",
              " '▁Father': 973,\n",
              " '▁즉': 974,\n",
              " '▁이아니라': 975,\n",
              " '거야': 976,\n",
              " '▁14': 977,\n",
              " '모든': 978,\n",
              " '▁s': 979,\n",
              " '발': 980,\n",
              " '▁future': 981,\n",
              " '▁ago': 982,\n",
              " '▁known': 983,\n",
              " '▁makes': 984,\n",
              " '▁brothers': 985,\n",
              " '▁이그': 986,\n",
              " '생활': 987,\n",
              " '▁lives': 988,\n",
              " '▁상': 989,\n",
              " '▁Or': 990,\n",
              " '▁Where': 991,\n",
              " 'or': 992,\n",
              " '▁nations': 993,\n",
              " '▁부터': 994,\n",
              " '▁그건': 995,\n",
              " '▁우린': 996,\n",
              " '▁Dr': 997,\n",
              " '▁생각': 998,\n",
              " '▁미국': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab['</s>'], vocab['<unk>'],  vocab['<pad>']"
      ],
      "metadata": {
        "id": "moWRve-EZr9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGyR4DiyJYo4"
      },
      "outputs": [],
      "source": [
        "# len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHKXaH5Hy0n2"
      },
      "outputs": [],
      "source": [
        "# train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-2. DataLoader"
      ],
      "metadata": {
        "id": "gFK3aKZ1Uy-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "valid_dl = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_dl, valid_dl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvSrOhPKjbYn",
        "outputId": "3ee86f03-bc0c-4c9e-da59-f4dde7f21383"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x79600fb625f0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x79600fb63f10>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YG1D-n9xMEAo"
      },
      "outputs": [],
      "source": [
        "# next(iter(dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6ZJRgq_Ud8iY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "31e6d7ba-125f-460a-d117-d9f56307bb47"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-c68f7aa29a88>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
          ]
        }
      ],
      "source": [
        "# next(iter(dataloader))['input_ids'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "YPNM1Afff5DM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b2e99be-5fd3-4ec1-93b2-f21e61e0bb98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  968,  6769,   225,  ..., 65000, 65000, 65000],\n",
              "         [   54,   633,    53,  ..., 65000, 65000, 65000],\n",
              "         [  515,   954, 12361,  ..., 65000, 65000, 65000],\n",
              "         ...,\n",
              "         [  777,   814,   603,  ..., 65000, 65000, 65000],\n",
              "         [  754,  5553, 19865,  ..., 65000, 65000, 65000],\n",
              "         [ 3895, 17520,  5875,  ..., 65000, 65000, 65000]]),\n",
              " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
              " 'decoder_input_ids': tensor([[  180,     9, 20747,  ..., 65000, 65000, 65000],\n",
              "         [   16,  1931,  1753,  ..., 65000, 65000, 65000],\n",
              "         [  138,     4,     9,  ..., 65000, 65000, 65000],\n",
              "         ...,\n",
              "         [   16,  1640,  5838,  ..., 65000, 65000, 65000],\n",
              "         [   91,    35,    11,  ..., 65000, 65000, 65000],\n",
              "         [ 1080,  3405,  1252,  ..., 65000, 65000, 65000]])}"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "batch = next(iter(train_dl))\n",
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# len(train_dl)"
      ],
      "metadata": {
        "id": "vM0Qrv0NFS9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_ids = batch['input_ids']\n",
        "# input_ids, input_ids.shape"
      ],
      "metadata": {
        "id": "gdoF1--KYZ4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translated_ids = model.generate(input_ids, num_beams=4, max_length=150, early_stopping=True)\n",
        "# print(translated_ids), len(translated_ids)"
      ],
      "metadata": {
        "id": "S19b35WkYw8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(len(translated_ids)):\n",
        "#     translated_text = tokenizer.decode(translated_ids[i], skip_special_tokens=True)\n",
        "#     print(f\"{i+1}번째 번역 결과:\", translated_text)"
      ],
      "metadata": {
        "id": "WrqBSiQ9ZN4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch['decoder_input_ids']"
      ],
      "metadata": {
        "id": "cHeJFqvZYhCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(len(batch['decoder_input_ids'])):\n",
        "#     translated_text = tokenizer.decode(batch['decoder_input_ids'][i], skip_special_tokens=True)\n",
        "#     print(f\"{i+1}번째 정답 결과:\", translated_text)"
      ],
      "metadata": {
        "id": "yw3Ma7caZ5kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBY-dPcmf5BL"
      },
      "outputs": [],
      "source": [
        "# model(input_ids=batch['input_ids'], decoder_input_ids=batch['decoder_input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# config = MarianConfig.from_pretrained(model_name)\n",
        "# config.decoder_layers = 1\n",
        "\n",
        "# new_model = MarianMTModel.from_pretrained(model_name, config=config).to(device)\n",
        "# new_model"
      ],
      "metadata": {
        "id": "0RiWFs4siTty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UvQdZMOA3HE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# input_ids = batch['input_ids'].to(device)\n",
        "# decoder_input_ids = batch['decoder_input_ids'].to(device)\n",
        "\n",
        "# output = new_model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
        "# output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG5w05VJ9IO5"
      },
      "source": [
        "# 5.새로운 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3ofWx1z53OY",
        "outputId": "2941da23-8a64-4a1d-94a9-6f851e679658"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MarianMTModel(\n",
              "  (model): MarianModel(\n",
              "    (shared): Embedding(65001, 512, padding_idx=65000)\n",
              "    (encoder): MarianEncoder(\n",
              "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): MarianDecoder(\n",
              "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# 모델 정의\n",
        "model_name = \"Helsinki-NLP/opus-mt-ko-en\"\n",
        "\n",
        "# 토크나이저 불러오기\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 모델 불러오기\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGvMnuVXQqFV"
      },
      "outputs": [],
      "source": [
        "# class TranslationModel(nn.Module):\n",
        "#     def __init__(self, pretrained_model_name = 'Helsinki-NLP/opus-mt-ko-en', num_decode_layers = 1):\n",
        "#         super(TranslationModel, self).__init__()\n",
        "#         # encoder-decoder 모델 가져오기\n",
        "#         self.marian_model = MarianMTModel.from_pretrained(pretrained_model_name)\n",
        "\n",
        "#         # 추가할 레이어\n",
        "#         self.dropout = nn.Dropout(dropout_prob)\n",
        "#         self.layer_norm = nn.LayerNorm(self.marian_model.config.hidden_size)\n",
        "#         self.lm_head = self.marian_model.lm_head\n",
        "\n",
        "#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#     def forward(self, input_ids, decoder_input_ids):\n",
        "\n",
        "#         # encoder / decoder 입력값\n",
        "#         input_ids = input_ids.to(self.device)                   # encoder input\n",
        "#         decoder_input_ids = decoder_input_ids.to(self.device)   # decoder input\n",
        "\n",
        "#         # 기존 모델\n",
        "#         logits = self.marian_model(input_ids=input_ids, decoder_input_ids=decoder_input_ids).logits\n",
        "#         # logits = outputs.logits\n",
        "\n",
        "#         # 추가한 레이어\n",
        "#         layer_norm_shape = list(logits.size()[1:])\n",
        "#         layer_norm = nn.LayerNorm(layer_norm_shape).to(self.device)\n",
        "\n",
        "#         # Apply dropout and layer normalization\n",
        "#         logits = self.dropout(logits)\n",
        "#         logits = layer_norm(logits)\n",
        "\n",
        "#         return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class TranslationModel(nn.Module):\n",
        "#     def __init__(self, model_name = 'Helsinki-NLP/opus-mt-ko-en', num_decode_layers = 1):\n",
        "#         super(TranslationModel, self).__init__()\n",
        "#         self.config = MarianConfig.from_pretrained(model_name)\n",
        "#         self.config.decoder_layers = num_decode_layers\n",
        "#         self.marian = MarianMTModel.from_pretrained(model_name, config=self.config)\n",
        "\n",
        "#         for param in self.marian.get_encoder().parameters():\n",
        "#                 param.requires_grad = False\n",
        "\n",
        "#     def forward(self, input_ids, decoder_input_ids):\n",
        "#         result = self.marian(input_ids, decoder_input_ids)\n",
        "\n",
        "#         return result[0]\n",
        "\n",
        "\n",
        "#     def generate(self, input_ids):\n",
        "#         result = self.marian(input_ids)\n",
        "\n",
        "#         return result[0]"
      ],
      "metadata": {
        "id": "FAUskeKaeLB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIhQy5BJ96io"
      },
      "outputs": [],
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # 새로운 모델 생성\n",
        "# new_model = TranslationModel().to(device)\n",
        "# new_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 디코더 레이어 6 -> 1 모델"
      ],
      "metadata": {
        "id": "6TMSnvCAYqYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Helsinki-NLP/opus-mt-ko-en\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "config = MarianConfig.from_pretrained(model_name)\n",
        "config.decoder_layers = 1\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "new_model = MarianMTModel.from_pretrained(model_name, config=config).to(device)\n",
        "new_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fpLx3XZi5GX",
        "outputId": "7ca5c89b-7345-4ccb-a541-91744592bbd4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at Helsinki-NLP/opus-mt-ko-en were not used when initializing MarianMTModel: ['model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.fc2.bias', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.2.encoder_attn.k_proj.weight']\n",
            "- This IS expected if you are initializing MarianMTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing MarianMTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MarianMTModel(\n",
              "  (model): MarianModel(\n",
              "    (shared): Embedding(65001, 512, padding_idx=65000)\n",
              "    (encoder): MarianEncoder(\n",
              "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): MarianDecoder(\n",
              "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0): MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC9ADffT7hwJ"
      },
      "source": [
        "# 6.모델 학습 및 검증"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6-1.모델 학습"
      ],
      "metadata": {
        "id": "E7VW_BJZ5lOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6개 디코더 레이어 원래 모델"
      ],
      "metadata": {
        "id": "ycAyNu4cbOJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 정의\n",
        "model_name = \"Helsinki-NLP/opus-mt-ko-en\"\n",
        "\n",
        "# 토크나이저 불러오기\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 모델 불러오기\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRWXy1D3fWQc",
        "outputId": "c3d42a78-58f0-47ff-9686-2e31c45f1db6"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MarianConfig {\n",
              "  \"_name_or_path\": \"Helsinki-NLP/opus-mt-ko-en\",\n",
              "  \"activation_dropout\": 0.0,\n",
              "  \"activation_function\": \"swish\",\n",
              "  \"add_bias_logits\": false,\n",
              "  \"add_final_layer_norm\": false,\n",
              "  \"architectures\": [\n",
              "    \"MarianMTModel\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"bad_words_ids\": [\n",
              "    [\n",
              "      65000\n",
              "    ]\n",
              "  ],\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classif_dropout\": 0.0,\n",
              "  \"classifier_dropout\": 0.0,\n",
              "  \"d_model\": 512,\n",
              "  \"decoder_attention_heads\": 8,\n",
              "  \"decoder_ffn_dim\": 2048,\n",
              "  \"decoder_layerdrop\": 0.0,\n",
              "  \"decoder_layers\": 6,\n",
              "  \"decoder_start_token_id\": 65000,\n",
              "  \"decoder_vocab_size\": 65001,\n",
              "  \"dropout\": 0.1,\n",
              "  \"encoder_attention_heads\": 8,\n",
              "  \"encoder_ffn_dim\": 2048,\n",
              "  \"encoder_layerdrop\": 0.0,\n",
              "  \"encoder_layers\": 6,\n",
              "  \"eos_token_id\": 0,\n",
              "  \"extra_pos_embeddings\": 65001,\n",
              "  \"forced_eos_token_id\": 0,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"LABEL_0\",\n",
              "    \"1\": \"LABEL_1\",\n",
              "    \"2\": \"LABEL_2\"\n",
              "  },\n",
              "  \"init_std\": 0.02,\n",
              "  \"is_encoder_decoder\": true,\n",
              "  \"label2id\": {\n",
              "    \"LABEL_0\": 0,\n",
              "    \"LABEL_1\": 1,\n",
              "    \"LABEL_2\": 2\n",
              "  },\n",
              "  \"max_length\": 512,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"marian\",\n",
              "  \"normalize_before\": false,\n",
              "  \"normalize_embedding\": false,\n",
              "  \"num_beams\": 6,\n",
              "  \"num_hidden_layers\": 6,\n",
              "  \"pad_token_id\": 65000,\n",
              "  \"scale_embedding\": true,\n",
              "  \"share_encoder_decoder_embeddings\": true,\n",
              "  \"static_position_embeddings\": true,\n",
              "  \"transformers_version\": \"4.33.1\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 65001\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = MarianConfig.from_pretrained(model_name)\n",
        "config.decoder_layers = 1\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "new_model = MarianMTModel.from_pretrained(model_name, config=config).to(device)\n",
        "new_model.config"
      ],
      "metadata": {
        "id": "UfUBnMBVIeO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee06e334-6753-4e87-ba90-362af78acda2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at Helsinki-NLP/opus-mt-ko-en were not used when initializing MarianMTModel: ['model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.fc2.bias', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.2.encoder_attn.k_proj.weight']\n",
            "- This IS expected if you are initializing MarianMTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing MarianMTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MarianConfig {\n",
              "  \"_name_or_path\": \"Helsinki-NLP/opus-mt-ko-en\",\n",
              "  \"activation_dropout\": 0.0,\n",
              "  \"activation_function\": \"swish\",\n",
              "  \"add_bias_logits\": false,\n",
              "  \"add_final_layer_norm\": false,\n",
              "  \"architectures\": [\n",
              "    \"MarianMTModel\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"bad_words_ids\": [\n",
              "    [\n",
              "      65000\n",
              "    ]\n",
              "  ],\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classif_dropout\": 0.0,\n",
              "  \"classifier_dropout\": 0.0,\n",
              "  \"d_model\": 512,\n",
              "  \"decoder_attention_heads\": 8,\n",
              "  \"decoder_ffn_dim\": 2048,\n",
              "  \"decoder_layerdrop\": 0.0,\n",
              "  \"decoder_layers\": 1,\n",
              "  \"decoder_start_token_id\": 65000,\n",
              "  \"decoder_vocab_size\": 65001,\n",
              "  \"dropout\": 0.1,\n",
              "  \"encoder_attention_heads\": 8,\n",
              "  \"encoder_ffn_dim\": 2048,\n",
              "  \"encoder_layerdrop\": 0.0,\n",
              "  \"encoder_layers\": 6,\n",
              "  \"eos_token_id\": 0,\n",
              "  \"extra_pos_embeddings\": 65001,\n",
              "  \"forced_eos_token_id\": 0,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"LABEL_0\",\n",
              "    \"1\": \"LABEL_1\",\n",
              "    \"2\": \"LABEL_2\"\n",
              "  },\n",
              "  \"init_std\": 0.02,\n",
              "  \"is_encoder_decoder\": true,\n",
              "  \"label2id\": {\n",
              "    \"LABEL_0\": 0,\n",
              "    \"LABEL_1\": 1,\n",
              "    \"LABEL_2\": 2\n",
              "  },\n",
              "  \"max_length\": 512,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"marian\",\n",
              "  \"normalize_before\": false,\n",
              "  \"normalize_embedding\": false,\n",
              "  \"num_beams\": 6,\n",
              "  \"num_hidden_layers\": 6,\n",
              "  \"pad_token_id\": 65000,\n",
              "  \"scale_embedding\": true,\n",
              "  \"share_encoder_decoder_embeddings\": true,\n",
              "  \"static_position_embeddings\": true,\n",
              "  \"transformers_version\": \"4.33.1\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 65001\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### learning rate 줄여가며 학습시켜보기 - lr_scheduler 사용 (별로 유의미하진 않음)"
      ],
      "metadata": {
        "id": "uN_XAh9cIq--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# 학습할 모델 정의\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# StepLR 스케줄러 설정\n",
        "step_size = 10  # 학습률을 조절할 주기 (에폭 단위)\n",
        "gamma = 0.5     # 학습률을 감소시킬 배율\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "# 학습 루프\n",
        "# encoder 파라미터 동결시키기\n",
        "for param in model.get_encoder().parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# loss 저장 리스트\n",
        "train_loss_list = []\n",
        "valid_loss_list = []\n",
        "\n",
        "num_epochs = 500\n",
        "\n",
        "total_start = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    epoch_start = time.time()\n",
        "    model.train()\n",
        "    train_epoch_loss = 0.0\n",
        "\n",
        "    for train_batch in train_dl:\n",
        "\n",
        "        input_ids = train_batch['input_ids'].to(device)                  # 인코더 input\n",
        "        attention_mask = train_batch['attention_mask'].to(device)\n",
        "        decoder_input_ids = train_batch['decoder_input_ids'].to(device)  # 디코더 input\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        decoder_input_ids=decoder_input_ids)\n",
        "\n",
        "        # loss 계산\n",
        "        train_batch_loss = criterion(outputs.logits.view(-1, outputs.logits.size(-1)), decoder_input_ids.view(-1))\n",
        "        train_epoch_loss += train_batch_loss.item()\n",
        "\n",
        "    train_epoch_loss = train_epoch_loss/len(train_dl)\n",
        "    train_loss_list.append(train_epoch_loss)\n",
        "\n",
        "    train_batch_loss.backward()\n",
        "    # optimizer.step()\n",
        "    # 에폭 끝에서 학습률 조정\n",
        "    scheduler.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0 :\n",
        "      print('Epoch:', '%02d' % (epoch + 1))\n",
        "      print('train loss =', '{:.4f}'.format(train_epoch_loss))\n",
        "      # 현재 학습률 출력\n",
        "      print(f'에폭 [{epoch+1}/{num_epochs}] - 현재 학습률: {scheduler.get_lr()}')\n",
        "\n",
        "total_end = time.time()\n",
        "print(f'총 걸린시간: {round(total_end-total_start, 4)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTwWPNFKILat",
        "outputId": "2988560b-54b2-4ae6-98b3-8d561a329323"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 100\n",
            "train loss = 0.0937\n",
            "에폭 [100/500] - 현재 학습률: [4.8828125e-05]\n",
            "Epoch: 200\n",
            "train loss = 0.0932\n",
            "에폭 [200/500] - 현재 학습률: [4.76837158203125e-08]\n",
            "Epoch: 300\n",
            "train loss = 0.0925\n",
            "에폭 [300/500] - 현재 학습률: [4.656612873077393e-11]\n",
            "Epoch: 400\n",
            "train loss = 0.0926\n",
            "에폭 [400/500] - 현재 학습률: [4.5474735088646414e-14]\n",
            "Epoch: 500\n",
            "train loss = 0.0930\n",
            "에폭 [500/500] - 현재 학습률: [4.4408920985006264e-17]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train_df 45000개, batch_size = 100 , epoch = 50"
      ],
      "metadata": {
        "id": "DgHZmQljYFXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수, 옵티마이저, 디바이스 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# encoder 파라미터 동결시키기\n",
        "for param in model.get_encoder().parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# loss 저장 리스트\n",
        "train_loss_list = []\n",
        "valid_loss_list = []\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "total_start = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    epoch_start = time.time()\n",
        "    model.train()\n",
        "    train_epoch_loss = 0.0\n",
        "\n",
        "    for train_batch in train_dl:\n",
        "\n",
        "        input_ids = train_batch['input_ids'].to(device)                  # 인코더 input\n",
        "        attention_mask = train_batch['attention_mask'].to(device)\n",
        "        decoder_input_ids = train_batch['decoder_input_ids'].to(device)  # 디코더 input\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        decoder_input_ids=decoder_input_ids)\n",
        "\n",
        "        # loss 계산\n",
        "        train_batch_loss = criterion(outputs.logits.view(-1, outputs.logits.size(-1)), decoder_input_ids.view(-1))\n",
        "        train_epoch_loss += train_batch_loss.item()\n",
        "\n",
        "    train_epoch_loss = train_epoch_loss/len(train_dl)\n",
        "    train_loss_list.append(train_epoch_loss)\n",
        "\n",
        "    train_batch_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0 :\n",
        "      print('Epoch:', '%02d' % (epoch + 1))\n",
        "      print('train loss =', '{:.4f}'.format(train_epoch_loss))\n",
        "\n",
        "    #####################################################################################################################\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        valid_epoch_loss = 0.0\n",
        "\n",
        "        for valid_batch in valid_dl:\n",
        "\n",
        "            input_ids = valid_batch['input_ids'].to(device)                  # 인코더 input\n",
        "            attention_mask = valid_batch['attention_mask'].to(device)\n",
        "            decoder_input_ids = valid_batch['decoder_input_ids'].to(device)  # 디코더 input\n",
        "\n",
        "            # 모델에 입력을 전달하여 예측값 얻기\n",
        "            outputs = model(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            decoder_input_ids=decoder_input_ids)\n",
        "\n",
        "            # 손실 계산\n",
        "            valid_batch_loss = criterion(outputs.logits.view(-1, outputs.logits.size(-1)), decoder_input_ids.view(-1))\n",
        "            valid_epoch_loss += valid_batch_loss.item()\n",
        "\n",
        "        valid_epoch_loss = valid_epoch_loss/len(valid_dl)\n",
        "        valid_loss_list.append(valid_epoch_loss)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0 :\n",
        "          print('validation loss =', '{:.4f}'.format(valid_epoch_loss))\n",
        "\n",
        "    # epoch_end = time.time()\n",
        "    # print(f'time per epoch: {round(epoch_end-epoch_start, 4)}초')\n",
        "    # print('==============================')\n",
        "\n",
        "total_end = time.time()\n",
        "print(f'총 걸린시간: {round(total_end-total_start, 4)}')\n",
        "\n",
        "torch.save(model, \"/content/drive/MyDrive/Lee/marianmodel_case1_epoch50_traindata45000\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVMbnE4KfWF1",
        "outputId": "890d95a1-014e-443c-f0f2-d6464173a559"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10\n",
            "train loss = 2.8954\n",
            "validation loss = 2.8142\n",
            "Epoch: 20\n",
            "train loss = 2.3129\n",
            "validation loss = 2.1807\n",
            "Epoch: 30\n",
            "train loss = 1.6868\n",
            "validation loss = 1.4628\n",
            "Epoch: 40\n",
            "train loss = 1.0516\n",
            "validation loss = 0.7847\n",
            "Epoch: 50\n",
            "train loss = 0.5875\n",
            "validation loss = 0.3906\n",
            "총 걸린시간: 4772.7809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss_list, label='Train Loss')\n",
        "plt.plot(valid_loss_list, label='Valid Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E9MCaA79fWAb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "9ead2be0-e80c-43a3-929e-4066a8807263"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAGwCAYAAACHJU4LAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABX9klEQVR4nO3dd3hUZd7G8e+kF9JJhRR6DaFDKDZQwEaxIIsKKyvriqy+q++r7lpw3RVXVxd7F9QVCyhYUREFQgu9F0GSEFIILY2QOuf94yQTIqGFZGaS3J/rmsuZ55yZ+eXAknvP0yyGYRiIiIiIOCEXRxcgIiIiciYKKiIiIuK0FFRERETEaSmoiIiIiNNSUBERERGnpaAiIiIiTktBRURERJyWm6MLuBhWq5XMzEz8/PywWCyOLkdERETOg2EYFBQUEBUVhYvL2e+ZNOqgkpmZSXR0tKPLEBERkTpIT0+ndevWZz2nUQcVPz8/wPxB/f39HVyNiIiInI/8/Hyio6Ntv8fPplEHlaruHn9/fwUVERGRRuZ8hm1oMK2IiIg4LQUVERERcVoKKiIiIuK0GvUYFRERaVoqKiooKytzdBlykdzd3XF1da2Xz1JQERERhzMMg+zsbHJzcx1ditSTwMBAIiIiLnqdMwUVERFxuKqQEhYWho+PjxbxbMQMw6CoqIicnBwAIiMjL+rzFFRERMShKioqbCElJCTE0eVIPfD29gYgJyeHsLCwi+oG0mBaERFxqKoxKT4+Pg6uROpT1Z/nxY45UlARERGnoO6epqW+/jwVVERERMRpKaiIiIiI01JQERERcRJxcXHMmjXL0WU4FQWVWpRXWMnKO8nB40WOLkVERJyQxWI562PGjBl1+tx169YxderUi6rtsssu47777ruoz3Ammp5ci3kbDvLw59u4onMY707u5+hyRETEyWRlZdmef/LJJzz22GPs2bPH1taiRQvbc8MwqKiowM3t3L9yQ0ND67fQJkB3VGoR5ucJQE5BsYMrERFpngzDoKi03O4PwzDOq76IiAjbIyAgAIvFYnu9e/du/Pz8WLRoEX369MHT05MVK1bw66+/Mnr0aMLDw2nRogX9+vXjxx9/rPG5v+36sVgsvP3224wdOxYfHx86dOjAl19+eVHX9rPPPqNbt254enoSFxfHc889V+P4q6++SocOHfDy8iI8PJwbb7zRdmz+/PnEx8fj7e1NSEgIw4cP58SJExdVz7nojkotwvy8AMjJL3FwJSIizdPJsgq6Pva93b93599H4ONRP78aH3roIf7973/Ttm1bgoKCSE9P5+qrr+af//wnnp6evP/++1x33XXs2bOHmJiYM37OE088wTPPPMOzzz7LSy+9xMSJE0lLSyM4OPiCa9qwYQM333wzM2bMYPz48axatYq7776bkJAQJk+ezPr16/nzn//MBx98wKBBgzh27BhJSUmAeRdpwoQJPPPMM4wdO5aCggKSkpLOO9zVlYJKLcL8zTsqRwpLqLAauLpobr+IiFyYv//971x55ZW218HBwSQkJNheP/nkkyxYsIAvv/ySe+6554yfM3nyZCZMmADAU089xYsvvsjatWsZOXLkBdf0/PPPM2zYMB599FEAOnbsyM6dO3n22WeZPHkyBw4cwNfXl2uvvRY/Pz9iY2Pp1asXYAaV8vJyxo0bR2xsLADx8fEXXMOFUlCpRYivBy4WsBpwtLCEMH8vR5ckItKseLu7svPvIxzyvfWlb9++NV4XFhYyY8YMvvnmG9sv/ZMnT3LgwIGzfk6PHj1sz319ffH397fto3Ohdu3axejRo2u0DR48mFmzZlFRUcGVV15JbGwsbdu2ZeTIkYwcOdLW7ZSQkMCwYcOIj49nxIgRXHXVVdx4440EBQXVqZbzpTEqtXBzdSGkRdU4FXX/iIjYm8ViwcfDze6P+lwd19fXt8brBx54gAULFvDUU0+RlJTE5s2biY+Pp7S09Kyf4+7uftq1sVqt9Vbnqfz8/Ni4cSMfffQRkZGRPPbYYyQkJJCbm4urqyuLFy9m0aJFdO3alZdeeolOnTqRkpLSILVUUVA5Aw2oFRGR+rRy5UomT57M2LFjiY+PJyIigtTUVLvW0KVLF1auXHlaXR07drRtHOjm5sbw4cN55pln2Lp1K6mpqfz000+AGZIGDx7ME088waZNm/Dw8GDBggUNWrO6fs4gzM+THWhArYiI1I8OHTrw+eefc91112GxWHj00Ucb7M7I4cOH2bx5c422yMhI7r//fvr168eTTz7J+PHjWb16NS+//DKvvvoqAF9//TX79+/nkksuISgoiG+//Rar1UqnTp1ITk5myZIlXHXVVYSFhZGcnMzhw4fp0qVLg/wMVRRUzsA280ddPyIiUg+ef/557rjjDgYNGkTLli158MEHyc/Pb5Dvmjt3LnPnzq3R9uSTT/LII4/w6aef8thjj/Hkk08SGRnJ3//+dyZPngxAYGAgn3/+OTNmzKC4uJgOHTrw0Ucf0a1bN3bt2sXy5cuZNWsW+fn5xMbG8txzzzFq1KgG+RmqWIyGnlfUgPLz8wkICCAvLw9/f/96/eznftjDSz/t49aBMfxjTMOPahYRaa6Ki4tJSUmhTZs2eHlp8kJTcbY/1wv5/a0xKmdQNdPnkLp+REREHEZB5QyqB9MqqIiIiDiKgsoZVAWVw/ma9SMiIuIoCipnUNX1c7iwpMGXBxYREZHaKaicQWjlgm9lFQbHi8ocXI2IiEjzpKByBh5uLgT5mKsBatE3ERERx1BQOYtwf+2iLCIi4kgKKmcRWjmg9pAG1IqIiDiEgspZaHVaERFpSJdddhn33Xef7XVcXByzZs0663ssFgsLFy5s0LqciYLKWYT5V05RVlAREZFTXHfddYwcObLWY0lJSVgsFrZu3XrBn7tu3TqmTp16UbVNnjyZMWPGXNRnOBMFlbPQDsoiIlKbKVOmsHjxYg4ePHjasdmzZ9O3b1969OhxwZ8bGhqKj49PfZTYZCionIWt60eDaUVE5BTXXnstoaGhzJkzp0Z7YWEh8+bNY8qUKRw9epQJEybQqlUrfHx8iI+P56OPPjrr5/6262fv3r1ccskleHl50bVrVxYvXnzRtS9btoz+/fvj6elJZGQkDz30EOXl5bbj8+fPJz4+Hm9vb0JCQhg+fDgnTpwAYOnSpfTv3x9fX18CAwMZPHgwaWlpF13T2Wj35LMI99cy+iIiDmEYUFZk/+919wGL5Zynubm5cfvttzNnzhz+9re/Yal8z7x586ioqGDChAkUFhbSp08fHnzwQfz9/fnmm2+47bbbaNeuHf379z/nd1itVsaNG0d4eDjJycnk5eXVGM9SFxkZGVx99dVMnjyZ999/n927d3PnnXfi5eXFjBkzyMrKYsKECTzzzDOMHTuWgoICkpKSMAyD8vJyxowZw5133slHH31EaWkpa9eutf3sDUVB5Syq7qgcyi/GMIwG/8MQEZFKZUXwVJT9v/evmeDhe16n3nHHHTz77LMsW7aMyy67DDC7fW644QYCAgIICAjggQcesJ0/ffp0vv/+ez799NPzCio//vgju3fv5vvvvycqyrwWTz31FKNGjbrwn6vSq6++SnR0NC+//DIWi4XOnTuTmZnJgw8+yGOPPUZWVhbl5eWMGzeO2NhYAOLj4wE4duwYeXl5XHvttbRr1w6ALl261LmW86Wun7OoGkxbUm4lv7j8HGeLiEhz0rlzZwYNGsS7774LwL59+0hKSmLKlCkAVFRU8OSTTxIfH09wcDAtWrTg+++/58CBA+f1+bt27SI6OtoWUgASExMvquZdu3aRmJhY4/94Dx48mMLCQg4ePEhCQgLDhg0jPj6em266ibfeeovjx48DEBwczOTJkxkxYgTXXXcdL7zwAllZWRdVz/nQHZWz8HJ3xc/LjYLicg4XFBPg7e7okkREmgd3H/PuhiO+9wJMmTKF6dOn88orrzB79mzatWvHpZdeCsCzzz7LCy+8wKxZs4iPj8fX15f77ruP0tLShqi8Xri6urJ48WJWrVrFDz/8wEsvvcTf/vY3kpOTadOmDbNnz+bPf/4z3333HZ988gmPPPIIixcvZuDAgQ1Wk+6onINt5o8G1IqI2I/FYnbB2PtxgV38N998My4uLsydO5f333+fO+64w3a3YuXKlYwePZpbb72VhIQE2rZtyy+//HLen92lSxfS09Nr3LVYs2bNBdVX22euXr26xma7K1euxM/Pj9atWwPmOi2DBw/miSeeYNOmTXh4eLBgwQLb+b169eLhhx9m1apVdO/enblz515UTeeiOyrnEObnxa+HT2hArYiInKZFixaMHz+ehx9+mPz8fCZPnmw71qFDB+bPn8+qVasICgri+eef59ChQ3Tt2vW8Pnv48OF07NiRSZMm8eyzz5Kfn8/f/va383pvXl4emzdvrtEWEhLC3XffzaxZs5g+fTr33HMPe/bs4fHHH+cvf/kLLi4uJCcns2TJEq666irCwsJITk7m8OHDdOnShZSUFN58802uv/56oqKi2LNnD3v37uX2228/38tVJwoq51A980drqYiIyOmmTJnCO++8w9VXX11jPMkjjzzC/v37GTFiBD4+PkydOpUxY8aQl5d3Xp/r4uLCggULmDJlCv379ycuLo4XX3zxjAvNnWrp0qX06tXrtDrffvttvv32W/73f/+XhIQEgoODmTJlCo888ggA/v7+LF++nFmzZpGfn09sbCzPPfcco0aN4tChQ+zevZv33nuPo0ePEhkZybRp0/jjH/94AVfrwlmMU+//NDL5+fkEBASQl5eHv79/g3zHU9/u4s3l+5kypA2PXnt+KVhERM5fcXExKSkptGnTBi8vL0eXI/XkbH+uF/L7W2NUzqF6dVp1/YiIiNibgso5hNoG06rrR0RExN4UVM6hatE3bUwoIiJifwoq5xCmZfRFREQcRkHlHKrGqBSWlHOiRKvTiog0lEY8t0NqUV9/ngoq59DC0w0fD1dAd1VERBqCu7u56ndRkQM2IZQGU/XnWfXnW1daR+UcLBYLYX6epB4tIie/mDYtz2+zKhEROT+urq4EBgaSk5MDgI+PjzaBbcQMw6CoqIicnBwCAwNxdXW9qM9TUDkPYX5eZlDRHRURkQYREREBYAsr0vgFBgba/lwvhoLKeQjVgFoRkQZlsViIjIwkLCyMsrIyR5cjF8nd3f2i76RUUVA5D9WLvmktFRGRhuTq6lpvv+CkadBg2vNgW0tFOyiLiIjYlYLKeQhX14+IiIhDKKich6o7Koe0jL6IiIhdOTSozJgxA4vFUuPRuXNnR5ZUK61OKyIi4hgOH0zbrVs3fvzxR9trNzeHl3SaqsG0eSfLKC6rwMtdA71ERETsweGpwM3NrV7mWTekAG93PNxcKC23crighOhgH0eXJCIi0iw4fIzK3r17iYqKom3btkycOJEDBw6c8dySkhLy8/NrPOzBYrEQ2kLdPyIiIvbm0KAyYMAA5syZw3fffcdrr71GSkoKQ4cOpaCgoNbzZ86cSUBAgO0RHR1tt1qrZv4c1loqIiIiduPQoDJq1ChuuukmevTowYgRI/j222/Jzc3l008/rfX8hx9+mLy8PNsjPT3dbrVWz/zRHRURERF7cfgYlVMFBgbSsWNH9u3bV+txT09PPD097VyVqXrmj+6oiIiI2IvDx6icqrCwkF9//ZXIyEhHl3Ia2zL6uqMiIiJiNw4NKg888ADLli0jNTWVVatWMXbsWFxdXZkwYYIjy6pVVdePBtOKiIjYj0O7fg4ePMiECRM4evQooaGhDBkyhDVr1hAaGurIsmqlHZRFRETsz6FB5eOPP3bk11+Q8KqNCTVGRURExG6caoyKM6saTHuksJSyCquDqxEREWkeFFTOU7CPB24uFgCOFKr7R0RExB4UVM6Ti4uFli0080dERMSeFFQugHZRFhERsS8FlQtgW0tFA2pFRETsQkHlAoT5V66loq4fERERu1BQuQC6oyIiImJfCioXwLY6re6oiIiI2IWCygWovqOioCIiImIPCioXQDsoi4iI2JeCygWo6vo5UlhKhdVwcDUiIiJNn4LKBWjZwgOLBSqsBsdOlDq6HBERkSZPQeUCuLm6EOKr7h8RERF7UVC5QLYBtZr5IyIi0uAUVC6QBtSKiIjYj4LKBdIdFREREftRULlAtkXftJaKiIhIg1NQuUDh6voRERGxGwWVCxSqOyoiIiJ2o6BygWyDaTVGRUREpMEpqFygqsG0hwtKMAytTisiItKQFFQuUGhlUCmtsJJbVObgakRERJo2BZUL5OnmSqCPO6BxKiIiIg1NQaUObGupaOaPiIhIg1JQqYNw/8qZPxpQKyIi0qAUVOqgapzKId1RERERaVAKKnVgW51Wd1REREQalIJKHZw6RVlEREQajoJKHWgHZREREftQUKkDbUwoIiJiHwoqdRB+yjL6Wp1WRESk4Sio1EHVHZWTZRUUlJQ7uBoREZGmS0GlDrw9XPHzdAM080dERKQhKajUUagG1IqIiDQ4BZU60hRlERGRhqegUkda9E1ERKThKajUUbi6fkRERBqcgsrZWK1nPKS1VERERBqegkpt9i6Gt4fDkifOeErV6rSH8nVHRUREpKEoqNSmrAgOroPtn53xrkrVDsq6oyIiItJwFFRq0+Eq8PCDvHQ4uLbWUyL8za6fzNyTFJdV2LM6ERGRZkNBpTbu3tD5GvP5tvm1nhIX4ktkgBfFZVZW7D1ix+JERESaDwWVM4m/0fzvzoVQcfoy+S4uFkZ0iwDgux3ZdixMRESk+VBQOZO2l4F3MJw4DCnLaj1lVHczqCzeeYiyijPPEBIREZG6UVA5E1d36DbGfL7981pP6RsXTMsWHuSdLGPN/qP2q01ERKSZUFA5m+6V3T+7voLy02f3uLpYuLKreVdl0XZ1/4iIiNQ3BZWziUkE/1ZQkmeurVKLqu6fH3YcosJq2LM6ERGRJk9B5WxcXKDbWPP59tpn/wxsG4K/lxtHCkvYkHbcjsWJiIg0fQoq59L9BvO/e76DksLTDnu4uTC8azgAi7Zn2bMyERGRJk9B5VyiekFwWyg/CXu+rfWUUd0jAfh+ezaGoe4fERGR+qKgci4WS/Wg2jMs/ja0Q0t8PFzJzCtm68E8OxYnIiLStCmonI+qxd9+XQJFx0477OXuyuWdwwDN/hEREalPThNUnn76aSwWC/fdd5+jSzldaCcIjwdrOez6stZTqmb/fLc9S90/IiIi9cQpgsq6det444036NGjh6NLObP4ykG1Z+j+ubxTGB5uLqQeLWLPoQI7FiYiItJ0OTyoFBYWMnHiRN566y2CgoIcXc6ZVc3+SV0B+afP7vH1dOOSDqEALNqm7h8REZH64PCgMm3aNK655hqGDx9+znNLSkrIz8+v8bCbwBiIHgAYsGNBradUd/8oqIiIiNQHhwaVjz/+mI0bNzJz5szzOn/mzJkEBATYHtHR0Q1c4W9Uzf45w+Jvw7uE4+ZiYc+hAvYfPn3NFREREbkwDgsq6enp3HvvvXz44Yd4eXmd13sefvhh8vLybI/09PQGrvI3uo0BiwtkbIBj+087HODjTmK7EAC+26G7KiIiIhfLYUFlw4YN5OTk0Lt3b9zc3HBzc2PZsmW8+OKLuLm5UVFRcdp7PD098ff3r/GwqxZh0OYS8/n2z2o9pWrxN3X/iIiIXDyHBZVhw4axbds2Nm/ebHv07duXiRMnsnnzZlxdXR1V2tnZun8+r/XwVd3CsVhg68E8MnJP2rEwERGRpsdhQcXPz4/u3bvXePj6+hISEkL37t0dVda5dbkOXD0gZycc2nna4ZYtPOkXFwzoroqIiMjFcvisn0bHOxDaX2k+P8Og2lMXfxMREZG6c6qgsnTpUmbNmuXoMs6tavG37Z9BLavQjqwMKuvTjpNTUGzPykRERJoUpwoqjUbHUeDuC8dTzRlAvxEZ4E3P6EAMA37Yccj+9YmIiDQRCip14eEDna82n59hSf2RWvxNRETkoimo1FXV7J+tn8CJI6cdrhqnsnr/UY6fKLVnZSIiIk2GgkpdtR8GoV3g5DH44p7TxqrEhvjSJdKfCqvBj7vU/SMiIlIXCip15eoON7xtTlX+ZRFsmH3aKSO7qftHRETkYiioXIyI7jB8hvn8u7/C4V9qHB4VbwaVpL1HNPtHRESkDhRULtaAP0Hby6H8JHz+ByivHo/SIawFPVoHUFph5YF5W7FaT5/KLCIiImemoHKxXFxgzGvgHQxZW+Dnf9oOWSwWnrspAS93F5b/cpi3V5y+kaGIiIicmYJKffCPhOtfMp+vfAFSltsOdQj347FruwHwzHd72JKe64ACRUREGicFlfrS5VroPQkwYMFdcPK47dCE/tFcHR9BudXgzx9vorCk3HF1ioiINCIKKvVp5EwIaQ/5GfDVfbYpyxaLhZlje9Aq0Ju0o0U8tnC7Y+sUERFpJBRU6pOHL4x7C1zcYOdC2PKR7VCAjzsv3NITFwt8vimDzzcedFydIiIijYSCSn1r1Rsu/6v5/Nv/hWPVA2j7xgVz3/COADy6cDspR044okIREZFGQ0GlIQy+D2IHQ2khfD4VKqrHpEy7vD0D2gRzorSCP3+0idJyq+PqFBERcXIKKg3BxRXGvgGeAXBwHSx/1nbI1cXCrFt6EujjzraMPP79wx4HFioiIuLcFFQaSmA0XPu8+Xz5M+YaK5UiA7x55oYeALy5fD9L9+Q4okIRERGnp6DSkOJvhG7jwLDC138Ba3U3z1XdIrg9MRaAB+Zt0RL7IiIitVBQaWgjZ4KHH2Ssh43v1Tj016u70DnCjyOFpdz/6RYtsS8iIvIbCioNzS8CrnjEfP7jDCg8bDvk5e7KSxN64eXuQtLeI7y7MsUxNYqIiDgpBRV76PcHiOgBxbmw+LEahzqE+/HotV0Bc4n93dn5DihQRETEOSmo2IOrG1z7H8ACW+ZC6soah3/XP4ZhncMorbBy38ebKS6rcEydIiIiTkZBxV5a94U+k83n39wPFWW2QxaLhadv6EGIrwe7swt4TlOWRUREAAUV+xr2GPi0hMO7YM2rNQ6F+nnyr8opy28lpbBq3xFHVCgiIuJUFFTsyScYrnrSfL70achNr3F4eNdwJvSPAeD+eVvIKyr77SeIiIg0Kwoq9pYwAWIGQVkRfPfQaYcfuaYLcSE+ZOUV88gX2mVZRESaNwUVe7NY4JrnzB2Wd38Nv3xf47Cvpxv/Gd8TVxcLX23J5IvNGQ4qVERExPEUVBwhvCsMvNt8/u0DUFpU43CvmCCmX9EegEcWbicj96S9KxQREXEKCiqOcumD4N8acg9A0nOnHb7n8vb0jA6koLicv3yymQqtWisiIs2QgoqjeLaAUf8yn698AQ7/UuOwm6sLs8b3xMfDleSUY7ydtN8BRYqIiDiWgoojdb4GOowAaxl8ez8YNe+axLX0ta1a++8f9rAzU6vWiohI86Kg4kgWC1z9DLh5QcpyWPbMaafc0i+a4V3CKaswuO+TTVq1VkREmhUFFUcLioOr/mE+X/oULP93jcPmqrXxtGzhwS+HCvnDe+s5Wlhi/zpFREQcQEHFGfS/E4Y9bj7/6UlYMavG4ZYtPJk1vhfe7q6s2HeEa19awcYDx+1fp4iIiJ0pqDiLoX+BKx4xn//4OKx6qcbhIR1asnDaYNq29CUrr5jxb6zm/dWpGIZmA4mISNOloOJMLvlfuOxh8/kPj8DqV2oc7hThxxf3DGZU9wjKKgwe+2IH932ymaLScgcUKyIi0vAUVJzNZQ/BJf9nPv/+r5D8Ro3Dfl7uvDqxN49c0wVXFwtfbM5k9Msr2ZdT6IBiRUREGpaCijO6/K8w9H7z+aL/g7Vv1ThssVj4w9C2fHTnQML8PNmbU8jol1fwzdYsBxQrIiLScBRUnJHFAlc8CoPvNV9/+wCsf/e00/q3CebrPw9hYNtgTpRWMG3uRp78eidlFVY7FywiItIwFFSclcUCw5+AxHvM11//D2x477TTwvy8+O+UAdx1aTsA3lmRwpXPL+PD5DStuSIiIo2exWjE00by8/MJCAggLy8Pf39/R5fTMAzDHKuy5lXAYs4OGvoAePicduoPO7L5v8+2kltUBkDLFh5MHhTHrQNjCfTxsHPhIiIitbuQ398KKo2BYcB3D0Hy6+brgGgY8RR0uc6883KKEyXlfLIunXdWpNh2XfbxcGV8v2imDGlD66DTA46IiIg9NXhQSU9Px2Kx0Lp1awDWrl3L3Llz6dq1K1OnTq1b1XXQbIIKmGFl11fm3ZW8dLOt3RUw6llo2f6008sqrHy7LYvXl+1nV5a5R5Cri4Vre0Qy9ZK2dIsKsGf1IiIiNg0eVIYOHcrUqVO57bbbyM7OplOnTnTr1o29e/cyffp0HnvssToXfyGaVVCpUloESc/BqhehohRc3GHQdLjkAfDwPe10wzBI2nuEN5fvZ8W+I7b2Ed3CmTmuB8G+6hISERH7upDf33UaTLt9+3b69+8PwKeffkr37t1ZtWoVH374IXPmzKnLR8r58vCBYY/C3Wug/ZXmzssrnoeX+8OOhaftwGyxWLikYyj//cMAvp4+hOsSonCxwPc7DnH1C0msTTnmmJ9DRETkPNQpqJSVleHp6QnAjz/+yPXXXw9A586dycrSWh52EdIOJs6DW+ZCYAzkH4R5k+CDMXD011rf0r1VAC9N6MXX04fSNtSX7PxibnlzNS//tJcKa6MdqiQiIk1YnYJKt27deP3110lKSmLx4sWMHDkSgMzMTEJCQuq1QDkLiwU6XwPT1sKlD4KrJ+xfCq8PgY0fnHZ3pUrXKH++umcI43q1wmrAv3/4hUnvriWnoNi+9YuIiJxDnYLKv/71L9544w0uu+wyJkyYQEJCAgBffvmlrUtI7Mjd21zNdtoaaHMJlBXBl/fA/N/Dydxa3+Lr6cbz43vy75sSbLsyX/3CClbsPVLr+SIiIo5Q5+nJFRUV5OfnExQUZGtLTU3Fx8eHsLCweivwbJrlYNpzsVbAyhfg53+CtRwCYuCGtyFmwBnfsi+ngGkfbmLPoQIsFrjn8vbcO6wDbq5aD1BEROpfgw+mPXnyJCUlJbaQkpaWxqxZs9izZ4/dQoqcgYuruSjcHT9AUBzkHYDZo2DZM2aIqUX7MHNX5gn9ozEMeOmnffzurWSy8k7at3YREZHfqFNQGT16NO+//z4Aubm5DBgwgOeee44xY8bw2muv1WuBUket+8Afk6DHeDAqzDss710HeQdrPd3L3ZWZ43rwwi098fVwZW3qMa5+IYllvxy2c+EiIiLV6hRUNm7cyNChQwGYP38+4eHhpKWl8f777/Piiy/Wa4FyEbz8YdybMPYN8GgBaSvhtcHmwnFnMLpnK77+81C6RflzvKiMybPX8sKPe7FqVpCIiDhAnYJKUVERfn5+APzwww+MGzcOFxcXBg4cSFpaWr0WKPUg4Rb443KI6g3FufDJrfDln6E4v9bT27T05bM/DbJ1Bf3nx1/4/Zx1HD9Rat+6RUSk2atTUGnfvj0LFy4kPT2d77//nquuugqAnJycCxrU+tprr9GjRw/8/f3x9/cnMTGRRYsW1aUkOZeQdnDH9zD4XvP1xvfg1UTY+2Otp1d1Bf37pgQ83VxY9sthrn1pBVvSc+1Xs4iINHt1CiqPPfYYDzzwAHFxcfTv35/ExETAvLvSq1ev8/6c1q1b8/TTT7NhwwbWr1/PFVdcwejRo9mxY0ddypJzcfOAK/8Ok742B9rmH4QPb4CFd8PJ47W+5cY+rVk4bTBxIT5k5J7kptdX88GaNBrxXpYiItKI1Hl6cnZ2NllZWSQkJODiYuadtWvX4u/vT+fOnetcUHBwMM8++yxTpkw557mannwRSk/AT/+ANa8BBrQIh2v/Yy4gV4v84jIe+HQLP+w8BMCYnlE8NS4eHw83OxYtIiJNQYNvSniqgwfNWSRVOynXVUVFBfPmzWPSpEls2rSJrl27nnZOSUkJJSUlttf5+flER0crqFyMA8nwxTQ4utd83f0Gc0dm39NXGDYMg7eS9vOv7/ZQYTXoGN6C127tQ7vQFnYuWkREGrMGX0fFarXy97//nYCAAGJjY4mNjSUwMJAnn3wSq9V6QZ+1bds2WrRogaenJ3fddRcLFiyoNaQAzJw5k4CAANsjOjq6LuXLqWIGwF0rYMj/gMUFtn8Gr/SH7Z/XusHh1EvaMfcPAwj18+SXQ4Vc/9IK3lq+n9LyC/tzFxEROR91uqPy8MMP88477/DEE08wePBgAFasWMGMGTO48847+ec//3nen1VaWsqBAwfIy8tj/vz5vP322yxbtkx3VBwhY6N5dyVnp/m60zVw5RPQssNpp+YUFDN97iaSK3dfjgvx4W/XdGV4lzAsFos9qxYRkUamwbt+oqKieP311227Jlf54osvuPvuu8nIyLjQj7QZPnw47dq144033jjnuRqj0gDKSyHp35D0nLkEv8UFev4OLn0IAmvewaqwGny28SDPfr+HwwVmgBzcPoRHr+1K5wj9eYiISO0avOvn2LFjtQ6Y7dy5M8eOHavLR9pYrdYad03Eztw8zA0O/5gEna4Gwwqb/gsv9YZFD0Fhju1UVxcLN/eN5ucHLuPuy9rh4ebCyn1HufqFJP62YBtHC/XnKCIiF6dOQSUhIYGXX375tPaXX36ZHj16nPfnPPzwwyxfvpzU1FS2bdvGww8/zNKlS5k4cWJdypL6FN4VJnwEUxZD3FCoKIXk1+CFnrDkyRq7MrfwdOP/RnZmyV8u5er4CKwGfJh8gMueXarxKyIiclHq1PWzbNkyrrnmGmJiYmxrqKxevZr09HS+/fZb2/L65zJlyhSWLFlCVlYWAQEB9OjRgwcffJArr7zyvN6vrh87MQzYvxSW/B0yN5ptXgEw+D4Y8Efw8K1xevL+o/z9653syDRXvo0L8WH6FR24LiEKDzftyCwi0tzZZXpyZmYmr7zyCrt37wagS5cuTJ06lX/84x+8+eabdfnIC6agYmeGAbu/MddfObzLbPMNg8S7oe8Uc2+hSrWNX4nw9+L3g+OYMCAGfy93R/wEIiLiBOy6jsqptmzZQu/evamoqKivjzwrBRUHsVbAtvmw9Ck4nmq2eQZAvykw8E/QIsx2amFJOe+tSmXOqlRbYGnh6cYt/aL5/ZA2tAr0dsAPICIijqSgIvZRUQbb5sGKWXBkj9nm6gm9boVB0yG4je3UkvIKvtycyVtJ+/nlUKF5qouFa3tEcufQtnRvFeCAH0BERBxBQUXsy2qFXxZB0vOQsd5ss7hAt3Ew5D6IiLedahgGy345zFtJ+1m576itfVC7EKYMacNlncJwddE6LCIiTZmCijiGYUDaSljxH9h3yq7M7a80d22OGwKnLAa3PSOPt5P289XWLCqs5l/D6GBvbhsYy819own08bD3TyAiInbQYEFl3LhxZz2em5vLsmXLFFQEsrbCylmwY4G5FgtAZAIM+jN0HQ2u1YNpM3JP8t6qVD5Zl07eyTIAPN1cGN0zitsT49QtJCLSxDRYUPn9739/XufNnj37fD/yoiioNALH9sOql2HzXCg/abb5tzYH3fa+vcZMoZOlFXy5JYP3VqWxMyvf1t4nNojbE2MZ1T1S05tFRJoAh3X92JuCSiNy4iisfxfWvgEnDpttnv7QZxIMuAsCqnffNgyDjQeO896qNL7dlkV5ZbdQyxae/K5/NBMHxhLu7+WIn0JEROqBgoo4r7Ji2PoJrH4Zjvxitrm4QbexkHgPRPWscXpOfjEfrU1n7to0DuWb05vdXCyM7B7B5EFx9IkN0iaIIiKNjIKKOD+rFfYthlUvQWpSdXvcUEicBh1GgEt1N09ZhZXvd2Tz/qo01qZW7yfVLcqfyYPiuC4hCi93V3v+BCIiUkcKKtK4ZG4yx7HsWABG5UDskPYw8G5ImAAePjVO35GZx3urUvlicyYllfsIBft6cEu/aG4dGEuUFpETEXFqCirSOOUdhOQ3YMN7UJJntnkHmcvz958KfuE1Tj9+opSP16Xz3zVpZOSaA3VdXSxc1TWc2xPjGNg2WN1CIiJOSEFFGreSAtj0Iax5FXLTzDZXD4i/ybzLEtG9xunlFVZ+3JXDe6tSWb2/ehG5juEtuD0xjrG9WuHr6WbPn0BERM5CQUWaBmsF7P4aVr8C6cnV7W0ugYHToMNVNcaxAOzJLuD91al8vjGDk2VmN5Kfpxs39m3NbQNjaRvawp4/gYiI1EJBRZqe9HWw5hXY+WX1OJbgtjDgT9Dzd+BZM4DknSzjsw0H+WBNGilHTtjaL+kYyqTEWC3VLyLiQAoq0nTlpsPaN2Hje1BcOY7FMwD63G6OYwmMqXG61WqQtO8I769K5ac9OVT9bY8O9mbigFhu6tOakBaedv4hRESaNwUVafpKCmHLR5D8OhzdZ7ZZXKDLdeY4lugBNfYVAjhwtIj/JqfVWKrfw9WFq+MjuC0xlt4xWpNFRMQeFFSk+ahaj2XNq7B/aXV7RA/zDkv8jeBec7ryydIKvtqayX/XpLH1YJ6tvXOEH7cOjGVMr1a00OBbEZEGo6AizdOhHbDmNdg2D8qLzTbvIOh1qznFObjNaW/ZejCX/65Jq7EmSwtPN8b0iuLWgbF0jtDfKxGR+qagIs1b0THY9F9Y93b19GYs0HEE9L8T2l5x2myhvKIy5m88yIdr0th/yuDbXjGBjO8bzbUJUbrLIiJSTxRURMCc3rx3sTn49tcl1e3B7aDfH6DnBPOOyykMw2DVr0f575o0fth5iIrKDRG93V25pkck4/tF01f7C4mIXBQFFZHfOrIP1r9j3mkpyTfb3Lyg6xjoMxliBp42+DanoJjPN2bw6fp09h+uvsvStqUvN/WN5oY+rQjz0y7OIiIXSkFF5ExKCs3dm9fPhkPbqttbdjIDS8It4BNc4y2GYbAh7TifrEvnm21ZFJWa67i4uli4vFMYN/ZpxeWdw/B006aIIiLnQ0FF5FwMAzI3woY5sO0zKKu8Y+LqCV1Hm6EldtBpd1kKS8r5Zmsmn6xLZ+OBXFu7v5cbV8dHMrpnKwa0CcZFi8mJiJyRgorIhSjON2cKbZgN2afcZQnpAL1vN3dwbhF62tv25RQwb/1BvticSXZ+sa09KsCL63pGMaZnK7pE6u+liMhvKaiI1IVhQOamyrss86vvsri4QceRZmhpNwxca87+sVoNklOOsXBTBt9uz6KguNx2rFO4H2N6teL6nlG0Cqy5nouISHOloCJysUoKYPtnsPEDyFhf3e4Xae4t1OtWc6+h3yguq+Dn3Tks3JzBz7sPU1phtR3r3yaYsb1acXX3SAJ83O3xU4iIOCUFFZH6dGinOVto68dQdLS6PW4o9LoNul5/2uq3YK7N8u32LBZuyiA55Zit3cPVhcs6hTKmVyuu6ByGl7sG4YpI86KgItIQykthz7ew8X349Seg8n86XgHQYzz0ngQR3Wt9a0buSb7cnMkXmzPYnV1ga/fzcmNU9wjG9GzFgLYh2tFZRJoFBRWRhpabDpvnmnda8g5Ut7fqYwaW7jeAZ4ta37o7O5+Fm8zQkpVXPQg3wt+L0b2iGNerNZ0i/Br6JxARcRgFFRF7sVph/8+w8T3Y/Q1YKwfSerQww0qfSRDV+7RpzuZbDdamHuOLzRl8szWL/FMG4XaN9Gdcb3MQrhaVE5GmRkFFxBEKc8y7LBvfh2O/VreHx0PfydDjljPeZSkpNwfhfr4xg5/35FBWYf7P0sUCQzuEMq53K67qGoG3h8aziEjjp6Ai4kiGAakrzLssO7+EihKz3TMAek009xkKaXfGtx8/UcrX27L4fONBNp2yqJyvhyuj4iO5qU9r+rcJ1n5DItJoKaiIOIuiY7DlY1j3FhzbX93e/koY8EdzXZbf7OR8qpQjJ1iwKYMFmw6Sfuykrb1NS19u6tuaG3u3JsxfXUMi0rgoqIg4G6vVnCm09g3Y+0N1e3A76H+nuTaLV8AZ316139D8DQf5aksmJ2rsNxTKzX2jubxzGO6uZw49IiLOQkFFxJkd/RXWvV1zJ2ePFuZS/YnTILjNWd9+oqScb7Zl8em6dNanHbe1t2zhyQ19WnFz32jahdY+FkZExBkoqIg0BiWF5iJya9+Cw7vNNosLdBsLg++DyB7n/Ih9OYXMW5/OZxsPcqSw1NY+sG0wkxLjuLJrOG66yyIiTkZBRaQxMQxIWQarXoJ9P1a3t7vCDCxtLql1evOpyiqs/LQ7h0/XpfPznhyslf+rjvD34ncDYrilf7SmOYuI01BQEWmssrbCyhdgx+dgVO4TFNUbhtwHna8Fl3NPT87IPcnc5DQ+XpvO0RPmXRZ3Vwsju0dye2IsfWODNGNIRBxKQUWksTueCqtehk0fQHnl6rUh7WHQn82xLG4e5/yIkvIKFm3L5v3VqWw8ZZpzl0h/bk+MZXTPKHw83M78ASIiDURBRaSpKDxszhRa+xYU55ptAdEw9H7oOfG8AgvA9ow8PlidxhdbMiguM+/U+Hu5cUv/GG4bGEt0sE8D/QAiIqdTUBFpakoKYMN75jiWwmyzLSAGLrkfEn533oElt6iUeesP8sGaNA4cKwLM1W+v7BrO5EFtGNhWC8mJSMNTUBFpqspOwoY5sOI/UHjIbAuMgaEPmGuxuLqf18dUWA2W7slhzqpUkvYesbV3jvBj8qA4RvdspeX6RaTBKKiINHVlJ2H9bDOwnMgx2wJj4JL/NcewnGdgAdh7qID3Vqfy2YYMTpaZC8kF+rhzS78YbkuMpVWgd0P8BCLSjCmoiDQXpUWwYTasmHVKYImFSx+EHuPB9fwHy+YVlTFvQzpzVqVy8Li5XL+ri4WruoYzeVCc9hcSkXqjoCLS3JQWwfp3YeUsOHHYbGvZES7/K3QZfdb9hH6rwmqwZNchZq9MZfX+o7b2LpH+/H5QHNf3jMLLXd1CIlJ3CioizVVpkbkB4or/wMnK5fUjesAVj0KHK8+5cNxv7ckuYM6qVBZsOmibLRTk484t/WO4daC6hUSkbhRURJq74jxY/SqsfgVKC8y26IEw7FGIG3LBH5dbVMqn69N5b1UaGbnqFhKRi6OgIiKmE0fN7qC1b1YvHNfuCrjiEWjV54I/rsJq8OOuQ8xRt5CIXAQFFRGpKT8Lkv5tTm22lpttna81x7CEd6vTR+7Ozue9Vaks2JRRo1toQmW3UJS6hUTkDBRURKR2x1Nh6b/MXZsNK2CB7jfAZQ9Dy/Z1+sjcolI+WZfO+6trdguN7BbB5MFx2ltIRE6joCIiZ3d4D/z8FOxcaL62uJgr3F76fxAUW6ePLK+w8uOuHOasSmHN/mO29m5R/kweFMd1CeoWEhGTgoqInJ+srWZg+WWR+drFHfpMMle69Y+s88fuyqruFiopr+4WGt8vhokDYrS3kEgz12iCysyZM/n888/ZvXs33t7eDBo0iH/961906tTpvN6voCJSTw6uh5/+Aft/Nl+7eUG/P8Dg+6BFaJ0/9viJUj5ad4AP1xywdQtZLDCsczi3J8YypH1LXFzULSTS3DSaoDJy5EhuueUW+vXrR3l5OX/961/Zvn07O3fuxNfX95zvV1ARqWepK2DJk5C+xnzt7gP974RBfwbflnX+2PIKK0t25/DB6jRW7KveW6htS19uHRjLjX1b4+91/sv+i0jj1miCym8dPnyYsLAwli1bxiWXXHLO8xVURBqAYcCvS8w7LJmbzLZ6CiwA+3IK+e+aNOZvOEhhiTkDycfDlTG9WjEpMY5OEX4X+xOIiJNrtEFl3759dOjQgW3bttG9e/fTjpeUlFBSUmJ7nZ+fT3R0tIKKSEMwDNj7AyydWTOw9PuDGVguoksIoLCknAUbD/L+6jT25hTa2hPbhjBpUBzDu4Th5nr+S/+LSOPRKIOK1Wrl+uuvJzc3lxUrVtR6zowZM3jiiSdOa1dQEWlADRxYDMNgzf5jvLcqlR92ZmOt/BepVaA3tw6M5ZZ+0QT5elzkDyEizqRRBpU//elPLFq0iBUrVtC6detaz9EdFREHMgzYu7gysGw029x9oO8dMGg6+EVc9Fdk5J7kv2vS+HjtAY4XlQHg6ebCmJ6tmDQojq5R+t+5SFPQ6ILKPffcwxdffMHy5ctp06bNeb9PY1REHKC2wOLqCb1uhcH31nkdllMVl1Xw5ZZM3luVyo7MfFt7/7hgJg2KY0S3cHULiTRijSaoGIbB9OnTWbBgAUuXLqVDhw4X9H4FFREHMgzY9yMsfxbSk802iyv0uBmG/AVCO9bDVxhsSDvOnFWpfLc9m/LKfqGoAC9uTYxlQr8YdQuJNEKNJqjcfffdzJ07ly+++KLG2ikBAQF4e597nxAFFREnYBiQthKW/7t6HRYs0OU6GHo/RPWsl6/Jzivmw+Q05iYf4OiJUsDsFhrbqxWTB8fROUL/Bog0Fo0mqJxp/4/Zs2czefLkc75fQUXEyWRsgKTnYffX1W3th5sr3cYm1stXFJdV8PXWLGavTKnRLZTYNoTJg+MY3iUcVy0iJ+LUGk1QuVgKKiJOKmcXrPgPbJsPRoXZFj3AXOm240hwufjxJYZhsD7tOHNWpvLdjmwqKruFWgd5c3tiLOP7xhDgo0XkRJyRgoqIOIdjKbByFmyeCxVmdw2hnc1pzfE3gVv9jC+pmi300doD5FbOFvJyN7uFbk+Mo0uk/n0QcSYKKiLiXAqyIfl1WPcOlFR21/hFQeLd0GcyeNbParTFZRUs3JTBnFWp7M4usLVXzRa6qls47potJOJwCioi4pyK82HDbFj9KhRmm21eAebicQPughZh9fI1hmGwLvU47602ZwtVdQuF+3sycUAsE/rHEOrnWS/fJSIXTkFFRJxbeQls/QRWvghH95ptrp7QcwIkToeW7evtq7LzipmbnMbctQc4Umh2P7m7WrgmPpLJg9vQMzqw3r5LRM6PgoqINA5WK+z51hzHcnBdZaMFOl9jDryN7ldvX1VSXsF327OZsyqVTQdybe29YgL5/eA2jOoeoW4hETtRUBGRxsUw4MBq8w7LL4uq22MSzYG39TRTqMrWg7nMWZXK11uyKK2wAhDh78VtiWa3ULAWkRNpUAoqItJ45eyG1S/Blk/Aas7goWVHcz+hHuPBrf7GluQUFDM3+QD/XXOAI4XmPmJVewtNHqzZQiINRUFFRBq//CxzptD62VCSZ7a1CDcH3fabYg7CrScl5RV8szWL2StT2ZaRZ2sf2DaYKUPackXnMC0iJ1KPFFREpOkozoeN78Ga1yA/w2zz8IO+v4eBd4N/ZL19VdXeQrN/s4hcbIgPkwfFcVPfaFp4utXb94k0VwoqItL0lJfC9s9g5QtweJfZ5uoBCbfAoHvrdaYQQGbuSd5fbS4il3fS7ILy83RjfL9oJg2KIzrYp16/T6Q5UVARkabLaoW9P5gzhQ6srmy0QJdrYfD/QOs+9fp1RaXlfLYxg9krU9h/+AQALhYY0S2CO4a0oW9s0Bn3LROR2imoiEjzcCDZDCx7vq1uixsKQ+6DdsOgHgOE1WqwbO9h3l2RQtLeI7b2Hq0DmDKkDVfHR2p6s8h5UlARkeYlZzesetFcRM5abrZF9YZL/hc6jarXwAKwJ7uA2StT+HxTBqXl5vTmVoHe/H5wHLf0j9E4FpFzUFARkeYpLwNWv2Iu019WZLaFx8MlD0CX6+t1LRaAo4UlfJh8gPdXp9pWvfXzcuN3/WOYPDiOyADvev0+kaZCQUVEmrcTR2D1y7D2LSgtNNtCO8PQB6D7OHBxrdevq9oM8a2k/fxaOY7FzcXC9QlR/GFoW7pG6d8nkVMpqIiIABQdM9diWfN69Voswe1g6P3Q42Zwda/Xr7NaDX7ek8NbSftZs/+YrX1I+5b8YWgbLu0YqoG3IiioiIjUVJwHa980d20+WRkgAmPMMSwJE+o9sIC5TP9bSSl8uy3Lth5Lx/AW/GFIW67vGYWXe/3e1RFpTBRURERqU1II69+BVS/BicNmW1CcGVh63AKu9T8INv1YEbNXpvLJugOcKK0AoGULD25PjOPWgbHaV0iaJQUVEZGzKS2C9e+aU5ttgaUNXPp/EH9zgwSWvJNlfLLuALNXppKVVwyY+wrd0Kc1U4a0oV1oi3r/ThFnpaAiInI+Sk/AunfM1W6LKtdGCW5XGVhuqvdBtwBlFVa+3ZbF20kpNfYVGtY5jD8MbcvAtsEaxyJNnoKKiMiFKCmEdW+bgaVqDEtIe7j0Qeh+Q4MEFsMwSE45xttJKSzZfYiqf4m7t/LnzqFttYCcNGkKKiIidVFSaA66XfUinDxutoV2hssebpB1WKrsP1zIOytSmL/hICVaQE6aAQUVEZGLUVIAyW+Yg26Lc8228Hi44m/QcWS9r3Rb5WhhCf9dYy4gd/RE5QJynm78boAWkJOmRUFFRKQ+FOeZU5pXvwKlBWZbqz5w+V/rfS+hGl9bVsGCygXk9p+ygNx1CVH8YWgbukUFNMj3itiLgoqISH0qOmZ2ByW/Ub00f0wiXP43aDO0wb62agG5N5fvJzml5gJyUy9py9AOLTXwVholBRURkYZQmAMrZpkDbytKzLY2l8AVj0J0/wb96q0Hc3lz+X4Wbc+2LSDXOcKPO4e25bqEKDzcNPBWGg8FFRGRhpSfCUnPwYb3wFpmtnUYYXYJRfVs0K+uWkDu43UHKKpcQC7c35PfD27DhP4xBHjX/yq7IvVNQUVExB6Op8HyZ2DzR2CYoYEu18Flf4Xwrg361XlFZXy4No05K1PJKTDv7vh6uHJL/xh+PziO1kE+Dfr9IhdDQUVExJ6O/gpLn4Zt8wADsED8jea05pB2DfrVJeUVfLk5k7eS9vPLIXOnaFcXC6O6R/CHoW3pGR3YoN8vUhcKKiIijpCzC35+CnZ9ab62uELPCXDJ/0FQbIN+tWEYLPvlMG8u38+qX4/a2vvGBjFlSBuu6haBq4sG3opzUFAREXGkrC1mYPnlO/O1izv0vh2G/gUCWjf41+/IzOOdFSl8tSWTsgrzn/joYG8mD2rDzX1b4+elcSziWAoqIiLOIH0d/PwP2L/UfO3qYQaWIX+BgFYN/vU5+cW8vzqN/yankVtkDvr183Tjlv7RTBqkcSziOAoqIiLOJHWFOYYlNcl87eoBvSfBkP+xS2A5WVrB55sO8s6KFNsCcq4uFkZ2i+COIXH0jgnSeixiVwoqIiLOKCUJls6EtJXma1cP6DPZDCz+UQ3+9VarOY7l7RX7WbmvehxLQnQgdwyO00aIYjcKKiIizixlOfw8Ew6sMl+7elYGlvvsElgAdmXlM3tlCgs3Z1JauRFihL8Xtw+K5Xf9Ywj08bBLHdI8KaiIiDg7wzADy9KZcGC12ebqAT0nmoElKM4uZRwpLOHDNQf4YE0aRwrN9Vi83F24oXdrfj84jvZhfnapQ5oXBRURkcaitsBicYX4m8xZQqGd7FJGSXkFX23J4t0VKezMyre1X9IxlN8PiuPSjqG4aHqz1BMFFRGRxih1JST9G379qbLBYq50O/T+Bl+av4phGCSnHOOdFSn8uOsQVb8h4kJ8uD0xjhv7tsZf05vlIimoiIg0ZhkbIOl52P11dVv74TD0AYhNtFsZaUdP8MHqND5Zn05BcTlgLtN/Y5/W3D4ojnahLexWizQtCioiIk1Bzi4zsGyfD4Y54JXYwTD4PuhwJdhpSvGJknI+35TBe6tS2ZdTaGtXt5DUlYKKiEhTcmw/rJgFm+dW79Yc1g0G3wvdx4GrfbpiDMNg5b6jzFmVypLd1d1CsSE+3DYwlpv6RBPgo24hOTcFFRGRpig/E1a/AhvmQGnlnY2AaEicBr1uA0/7dcUcOFrEB2tS+XhddbeQl7sLoxNacVtiLN1bBditFml8FFRERJqyk8dh/buw5nU4kWO2eQVC/6nmo0Wo3UopKi3ni82ZvL86jV2nzBbqFRPI7YmxXB0fiaebq93qkcZBQUVEpDkoK4YtH8GqF83uIQA3L+h1KyTeA8Ft7FaKYRhsSDvO+6vTWLQ9y7YZYoivB+P7RTNxYCytAr3tVo84NwUVEZHmxFphzhBaMQsyN5ptFhfoOgYG/xmietm1nMMFJXy89gBz1x4gK68YABcLXNE5nFsHxnBJBw2+be4UVEREmiPDMDc+XPkC7Puxur3NpeZqt20vt9tMIYDyCis/7srhgzWpNfYWign2YeKAGG7qG02wr5bqb44UVEREmrvsbbDyRdj+GRgVZltEvDm1uesYcHWzazm/Hi7kwzUHmLehevCth5sL18RHcuvAWHrHBGoH52ZEQUVEREy5B2D1q7DxPSgrMtsCYyBxOvSaCB6+di3nZGkFX23J5IM1aWzLyLO1d4n059aBMYzu2YoWnvYNUWJ/CioiIlJT0TFY9zYkvw5Fld0w3kHVM4V8W9q9pC3puXywJo2vtmRSUrmDs6+HK9f3jGJC/xjiWwXoLksTpaAiIiK1Ky2CLXNh1UtwPNVsc/Myd21OnAYh7exeUm5RKfM3HGRu8gH2Hzlha+8W5c8t/WMY3TNK+ws1MQoqIiJydtYK2PWlOfA2c1NlowW6Xg+D7oXWfexeUtWGiB+tPcCi7dmUVt5l8XZ35bqESCb0j6FntMayNAUKKiIicn4MA1JXmGux7P2huj12sLlEf/srwcXF7mUdP1HK55sy+GjtgRr7C3WO8GN8v2jG9mpFoI9mDDVWjSaoLF++nGeffZYNGzaQlZXFggULGDNmzHm/X0FFRKQeHdppdgltm1e9p1BoZxg0HeJvAjdPu5dUtZDc3LUH+GZrlm0si4ebCyO6RTC+bzSD2oVoXZZGptEElUWLFrFy5Ur69OnDuHHjFFRERJxBXgYkvwbr50BpgdnmFwkD7oK+vwcvx+zjk1dUxsLNGXyyLp2dpyzX3zrIm5v7RnNjn9ZEafXbRqHRBJVTWSwWBRUREWdSnAfrZ5szhQqyzDYPP+g7GQb8CQJaOay07Rl5fLzuAF9szrSty+JigaEdQrmlXzTDuoTj4Wb/Lis5P002qJSUlFBSUmJ7nZ+fT3R0tIKKiEhDKi81u4NWvQSHd5ltLm5md9Cg6RDezWGlnSyt4LsdWXyyLp01+4/Z2oN83BndsxU39mmtnZydUJMNKjNmzOCJJ544rV1BRUTEDqxW2LfYXPE2bUV1e/srzcDS5hK7LtH/W6lHTvDp+nTmbzhITkH1/6ntEunPjX1aM7pnFC1b2H+cjZyuyQYV3VEREXESGRvMwLLrSzDMAa5E9IBBf4ZuY8DVceuelFdYSdp3hPkbDrJ4xyFKK8z63FwsXN45jBv7tObyTmHqGnKgJhtUfktjVEREHOzYfljzGmz6b/US/f6tYeBd0HsSeDn23+bcolK+2prF/A0H2ZKea2sP9vXg+oQoxvVupRVwHUBBRURE7KvoGKx/B5LfhBM5ZpunP/SZZM4WCmjt2PqAvYcKmL/xIJ9vzODwKV1DbUN9GduzFWN6tSI62MeBFTYfjSaoFBYWsm/fPgB69erF888/z+WXX05wcDAxMTHnfL+CioiIkykrhm2fwqqX4cges83FDbqNg4F/gla9HVsflV1De4+wYFMGP+zMprjMajvWNzaIMb1acU18JEG+WlCuoTSaoLJ06VIuv/zy09onTZrEnDlzzvl+BRURESdltcK+H80Vb1OTqtujB5rdQp2vA1fH75JcWFLO99uzWbg5g5X7jmCt/I3o7mrhsk5hjO3Viis6h+Hl7urYQpuYRhNULpaCiohII5C5yRzHsv3z6hVv/VtD/zuh9+3gE+zY+iodyi/my82ZLNiUUWNBOT8vN0Z1j2BMr1YMbKNVcOuDgoqIiDifgmxY9w6sfxeKjphtbt7Qc4I5jiW0k2PrO8We7AIWbMrgy80ZZOYV29oj/L0Y3TOKMb1a0SVSv3fqSkFFREScV1kxbJ8Pa16HQ9uq29tdAf3uhI4jwMU5ulqsVoO1qcf4YnMG32zNIr9yFVyATuF+jO4VxeierWilpfsviIKKiIg4v6qdm5Nfh93fAJW/jvxbm7OFet8OfhEOLfFUJeUV/Lz7MAs3ZfDT7hzb+iwAvWMCuTo+kqvjI7Xf0HlQUBERkcblWIo5vXnTh3Cycil8FzfofA30vQPaXOrQVW9/K6+ojEXbs1i4OYPklGOc+pu0Z3Qg18RHMio+gtZBmu5cGwUVERFpnMqKYecXZmhJT65uD2lvBpaECU4z+LbKofxiFm3L4tvt2axLrRlaEqIDuSY+glHdI7VGyykUVEREpPHL3m4OvN36CZQWmm1uXtDleuj5O/Mui4tzLYOfk1/Mdzuy+WZrFmt/E1q6RflzVdcIruoWTucIv2a9Gq6CioiINB0lBebuzeverTn41r81JNxihpaQdo6r7wxyCor5fschvt2aRXLKUdsaLQAxwT5c1TWcq7pF0Cc2CNdmNuVZQUVERJoew4CMjbD5Q3PWUHFe9bGYRDOwdB3j8P2FanO0sIQlu3P4YcchkvYepqS8eiBusK8Hw7uEcVXXCIZ0aNksFpdTUBERkaatrBj2fAub58KvS6p3cHbzhq7XQ4/xZteQE6x++1tFpeUs/+UIP+zMZsmuHPJOltmOebq5kNguhMs7hXF5pzBiQprmuBYFFRERaT7ys2Drx2ZoOfJLdbtvqHmHpfsNED3A6cazgLnv0NrUY/yw4xCLdx4iI/dkjeNtQ31toaVfmyA83ZrG3RYFFRERaX4MAzI2mF1DOxZWT3MGCIiGbmMh/kaI6OFUU52rGIbB3pxCft6dw897clifepzyUwa2+Hi4Mrh9Sy7rFMrQ9qGN+m6LgoqIiDRvFWWwf5k5lmXX11BaUH0spIN5l6X7DRDa0XE1nkN+cRkr9x7h5z05/LznMIcLSmocjw3xYUj7lgzt0JLEdi0J8HZ3UKUXTkFFRESkStlJ2PsDbJsPv3wPFaf8wm/ZCbpcZz4iE5zyTguYS/nvzMrn5905JO09wsYDNe+2uFjMNVuGdghlaIeW9IwOxN3V+bq6qiioiIiI1KY43xyEu20+7F9avZszQEAMdLnWDC3RA5xmv6HaFJaUs+bXoyTtPUzSviPsP3yixnFfD1f6tQlmYNsQBrYNoXuUP25OFFwUVERERM6lOA9++QF2fQn7foSyoupjvqHm8v2dr4O4IeDu5bg6z0NG7klW7D3M8r1HWLnvCLlFZTWO+3q40jeuKrgE071VgEPvuCioiIiIXIjSIvj1J9j1FfyyqOYaLe6+0PYyc1fnjiOcaqPE2lRYDXZn57Nm/zHW7D9K8v6jNXZ9BjO49IkLZkCbYPrFBdOjdYBd129RUBEREamrijJITYKdX8Iv30FBVs3jkT2h40gztET2dMppz6c6Nbgk7z9KcsqxGmu3AHi4uZDQOoB+ccH0axNMn9gg/L0abnCugoqIiEh9MAzI3moOwv3lO3P686lahEOHK6HdMPOui5NtmFgbq9Vgd3YBySlHWZd6jLUpxzlSWHNGkYsFukT60y8umEs7hnJ557B6rUFBRUREpCEUHIJ9i83Q8uvP1ZslAmCBqJ7Q7grz0bo/uHk4qtLzZhgGqUeLWJtylLUpx1mXeowDx6rH61zZNZy3bu9br9+poCIiItLQyksgbRXsXQz7f4acnTWPu/tC3GAztLS9HEI7Oe305986lF/M2pRjrEs9Ru+YIMb0alWvn6+gIiIiYm/5WeaU519/MoPLicM1j7cIN2cQxQ01HyHtGk1wqW8KKiIiIo5ktULODrN76Nef4MBqKC+ueY5fZGVwqQwvwW2bTXBRUBEREXEmZcWQsR5SkiB1BRxcCxWlNc/xizK7imIGQkwihHZx+hlFdaWgIiIi4szKTsLBdWZoSUkyn1trThnGKwCiB1YHl6heTr/w3PlSUBEREWlMSovMuywH1pjdROnroKzmsvi4ekCrPhDd35xR1Lof+IU7pt6LpKAiIiLSmFWUQfa2yuCyyvzvbwfngrk/Ueu+leGlH0TEg5un/eu9QAoqIiIiTYlhwLH95nTog2vh4HrI2QX85le4q4e5C3SrvuaaLpE9oWUHp9tgUUFFRESkqSvOh8yN5viWg+vN/xYdPf08d1/zTktVcInqCSEdwNXNzgVXU1ARERFpbgwDjqeY41syN0LmZnP5/1N3ha7i5m2Gl4juEN4NwrtDWFfwss/vUgUVERERAWsFHNkLWVsga3N1eKmx9P8pAmOqQ0tVgAluW+93XxRUREREpHZWKxzdZ4aXnB1wqPKRn1H7+e2ugNsW1GsJF/L723EdVCIiImJ/Li4Q2tF8cFN1+8njcGhnZXDZbv43Zye07OSwUkFBRURERAC8g8yVceMGV7dZrVB+0nE1AU1zbV4RERG5eC4u4OHr2BIc+u0iIiIiZ6GgIiIiIk5LQUVEREScloKKiIiIOC0FFREREXFaCioiIiLitBRURERExGkpqIiIiIjTUlARERERp6WgIiIiIk5LQUVEREScloKKiIiIOC0FFREREXFabo4u4GIYhgFAfn6+gysRERGR81X1e7vq9/jZNOqgUlBQAEB0dLSDKxEREZELVVBQQEBAwFnPsRjnE2eclNVqJTMzEz8/PywWS71+dn5+PtHR0aSnp+Pv71+vny2n0/W2L11v+9L1ti9db/uqy/U2DIOCggKioqJwcTn7KJRGfUfFxcWF1q1bN+h3+Pv76y+6Hel625eut33petuXrrd9Xej1PtedlCoaTCsiIiJOS0FFREREnJaCyhl4enry+OOP4+np6ehSmgVdb/vS9bYvXW/70vW2r4a+3o16MK2IiIg0bbqjIiIiIk5LQUVEREScloKKiIiIOC0FFREREXFaCiq1eOWVV4iLi8PLy4sBAwawdu1aR5fUJCxfvpzrrruOqKgoLBYLCxcurHHcMAwee+wxIiMj8fb2Zvjw4ezdu9cxxTYBM2fOpF+/fvj5+REWFsaYMWPYs2dPjXOKi4uZNm0aISEhtGjRghtuuIFDhw45qOLG7bXXXqNHjx62Ra8SExNZtGiR7biudcN6+umnsVgs3HfffbY2XfP6M2PGDCwWS41H586dbccb8lorqPzGJ598wl/+8hcef/xxNm7cSEJCAiNGjCAnJ8fRpTV6J06cICEhgVdeeaXW48888wwvvvgir7/+OsnJyfj6+jJixAiKi4vtXGnTsGzZMqZNm8aaNWtYvHgxZWVlXHXVVZw4ccJ2zv/8z//w1VdfMW/ePJYtW0ZmZibjxo1zYNWNV+vWrXn66afZsGED69ev54orrmD06NHs2LED0LVuSOvWreONN96gR48eNdp1zetXt27dyMrKsj1WrFhhO9ag19qQGvr3729MmzbN9rqiosKIiooyZs6c6cCqmh7AWLBgge211Wo1IiIijGeffdbWlpuba3h6ehofffSRAypsenJycgzAWLZsmWEY5vV1d3c35s2bZztn165dBmCsXr3aUWU2KUFBQcbbb7+ta92ACgoKjA4dOhiLFy82Lr30UuPee+81DEN/v+vb448/biQkJNR6rKGvte6onKK0tJQNGzYwfPhwW5uLiwvDhw9n9erVDqys6UtJSSE7O7vGtQ8ICGDAgAG69vUkLy8PgODgYAA2bNhAWVlZjWveuXNnYmJidM0vUkVFBR9//DEnTpwgMTFR17oBTZs2jWuuuabGtQX9/W4Ie/fuJSoqirZt2zJx4kQOHDgANPy1btSbEta3I0eOUFFRQXh4eI328PBwdu/e7aCqmofs7GyAWq991TGpO6vVyn333cfgwYPp3r07YF5zDw8PAgMDa5yra15327ZtIzExkeLiYlq0aMGCBQvo2rUrmzdv1rVuAB9//DEbN25k3bp1px3T3+/6NWDAAObMmUOnTp3IysriiSeeYOjQoWzfvr3Br7WCikgzMG3aNLZv316jT1nqX6dOndi8eTN5eXnMnz+fSZMmsWzZMkeX1SSlp6dz7733snjxYry8vBxdTpM3atQo2/MePXowYMAAYmNj+fTTT/H29m7Q71bXzylatmyJq6vraSOVDx06REREhIOqah6qrq+uff275557+Prrr/n5559p3bq1rT0iIoLS0lJyc3NrnK9rXnceHh60b9+ePn36MHPmTBISEnjhhRd0rRvAhg0byMnJoXfv3ri5ueHm5sayZct48cUXcXNzIzw8XNe8AQUGBtKxY0f27dvX4H+/FVRO4eHhQZ8+fViyZImtzWq1smTJEhITEx1YWdPXpk0bIiIialz7/Px8kpOTde3ryDAM7rnnHhYsWMBPP/1EmzZtahzv06cP7u7uNa75nj17OHDggK55PbFarZSUlOhaN4Bhw4axbds2Nm/ebHv07duXiRMn2p7rmjecwsJCfv31VyIjIxv+7/dFD8dtYj7++GPD09PTmDNnjrFz505j6tSpRmBgoJGdne3o0hq9goICY9OmTcamTZsMwHj++eeNTZs2GWlpaYZhGMbTTz9tBAYGGl988YWxdetWY/To0UabNm2MkydPOrjyxulPf/qTERAQYCxdutTIysqyPYqKimzn3HXXXUZMTIzx008/GevXrzcSExONxMREB1bdeD300EPGsmXLjJSUFGPr1q3GQw89ZFgsFuOHH34wDEPX2h5OnfVjGLrm9en+++83li5daqSkpBgrV640hg8fbrRs2dLIyckxDKNhr7WCSi1eeuklIyYmxvDw8DD69+9vrFmzxtElNQk///yzAZz2mDRpkmEY5hTlRx991AgPDzc8PT2NYcOGGXv27HFs0Y1YbdcaMGbPnm075+TJk8bdd99tBAUFGT4+PsbYsWONrKwsxxXdiN1xxx1GbGys4eHhYYSGhhrDhg2zhRTD0LW2h98GFV3z+jN+/HgjMjLS8PDwMFq1amWMHz/e2Ldvn+14Q15ri2EYxsXflxERERGpfxqjIiIiIk5LQUVEREScloKKiIiIOC0FFREREXFaCioiIiLitBRURERExGkpqIiIiIjTUlARERERp6WgIiJNisViYeHChY4uQ0TqiYKKiNSbyZMnY7FYTnuMHDnS0aWJSCPl5ugCRKRpGTlyJLNnz67R5unp6aBqRKSx0x0VEalXnp6eRERE1HgEBQUBZrfMa6+9xqhRo/D29qZt27bMnz+/xvu3bdvGFVdcgbe3NyEhIUydOpXCwsIa57z77rt069YNT09PIiMjueeee2ocP3LkCGPHjsXHx4cOHTrw5ZdfNuwPLSINRkFFROzq0Ucf5YYbbmDLli1MnDiRW265hV27dgFw4sQJRowYQVBQEOvWrWPevHn8+OOPNYLIa6+9xrRp05g6dSrbtm3jyy+/pH379jW+44knnuDmm29m69atXH311UycOJFjx47Z9ecUkXpSL3swi4gYhjFp0iTD1dXV8PX1rfH45z//aRiGYQDGXXfdVeM9AwYMMP70pz8ZhmEYb775phEUFGQUFhbajn/zzTeGi4uLkZ2dbRiGYURFRRl/+9vfzlgDYDzyyCO214WFhQZgLFq0qN5+ThGxH41REZF6dfnll/Paa6/VaAsODrY9T0xMrHEsMTGRzZs3A7Br1y4SEhLw9fW1HR88eDBWq5U9e/ZgsVjIzMxk2LBhZ62hR48etue+vr74+/uTk5NT1x9JRBxIQUVE6pWvr+9pXTH1xdvb+7zOc3d3r/HaYrFgtVoboiQRaWAaoyIidrVmzZrTXnfp0gWALl26sGXLFk6cOGE7vnLlSlxcXOjUqRN+fn7ExcWxZMkSu9YsIo6jOyoiUq9KSkrIzs6u0ebm5kbLli0BmDdvHn379mXIkCF8+OGHrF27lnfeeQeAiRMn8vjjjzNp0iRmzJjB4cOHmT59Orfddhvh4eEAzJgxg7vuuouwsDBGjRpFQUEBK1euZPr06fb9QUXELhRURKRefffdd0RGRtZo69SpE7t37wbMGTkff/wxd999N5GRkXz00Ud07doVAB8fH77//nvuvfde+vXrh4+PDzfccAPPP/+87bMmTZpEcXEx//nPf3jggQdo2bIlN954o/1+QBGxK4thGIajixCR5sFisbBgwQLGjBnj6FJEpJHQGBURERFxWgoqIiIi4rQ0RkVE7EY9zSJyoXRHRURERJyWgoqIiIg4LQUVERERcVoKKiIiIuK0FFRERETEaSmoiIiIiNNSUBERERGnpaAiIiIiTuv/AQBP2g30UZ0NAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6-2.모델 평가"
      ],
      "metadata": {
        "id": "NnEShtLj5vs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = torch.load(\"/content/drive/MyDrive/Lee/marianmodel_case1_epoch50_traindata45000\")\n",
        "trained_model.eval()"
      ],
      "metadata": {
        "id": "LZyJieShh-mI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea6a69a9-e93a-4c56-b608-9a7b1320feb2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MarianMTModel(\n",
              "  (model): MarianModel(\n",
              "    (shared): Embedding(65001, 512, padding_idx=65000)\n",
              "    (encoder): MarianEncoder(\n",
              "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): MarianDecoder(\n",
              "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84301a0b-594e-4b2e-ff7f-43f1a6c72336",
        "id": "w7hmBlnMkm8m"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[14597, 28541,    48,  ..., 65000, 65000, 65000],\n",
              "         [    9,  9099,  1815,  ..., 65000, 65000, 65000],\n",
              "         [  515,  1329,    53,  ..., 65000, 65000, 65000],\n",
              "         ...,\n",
              "         [    9, 33909,   500,  ..., 65000, 65000, 65000],\n",
              "         [  471,  9024, 20092,  ..., 65000, 65000, 65000],\n",
              "         [    9, 17128,    53,  ..., 65000, 65000, 65000]]),\n",
              " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
              " 'decoder_input_ids': tensor([[ 1278, 34239,     4,  ..., 65000, 65000, 65000],\n",
              "         [  125,  2188,  1673,  ..., 65000, 65000, 65000],\n",
              "         [   57,   101,    35,  ..., 65000, 65000, 65000],\n",
              "         ...,\n",
              "         [   57,  1102,   494,  ..., 65000, 65000, 65000],\n",
              "         [  158,    10,     4,  ...,  5659,    49,     0],\n",
              "         [   57,     9,  3166,  ...,  3001,    31,     0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "batch = next(iter(valid_dl))\n",
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "input_ids = batch['input_ids'].to(device)\n",
        "input_ids, input_ids.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09368a08-e55a-46f9-abf9-2886e03f5228",
        "id": "dBp-bct1km8n"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[14597, 28541,    48,  ..., 65000, 65000, 65000],\n",
              "         [    9,  9099,  1815,  ..., 65000, 65000, 65000],\n",
              "         [  515,  1329,    53,  ..., 65000, 65000, 65000],\n",
              "         ...,\n",
              "         [    9, 33909,   500,  ..., 65000, 65000, 65000],\n",
              "         [  471,  9024, 20092,  ..., 65000, 65000, 65000],\n",
              "         [    9, 17128,    53,  ..., 65000, 65000, 65000]], device='cuda:0'),\n",
              " torch.Size([50, 128]))"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translated_ids = trained_model.generate(input_ids, num_beams=2, max_length=128, early_stopping=False)\n",
        "print(translated_ids), len(translated_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cb24750-2d3c-498b-c432-821e85efefef",
        "id": "uf3gCLUNkm8n"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[65000,    16,    12,  ..., 65000, 65000, 65000],\n",
            "        [65000,  9750,  6087,  ..., 65000, 65000, 65000],\n",
            "        [65000,   138,    94,  ...,  2959,  2959,     0],\n",
            "        ...,\n",
            "        [65000,    57,   419,  ..., 65000, 65000, 65000],\n",
            "        [65000,   158,   807,  ...,  2906,  2906,     0],\n",
            "        [65000,   327,     4,  ...,   350,   350,     0]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translated_text_list = []\n",
        "for i in range(len(translated_ids)):\n",
        "    translated_text = tokenizer.decode(translated_ids[i], skip_special_tokens=True)\n",
        "    translated_text_list.append(translated_text)\n",
        "    print(f\"{i+1}번째 번역 결과:\", translated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9aba2dc-fd5a-4a10-9efb-fc0abefc24c7",
        "id": "E5D_mgoPkm8o"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1번째 번역 결과: I'm finally opening up Elsie tomorrow.\n",
            "2번째 번역 결과: Sunbaeon, do you know your name?\n",
            "3번째 번역 결과: In an online video last July, entitled \"Syriammmiririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririr\n",
            "4번째 번역 결과: As we talked about in the last column, there are a lot of areas in which medication can be helpful, but there are many problems that are not solved with medication.\n",
            "5번째 번역 결과: The feeling that South Korea is relatively small when it is tied up with China and Japan with the Eastern Festival was not different from this festival.\n",
            "6번째 번역 결과: The older you get, the more transformational you are.\n",
            "7번째 번역 결과: It's a 35-dollar gas per hour for Sydney's guided tour.\n",
            "8번째 번역 결과: A survey of the ‘unreplicititititalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalal\n",
            "9번째 번역 결과: As much as it's been said, \"Pretendendententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententent\n",
            "10번째 번역 결과: We can handle this exchange today and we can just ship the jackets right away.\n",
            "11번째 번역 결과: I'm trying to get a bank account on the Internet, but I can't.\n",
            "12번째 번역 결과: As we're doing this, as if we're buying things at the store, we're showing a barcode on the app, and the purchase is a blockededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededed\n",
            "13번째 번역 결과: The head of the agency has signed an agreement that the company, if it wants to be a private agent, has to be in the service of the state...............................................................................................\n",
            "14번째 번역 결과: The police officer told me that he was grateful.\n",
            "15번째 번역 결과: It's a fact that it's a source of information on the nature of the event or the character of the object.\n",
            "16번째 번역 결과: It's only been a year since the public press reports on the scandal that the prosecutor was sexually abused in late January.\n",
            "17번째 번역 결과: In a report from the United Nations Trade and Development Conference, there is no direct evidence that the international right to make payments increases.\n",
            "18번째 번역 결과: On the international market, the goods were full of illegal spills, such as wheat trading goods, U.S. military goods, and U.N. aid.\n",
            "19번째 번역 결과: A lot of people get their income from the car industry.\n",
            "20번째 번역 결과: In spite of being the world's most famous Boy Group, many people in Belgium have heard the name for the first time in this article.\n",
            "21번째 번역 결과: The missing woman was confirmed by the SNS message to her friend just before she disappeared, saying, \"If anything happens to me, report it to me.\"\n",
            "22번째 번역 결과: A growing number of voices are that the city and the government need to support it in order to reduce the volume of the city's subways, which is growing with the number of off-limitititationalalseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseses\n",
            "23번째 번역 결과: More of the companies are being used to make use of the trade-lines..............................................................................................................\n",
            "24번째 번역 결과: The post-opttimistryryryinginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginginglylylylylyly,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
            "25번째 번역 결과: The head of the board has to create the conditions for the rise of reading and culture, and so on.........................................................................................................\n",
            "26번째 번역 결과: Death is not just sad because it gives birth to a new life.\n",
            "27번째 번역 결과: My daughter's going to go to college next year.\n",
            "28번째 번역 결과: The following year the first manuscript was tied up by a ‘ Radio Talk ’ and was published in 1943 as a three - volume booklet ‘ Christian Behavior ’ and ‘ Beyond character ’ in 1945.\n",
            "29번째 번역 결과: This is a popular K-Pop music video, and it's a little bit of a comicalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalal\n",
            "30번째 번역 결과: The Russian game that hit Spain was a typical one of these..................................................................................................................\n",
            "31번째 번역 결과: If you look at it on Google, you'll see that it's a huge amount of data.\n",
            "32번째 번역 결과: A group of new students, including K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K\n",
            "33번째 번역 결과: The fear of the \"Saturmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm\n",
            "34번째 번역 결과: In an interview with the Wall Street Journal, who was interested in his new book, he talked about the anger of Asian-Americansseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseses\n",
            "35번째 번역 결과: The Athleteerereralalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalal\n",
            "36번째 번역 결과: While the semiconductoresissseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseseses\n",
            "37번째 번역 결과: I'm going to review the plans and return them to you.\n",
            "38번째 번역 결과: A week ago, a card terminal called for a change was not heard.\n",
            "39번째 번역 결과: Food and Drug Administration has also suggested that the cost of raw materials, including the annual ups and ups and ups, “is that the cost of the human beings is being absorbed into their own resources, and they are no longer able to afford it. ”\n",
            "40번째 번역 결과: What new business is so popular in Indonesia?\n",
            "41번째 번역 결과: It's a hip-hop of the Midium Tempo, composed and sung by rapper RuFFeeeeeonononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononon\n",
            "42번째 번역 결과: It's 14 extraordinary jobs if you know.\n",
            "43번째 번역 결과: The fire department's ever-ending challenge to the top ten-point-10ththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththth\n",
            "44번째 번역 결과: I hope the engine doesn't fail.\n",
            "45번째 번역 결과: According to the January 27th daily newspaper Giario de S. S. S. Paulo, the cause of the heavy rains is a greenhouse phenomenon that causes the summer showers in southeastern Brazil.\n",
            "46번째 번역 결과: The Cultural Stadium miners and the South Korean Tourism Projectalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalal\n",
            "47번째 번역 결과: My greatest wish is to stand by you, to be near you.\n",
            "48번째 번역 결과: However, the steamshipsserererateateateateateateateatementmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentedededededededededededed\n",
            "49번째 번역 결과: The army has already been on the line for nine years.\n",
            "50번째 번역 결과: The \"Intertereral Marketing Show\" was last year with a total of 2,000 foreign audiences in the Philippines, Canada, Belgium and Spain.\n",
            "51번째 번역 결과: Shin has been releasing his real name and face online and has claimed that the content of the 'I want to know' is not true...................................................................................................\n",
            "52번째 번역 결과: The National Defense Department has one vice-presidididententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententent\n",
            "53번째 번역 결과: This is a measure of less weight to take care of the major time zone on Saturday.............................................................................................................\n",
            "54번째 번역 결과: I go to the library at the end of two o'clock to study all the time.\n",
            "55번째 번역 결과: The medium said that the patient has been unconscious since the 10th day of the hospital, and has relied on the ventilator to be in the intensive care unit.\n",
            "56번째 번역 결과: He went to the spot to see the Assistantantantantantantantantantantantantantantantantantantantantantantantantantantantantantantantantantantantantantantantantantantantantialialialialialialialialialialialialialialialialalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalal\n",
            "57번째 번역 결과: You need to tell the person in charge to take a good look at the rules.\n",
            "58번째 번역 결과: Japanese courts have again dismissed claims of damages to the Japanese government from their families who have suffered bomb damage in Japan.......................................................................................................\n",
            "59번째 번역 결과: I was made to use this accessory by connecting the camera and the accessory to the others in the first place.\n",
            "60번째 번역 결과: All right, well, at the next fair, you're gonna have to be careful to get ready for us to join in.\n",
            "61번째 번역 결과: The idea is that the law and ratifications will be changed from the \"after the pre-constitutional\" policy to the \"after the ratification of the law\" to the \"after the law\" of the law to the law of the law to the law of the law to the law of the law to the law of the law to the law of the law to the law of the law to the law of the law to the law of the law to the law of the law to the law of the law to the law of the law to the law of the law of the law to the the law\n",
            "62번째 번역 결과: In addition, the name of the former pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro pro\n",
            "63번째 번역 결과: This is going to be the best school in the country.\n",
            "64번째 번역 결과: On the 27thth, the National Bankerateateateateateateatementmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentmentment\n",
            "65번째 번역 결과: Many people are not finding their way into this new environment.\n",
            "66번째 번역 결과: The game was to grant a national office a first-class, a top-level, a serviceableableableableableableableableableableableableableableableableableableableablesssssssssssssssssssssededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededed\n",
            "67번째 번역 결과: A brewery that was founded by Ian Hill, a famous bottle of beer in Vancouver.............................................................................................................\n",
            "68번째 번역 결과: As long as the window employees don't have to ask a number of times what their customers want, they can cut back on time and effort.................................................................................................\n",
            "69번째 번역 결과: The National Marine Survey has announced its \"No-Sepmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm\n",
            "70번째 번역 결과: Incomeoveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroveroverover\n",
            "71번째 번역 결과: A few, up to 30 years of age, are women who are out of their jobs, challenging the IT field programmers.....................................................................................................\n",
            "72번째 번역 결과: What if there's no room for it this time?\n",
            "73번째 번역 결과: The head of the Ministry of Justice for the state of the country, in the case of the state of the country, is the the state of the country...............................................................................................\n",
            "74번째 번역 결과: With this, the opening of the May issue of the New York State Department of Public Health and Public Health Service of the United States, the first public - state - state - state public - state public - state - state public - state public - state public - state - state public - state public - of - the public public - of - the state of the country, has been on a high level for about a year..........................................\n",
            "75번째 번역 결과: Middle Eastern terrorist groups have been waiting for the U.S. to pull their feet off.\n",
            "76번째 번역 결과: Why haven't you got a check email yet?\n",
            "77번째 번역 결과: Actualityalalizationalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalal\n",
            "78번째 번역 결과: I need a lot of comfort around here, and that's even more annoying.\n",
            "79번째 번역 결과: The by-products of the second part of this section of this section, which is the most important, are the re-use of the water or the water, or the water, or the water, or the water, or the water, or the water, or the water, or the water, or the water, or the water, or the water, or the water, or the water, or the water, or the water, or the water, or the water, or the water, or the water, or the water, or the water, or the water, or the water, or the water,\n",
            "80번째 번역 결과: Let's go to the movies.\n",
            "81번째 번역 결과: The police are investigating the exact cause of the accident, such as checking for CCTVs, as they see one side of the vehicle's traffic in the wrong direction.\n",
            "82번째 번역 결과: Bakleithththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththththth\n",
            "83번째 번역 결과: In the summer, cool Cokes are popular with people.\n",
            "84번째 번역 결과: The lab explains the research background of the country's new demand for new land, while the country's production is decreasing, especially on the South Coast, with 90 percent of its output being concentrated, and new form of land development is needed.\n",
            "85번째 번역 결과: I'm very happy with my position, and I'm particularly well-stappededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededed\n",
            "86번째 번역 결과: In the case of the country, more than 20 million of them were sold from March to the end of the year, with over 65 million of them sold this year, and in about one year and eight months, they had more than 65 million of them.\n",
            "87번째 번역 결과: The \"businesser\" is the one who is trying to run the road, who is trying to run the road, who is doing it or who is doing it.\n",
            "88번째 번역 결과: And even if they did agree, it's actually not good for the media to do this, to look at the victim, to look at the victim's past, and to show him the video or the picture.\n",
            "89번째 번역 결과: These are examples of the ability of the Earth to self-immolololtontontontontontontontontontontontontontontonssssssssedededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededededed\n",
            "90번째 번역 결과: The police also see Mr. A as the one who took his own life, but they're doing a multiple-way investigation without not ruling out the possibility of his being killed.\n",
            "91번째 번역 결과: It's just that they're not in the right place for a long time.............................................................................................................\n",
            "92번째 번역 결과: It is the school of dreams, the key commitment of this sense of education, and it is the University of Dreams, and it is the program that is providing children with innovations that are out of school....................................................................................\n",
            "93번째 번역 결과: The quality of the construction work is clearly determined by how the predictions before the project match up with the actual work done.\n",
            "94번째 번역 결과: The government has listed as a ‘Pidididententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententent\n",
            "95번째 번역 결과: I want to make sure that they don't have a meeting next week.\n",
            "96번째 번역 결과: What about the shampoos that I bought?\n",
            "97번째 번역 결과: And by that time, there was a little disturbance in the U.S.\n",
            "98번째 번역 결과: The homesteady courses that we recommend are free to learn.\n",
            "99번째 번역 결과: A four-day, four-day Abu Dhabiaiririririririririririririririririririririririririririririririririririririririririririririririririririririrururururururururururururururururururururururururururururururururururururururururururururururururururururururururururururur\n",
            "100번째 번역 결과: As the 5th party agreed to reform the election system, the pro-consistivivivivivististististististististististististryryryryryryryryyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch['decoder_input_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "588ca50d-6f72-4f4b-d345-1d389c2acd4e",
        "id": "IzhzY_B8km8o"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1278, 34239,     4,  ..., 65000, 65000, 65000],\n",
              "        [  125,  2188,  1673,  ..., 65000, 65000, 65000],\n",
              "        [   57,   101,    35,  ..., 65000, 65000, 65000],\n",
              "        ...,\n",
              "        [   57,  1102,   494,  ..., 65000, 65000, 65000],\n",
              "        [  158,    10,     4,  ...,  5659,    49,     0],\n",
              "        [   57,     9,  3166,  ...,  3001,    31,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_translated_text_list = []\n",
        "for i in range(len(batch['decoder_input_ids'])):\n",
        "    answer_translated_text = tokenizer.decode(batch['decoder_input_ids'][i], skip_special_tokens=True)\n",
        "    answer_translated_text_list.append(answer_translated_text)\n",
        "    print(f\"{i+1}번째 정답 결과:\", answer_translated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c22f1e7-d410-45fa-d9c2-ddeb2244d702",
        "id": "JBS_OCnekm8p"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1번째 정답 결과: With the National Pension Service set to introduce the \"Stewardship Code\" in the second half, companies with no or low propensity to dividend among the companies in which 5% or more of the stake is owned by National Pension Service has emerged as the beneficiaries of the Stewardship Code.\n",
            "2번째 정답 결과: Heungkuk Life played the first game of the fifth round three days ago and rested for two days, but it didn't seem to have any physical problems.\n",
            "3번째 정답 결과: There was a scene where she crosses the river on the way to meet the wizard of OZ.\n",
            "4번째 정답 결과: In the Seoul concert, the blind Jeong Seon-hwa and Myung Chang will sing a pansori to the drumbeat of Cho Kyung-gon, a visually impaired entertainer, who holds the No. 23 (drum and janggu) intangible intangible asset of Incheon City.\n",
            "5번째 정답 결과: The head of the Gu may choose not to place an advertisement where it is impossible to post the advertisement due to an event of force majeure, such as a natural disaster. In such cases, the head of the Gu shall return the amount equivalent to the remaining period to the advertiser.\n",
            "6번째 정답 결과: The Gyeonggi-do Province Government will support promising provincial exporters to advance to the Southeast Asian market and export their products.\n",
            "7번째 정답 결과: The head of Gangseo-gu Busan Metropolitan Government (hereinafter referred to as the \"head of the Gu\") shall establish and implement policies to support the following projects in order to foster competitive fisheries:\n",
            "8번째 정답 결과: Not because you don't know it, but because you can't solve it.\n",
            "9번째 정답 결과: Although there were no sources or evidences to judge the written content to be true, it was quickly spreaded because it was a provocative price that could heavily damage the image of three.\n",
            "10번째 정답 결과: I will watch this movie again as many times as time permits.\n",
            "11번째 정답 결과: It has developed into a strong sense of mission to disseminate the significance of traditional craft in a wider world.\n",
            "12번째 정답 결과: After the invasion of Iraq, Kuwait has had the largest proportion of national security and terrorism prevention, and 15,000 US troops pulling out of Iraq have been stationed in Kuwait.\n",
            "13번째 정답 결과: The Ministry of Science and ICT and the Korea Communications Commission will establish the guidelines for the use of the network and establish guidelines within the year to restrict global CP from unfair acts by using superior status.\n",
            "14번째 정답 결과: The next day, the film festival screened \"The Perfect Way to Steal a Dog\", a movie that the whole family can watch together.\n",
            "15번째 정답 결과: In order to see a profit from Gap investment, the charter price has to rise or the sale price has to rise.\n",
            "16번째 정답 결과: They are using a computer instead of a car.\n",
            "17번째 정답 결과: The Stone Standing Buddha Statue in Yonghwasa Temple is a work of Goryeo period which inherited the traditional style of Unified Silla.\n",
            "18번째 정답 결과: President Moon is pitiful for relying on relief politics.\n",
            "19번째 정답 결과: I'll come to pick you up at 3.\n",
            "20번째 정답 결과: Where the head of the tax levy department receives the Gu tax report from the person liable for duty payment, he/she shall prepare the receipts ledger of attached Form 8 by date and notify the head of the collection office by the fifth day of the following month.\n",
            "21번째 정답 결과: However, in some areas, the daytime temperature is up to 33 degrees.\n",
            "22번째 정답 결과: I planted kimchi vegetables well today.\n",
            "23번째 정답 결과: I prepared to study English.\n",
            "24번째 정답 결과: According to the video uploaded to various communities on 31st, Lee Ju-yeon posted a short video on Instagram the day before, and deleted it.\n",
            "25번째 정답 결과: As of January-September this year, Woori Savings Bank had a default rate of 9.91 percent and Chohung Savings Bank 8.60 percent, widening the gap with the total loan delinquency rate of all savings banks during the same period.\n",
            "26번째 정답 결과: Choi Hwa-jung showed up in a red swimsuit and showed off her fat-free body despite her age approaching sixty, drawing admiration from Kim Sook and Song Eun-yi.\n",
            "27번째 정답 결과: I have responsibilities and I always try to do my duty.\n",
            "28번째 정답 결과: The artificial flower that Busan Middle School has sent, where he graduated, was also noticeable.\n",
            "29번째 정답 결과: Regarding Suji, Pungdeok, Hongcheon, Bojeong High School, there are a lot of students with relatively good grades.\n",
            "30번째 정답 결과: They belong to the same chrysanthemum family, but the name of the plant was very different.\n",
            "31번째 정답 결과: In a sense that the government has not confirmed it, it was named a draft.\n",
            "32번째 정답 결과: I want to order a few products of various types rather than a large quantity of just 1 product.\n",
            "33번째 정답 결과: In Eumseong, Chungbuk, a 20-year-old who ran away after causing a car accident was caught by the police.\n",
            "34번째 정답 결과: He also emphasized that \"All employees will strengthen our status as a leading company through thorough preparation and challenge, contributing to the development of shareholders, society, employees and the company together.\"\n",
            "35번째 정답 결과: Temperatures have dropped slightly below 30 degrees in some areas that have been swept by showers, but in many places, the sweltering weather still continues to hover above 35 degrees.\n",
            "36번째 정답 결과: I didn't understand it itself if the other talked indirectly or talked in circle without pointing it out.\n",
            "37번째 정답 결과: They only recorded 1 loss, but that loss was on the first day by 2-4 to Daedeok University, and as a result, they had to accept second place.\n",
            "38번째 정답 결과: The loss of the Vice President Lee in this process is even greater.\n",
            "39번째 정답 결과: I really wish you a fortune so you can go wherever you want to go.\n",
            "40번째 정답 결과: \"It was because of the rules of the Asian game that one hour and thirty minutes before the start of the game, some information about the starting pitcher of his team should be given to the opponent.\"\n",
            "41번째 정답 결과: Vehicles in operation can quickly stop on the right side of the road and evacuate, and if it is difficult to evacuate, turn off the ignition and listen to radio broadcasts in the vehicle.\n",
            "42번째 정답 결과: A director of Seoul Dongzak, Korea Workers' Compensation & Welfare Service has visited.\n",
            "43번째 정답 결과: Despite his relatively young age, a talent who gained attention in the Korean art scene by holding private exhibitions at the Grand Hyatt Hotel in Seoul and the LG Arts Center has attracted the attention of local experts as he participated in the group exhibition in New York.\n",
            "44번째 정답 결과: I will study hard my major and get a job in a company I have been wanting.\n",
            "45번째 정답 결과: Rather, more than half of the 27 schools surveyed, due to reasons such as a decrease in population, had a reduced or unchanged student population.\n",
            "46번째 정답 결과: While awareness and acceptance of multiculturalism is all the more important, the institute will push to foster nurturing multicultural education instructors to improve multicultural awareness among infants and children at daycare centers this year.\n",
            "47번째 정답 결과: To explain In terms of love and marriage, love is individual.\n",
            "48번째 정답 결과: The Ministry of National Defense plans to announce a conscientious objector alternative service in the coming month and provide legislative notice of the bill (an amendment to the Military Service Act).\n",
            "49번째 정답 결과: As the main supply class is college students and social beginners, a living-type shared space that reflects the residential trends of the youth will be designed, and community facilities such as open kitchen, car sharing, and shared laundry room, which will enable social dining, and youth start\n",
            "50번째 정답 결과: The entities eligible for the Fund loan shall be the business operators in Gangnam-gu Seoul Metropolitan Government. Provided, That karaoke bars, entertainment bars, and other business operators prescribed by the Rule which the head of the Gu deems inappropriate to finance shall be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6-3. 단어-단어 accuracy 측정 - train data"
      ],
      "metadata": {
        "id": "STOcaMb8xuak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 - 단어로 accuracy 측정\n",
        "\n",
        "trained_model = torch.load(\"/content/drive/MyDrive/Lee/marianmodel_case1_epoch50_traindata45000\")\n",
        "trained_model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "batch_size = 100\n",
        "max_length = 128  # 토큰 최대 길이\n",
        "batch_count = 0   # total_accuracy 계산을 위해 몇 번 배치가 실행되었는지 count\n",
        "total_batch_accuracy_list = []\n",
        "\n",
        "for idx, train_batch in enumerate(train_dl):\n",
        "\n",
        "    input_ids = train_batch['input_ids'].to(device)                                          # 배치 단위의 한글 문장\n",
        "    translated_ids = trained_model.generate(input_ids, max_length=128, early_stopping=True)  # 배치 단위의 모델 번역 문장\n",
        "    decoder_input_ids = train_batch['decoder_input_ids']                                     # 배치 단위의 영어 문장(정답)\n",
        "    one_batch_accuracy_list = []                                                             # 배치 단위의 정확도 리스트\n",
        "\n",
        "    translated_text_list = []\n",
        "    target_text_list = []\n",
        "\n",
        "    for i in range(len(translated_ids)):\n",
        "        translated_text = tokenizer.decode(translated_ids[i], skip_special_tokens=True)\n",
        "        translated_text_list.append(translated_text)\n",
        "        # print(f\"{i+1}번째 번역 결과:\", translated_text)\n",
        "    # print(len(translated_text_list))\n",
        "\n",
        "    for i in range(len(decoder_input_ids)):\n",
        "        targer_text = tokenizer.decode(decoder_input_ids[i], skip_special_tokens=True)\n",
        "        target_text_list.append(targer_text)\n",
        "        # print(f\"{i+1}번째 번역 결과:\", targer_text)\n",
        "    # print(target_text_list)\n",
        "\n",
        "\n",
        "    for i in range(batch_size):\n",
        "\n",
        "       # 정확도 계산\n",
        "        correct_count = 0\n",
        "        if len(translated_text_list[i]) < len(target_text_list[i]):  # 예측 길이 < 정답 길이\n",
        "\n",
        "            for j in range(len(translated_text_list[i])):\n",
        "                if translated_text_list[i][j] == target_text_list[i][j]:\n",
        "                    correct_count += 1\n",
        "\n",
        "            accuracy_score = correct_count / len(target_text_list[i])\n",
        "\n",
        "\n",
        "        elif len(translated_text_list[i]) > len(target_text_list[i]):  # 예측 길이 > 정답 길이\n",
        "\n",
        "            for j in range(len(target_text_list[i])):\n",
        "                if target_text_list[i][j] == translated_text_list[i][j]:\n",
        "                    correct_count += 1\n",
        "\n",
        "            accuracy_score = correct_count / len(translated_text_list[i])\n",
        "\n",
        "\n",
        "        elif len(translated_text_list[i]) == len(target_text_list[i]):  # 예측 길이 = 정답 길이\n",
        "\n",
        "            for j in range(len(translated_text_list[i])):\n",
        "                if translated_text_list[i][j] == target_text_list[i][j]:\n",
        "                    correct_count += 1\n",
        "\n",
        "            accuracy_score = correct_count / len(translated_text_list[i])\n",
        "\n",
        "        else:\n",
        "            print('ids length error')\n",
        "\n",
        "        one_batch_accuracy_list.append(accuracy_score)\n",
        "\n",
        "    # print(len(one_batch_accuracy_list))\n",
        "    one_batch_accuracy = sum(one_batch_accuracy_list) / batch_size\n",
        "    # print(one_batch_accuracy)\n",
        "    # print(f\"[{str(idx+1).zfill(2)}번째 배치] 모델 번역 단어와 정답 단어의 정확도: {one_batch_accuracy:.2%}\")\n",
        "    total_batch_accuracy_list.append(one_batch_accuracy)\n",
        "    # print(len(total_batch_accuracy_list))\n",
        "    batch_count += 1\n",
        "\n",
        "# print(batch_count)\n",
        "total_batch_accuracy = sum(total_batch_accuracy_list) / batch_count\n",
        "# print(total_batch_accuracy)\n",
        "print('=================================================================')\n",
        "print(f\"[모든 배치] 모델 번역 단어와 정답 단어의 정확도: {total_batch_accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "Yl4tEY68FzOW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67621d9d-05be-4d3d-c960-aa7ec324c171"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "[모든 배치] 모델 번역 단어와 정답 단어의 정확도: 10.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SCiIRMe5FzJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s3D0OREnFzGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u63EGhqQFy_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZQk62I6TFy75"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "eGUpZtSIAquN",
        "tZ6YIV6xAvTK"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}